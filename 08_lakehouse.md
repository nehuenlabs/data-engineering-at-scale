# GuÃ­a de Ejercicios â€” Cap.08: El Lakehouse â€” Delta Lake, Iceberg y Hudi

> Antes del lakehouse, el stack de datos tenÃ­a dos capas separadas:
> el data lake (almacenamiento barato, sin transacciones, difÃ­cil de actualizar)
> y el data warehouse (rÃ¡pido para queries, caro, requiere ETL rÃ­gido).
>
> El lakehouse es la apuesta de que puedes tener las propiedades de un warehouse
> (ACID, esquema, actualizaciones eficientes) sobre el almacenamiento barato de un lake.
>
> Delta Lake, Apache Iceberg, y Apache Hudi son tres implementaciones
> de esa apuesta â€” con filosofÃ­as distintas y tradeoffs distintos.

---

## El problema que resuelven

```
Data Lake tradicional (Parquet en S3):
  âœ“ Barato: $23/TB/mes en S3
  âœ“ Escalable: exabytes sin problema
  âœ— Sin transacciones: dos writes simultÃ¡neos pueden corromperse
  âœ— Sin schema enforcement: cualquiera puede escribir cualquier cosa
  âœ— Actualizar una fila: tienes que reescribir el archivo completo
  âœ— Time travel: imposible sin copias manuales
  âœ— Vacuum: los archivos viejos se acumulan indefinidamente

Data Warehouse (Snowflake, Redshift):
  âœ“ ACID completo
  âœ“ Schema enforcement
  âœ“ Actualizaciones eficientes (row-level)
  âœ“ Time travel integrado
  âœ— Caro: $2,000-5,000/TB/mes de almacenamiento
  âœ— Lock-in: formato propietario
  âœ— DifÃ­cil de integrar con ML (exportar datos es lento)

Lakehouse (Delta Lake / Iceberg / Hudi):
  âœ“ Barato: almacenamiento en S3/GCS/ADLS ($23/TB/mes)
  âœ“ ACID completo
  âœ“ Schema enforcement y evoluciÃ³n
  âœ“ Actualizaciones eficientes (file-level)
  âœ“ Time travel integrado
  âœ“ Abierto: Parquet + metadata en JSON/Avro
  âœ“ Multi-engine: Spark, Flink, Trino, DuckDB pueden leer la misma tabla
```

---

## La diferencia central: cÃ³mo gestionan el metadata

```
Delta Lake:
  Transacciones como archivo de log JSON en _delta_log/
  _delta_log/00000000000000000000.json  â† commit inicial
  _delta_log/00000000000000000001.json  â† add files
  _delta_log/00000000000000000002.json  â† remove files (delete)
  _delta_log/00000000000000000010.json  â† checkpoint (snapshot)
  Modelo: append-only log de commits

Apache Iceberg:
  Ãrbol de metadata: snapshot â†’ manifest list â†’ manifest files â†’ data files
  v2/metadata/snap-1234567890.avro   â† snapshot (lista de manifests)
  v2/metadata/manifest-abc.avro      â† manifest (lista de data files)
  v2/data/*.parquet                  â† data files
  Modelo: Ã¡rbol inmutable de snapshots

Apache Hudi:
  Timeline de commits en .hoodie/
  .hoodie/20240115142301.commit       â† commit metadata
  .hoodie/20240115142301.deltacommit  â† delta de cambios
  Datos en Copy-on-Write o Merge-on-Read
  Modelo: timeline de operaciones con dos storage types
```

---

## Tabla de contenidos

- [SecciÃ³n 8.1 â€” Delta Lake: el log de transacciones](#secciÃ³n-81--delta-lake-el-log-de-transacciones)
- [SecciÃ³n 8.2 â€” ACID en el lakehouse: cÃ³mo funciona realmente](#secciÃ³n-82--acid-en-el-lakehouse-cÃ³mo-funciona-realmente)
- [SecciÃ³n 8.3 â€” Time travel y auditorÃ­a](#secciÃ³n-83--time-travel-y-auditorÃ­a)
- [SecciÃ³n 8.4 â€” Schema evolution y enforcement](#secciÃ³n-84--schema-evolution-y-enforcement)
- [SecciÃ³n 8.5 â€” Operaciones DML: UPDATE, DELETE, MERGE](#secciÃ³n-85--operaciones-dml-update-delete-y-merge)
- [SecciÃ³n 8.6 â€” Apache Iceberg: el modelo de Ã¡rbol](#secciÃ³n-86--apache-iceberg-el-modelo-de-Ã¡rbol)
- [SecciÃ³n 8.7 â€” Comparativa y decisiÃ³n: Delta Lake vs Iceberg vs Hudi](#secciÃ³n-87--comparativa-y-decisiÃ³n-delta-lake-vs-iceberg-vs-hudi)

---

## SecciÃ³n 8.1 â€” Delta Lake: el Log de Transacciones

### Ejercicio 8.1.1 â€” Leer: la estructura del _delta_log

**Tipo: Leer/analizar**

Inspeccionar la estructura real de una tabla Delta Lake:

```python
from delta import DeltaTable
from pyspark.sql import SparkSession
import os
import json

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog",
            "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Crear una tabla Delta Lake:
df = spark.createDataFrame([
    (1, "norte", 100.0),
    (2, "sur",   200.0),
    (3, "norte", 150.0),
], ["id", "region", "monto"])

df.write.format("delta").save("/tmp/mi_tabla_delta/")

# Inspeccionar el _delta_log:
log_dir = "/tmp/mi_tabla_delta/_delta_log/"
for archivo in sorted(os.listdir(log_dir)):
    ruta = os.path.join(log_dir, archivo)
    print(f"\n=== {archivo} ===")
    with open(ruta) as f:
        for linea in f:
            print(json.dumps(json.loads(linea), indent=2))
```

**Preguntas:**

1. Â¿QuÃ© informaciÃ³n contiene el primer archivo JSON del `_delta_log`?
   Â¿QuÃ© acciÃ³n (`add`, `remove`, `metaData`, `commitInfo`) esperas ver?

2. Si escribes 5 veces a la tabla, Â¿cuÃ¡ntos archivos hay en el `_delta_log`?

3. Â¿Un archivo `add` en el log contiene los datos o solo la referencia al archivo?

4. Â¿QuÃ© son los "checkpoints" del `_delta_log` y por quÃ© son necesarios?

5. Si borras un archivo del `_delta_log` manualmente, Â¿quÃ© pasa cuando
   intentas leer la tabla?

**Pista:** El primer commit de Delta Lake contiene al menos tres tipos de acciones:
`metaData` (schema de la tabla, configuraciÃ³n), `protocol` (versiÃ³n del protocolo
que necesita el lector), y `add` (uno por cada archivo Parquet creado).
Los checkpoints son necesarios porque leer el log de transacciones completo
requiere leer todos los archivos JSON desde el inicio â€” con 10,000 commits,
eso es 10,000 archivos. El checkpoint consolida todos los commits anteriores
en un solo snapshot Parquet, reduciendo el tiempo de lectura del log.

---

### Ejercicio 8.1.2 â€” Entender el protocolo de escritura

```python
from delta import DeltaTable
from pyspark.sql import functions as F

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .getOrCreate()

# Escritura concurrente: Â¿quÃ© pasa si dos writers escriben simultÃ¡neamente?
# Delta Lake usa Optimistic Concurrency Control (OCC):
# 1. Writer A lee la versiÃ³n actual del log (v5)
# 2. Writer B lee la versiÃ³n actual del log (v5)
# 3. Writer A escribe su commit (v6) â†’ Ã©xito
# 4. Writer B intenta escribir su commit (v6) â†’ conflicto detectado
#    Delta verifica: Â¿las operaciones de A y B se solapan?
#    Si no se solapan (particiones distintas): merge automÃ¡tico â†’ ambos tienen Ã©xito
#    Si se solapan: Writer B falla con ConcurrentModificationException

# Simular escritura concurrente (en prÃ¡ctica, desde dos procesos):
tabla_delta = DeltaTable.forPath(spark, "/tmp/tabla_concurrente/")

# OperaciÃ³n 1: append a particiÃ³n norte
df_norte = spark.createDataFrame([(10, "norte", 500.0)], ["id", "region", "monto"])
df_norte.write.format("delta").mode("append") \
    .option("txnAppId", "writer_norte") \
    .option("txnVersion", 1) \
    .save("/tmp/tabla_concurrente/")

# OperaciÃ³n 2: append a particiÃ³n sur (no conflicto con operaciÃ³n 1)
df_sur = spark.createDataFrame([(11, "sur", 600.0)], ["id", "region", "monto"])
df_sur.write.format("delta").mode("append") \
    .option("txnAppId", "writer_sur") \
    .option("txnVersion", 1) \
    .save("/tmp/tabla_concurrente/")
```

**Preguntas:**

1. Â¿QuÃ© es Optimistic Concurrency Control (OCC) y cÃ³mo lo implementa Delta Lake?

2. Â¿CuÃ¡ndo dos escrituras concurrentes se pueden "mergear" automÃ¡ticamente?
   Â¿CuÃ¡ndo no?

3. Si Writer B falla con `ConcurrentModificationException`, Â¿debe reintentar
   desde cero o puede recalcular solo el conflicto?

4. Â¿Delta Lake usa locking (bloquear la tabla durante la escritura)?
   Â¿Por quÃ© no?

5. Â¿El OCC funciona con S3 que no garantiza consistencia inmediata?
   Â¿CÃ³mo Delta Lake gestiona esto?

**Pista:** OCC no bloquea recursos â€” permite mÃºltiples writers simultÃ¡neos
y resuelve conflictos al commitear. Delta Lake usa una transacciÃ³n atÃ³mica
del filesystem (renombrado atÃ³mico en HDFS, put-if-absent en S3)
para el commit del log. Si dos writers intentan crear el mismo archivo
`00000000000006.json`, solo uno tiene Ã©xito â€” el otro recibe un error
y debe reintentar. La "resoluciÃ³n automÃ¡tica" ocurre cuando las particiones
modificadas no se solapan â€” Delta Lake detecta que A modificÃ³ particiÃ³n norte
y B modificÃ³ particiÃ³n sur, y ambas pueden coexistir sin conflicto.

---

### Ejercicio 8.1.3 â€” Leer el historial de una tabla

```python
from delta import DeltaTable

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .getOrCreate()

# Crear una tabla con historial de operaciones:
spark.createDataFrame([(1, 100.0), (2, 200.0)], ["id", "monto"]) \
    .write.format("delta").save("/tmp/tabla_historial/")

spark.createDataFrame([(3, 300.0), (4, 400.0)], ["id", "monto"]) \
    .write.format("delta").mode("append").save("/tmp/tabla_historial/")

DeltaTable.forPath(spark, "/tmp/tabla_historial/") \
    .delete("id = 1")

spark.createDataFrame([(5, 500.0)], ["id", "monto"]) \
    .write.format("delta").mode("append").save("/tmp/tabla_historial/")

# Ver el historial completo:
tabla = DeltaTable.forPath(spark, "/tmp/tabla_historial/")
historial = tabla.history()
historial.show(truncate=False)

# El historial muestra:
# version  timestamp   userId  operationName  operationParameters
# 3        2024-01-15  ...     WRITE           {mode: Append, ...}
# 2        2024-01-15  ...     DELETE          {predicate: [id = 1]}
# 1        2024-01-15  ...     WRITE           {mode: Append, ...}
# 0        2024-01-15  ...     WRITE           {mode: ErrorIfExists, ...}
```

**Restricciones:**
1. Crear el historial de operaciones descrito
2. Â¿El historial muestra cuÃ¡ntos archivos se leyeron y escribieron en cada operaciÃ³n?
3. Â¿El historial persiste indefinidamente? Â¿CuÃ¡ndo se trunca?
4. Implementar una funciÃ³n que audita quiÃ©n hizo quÃ© cambio y cuÃ¡ndo

---

### Ejercicio 8.1.4 â€” Vacuum: limpiar archivos huÃ©rfanos

```python
from delta import DeltaTable

tabla = DeltaTable.forPath(spark, "/tmp/mi_tabla/")

# Los archivos que Delta Lake marca como "remove" en el log
# siguen existiendo en disco â€” son necesarios para time travel.
# VACUUM los elimina fÃ­sicamente:

# Ver cuÃ¡ntos archivos se eliminarÃ­an (dry run):
tabla.vacuum(retentionHours=0, dryRun=True)

# Eliminar archivos mÃ¡s viejos que 7 dÃ­as (168 horas, el default):
tabla.vacuum(retentionHours=168)

# PELIGROSO: eliminar archivos mÃ¡s viejos que 0 horas
# (desactiva time travel completamente, pero libera espacio inmediato)
spark.conf.set("spark.databricks.delta.retentionDurationCheck.enabled", "false")
tabla.vacuum(retentionHours=0)
```

```
Estado de la tabla antes del vacuum:

/tmp/mi_tabla/
  _delta_log/
    00000.json  â† commit: add file_A.parquet
    00001.json  â† commit: add file_B.parquet, remove file_A.parquet
    00002.json  â† commit: add file_C.parquet
  file_A.parquet  â† "remove" en el log, pero existe fÃ­sicamente
  file_B.parquet  â† activo
  file_C.parquet  â† activo

DespuÃ©s del vacuum (retentionHours=168):
  file_A.parquet se elimina si tiene mÃ¡s de 7 dÃ­as
  file_B.parquet y file_C.parquet se conservan
```

**Preguntas:**

1. Â¿Por quÃ© el vacuum tiene un retention period mÃ­nimo de 7 dÃ­as por defecto?

2. Â¿QuÃ© pasa si un lector estÃ¡ leyendo `file_A.parquet` en el momento
   en que el vacuum lo elimina?

3. Â¿El vacuum elimina archivos del `_delta_log/` tambiÃ©n?

4. Â¿CuÃ¡nto espacio ocupa tÃ­picamente el `_delta_log/` vs los datos?
   Â¿Crece indefinidamente?

5. Â¿QuÃ© es `OPTIMIZE` y cÃ³mo se diferencia de `VACUUM`?

**Pista:** El retention period de 7 dÃ­as protege contra el caso donde
un lector tiene una transacciÃ³n long-running que lee `file_A` (que fue
"eliminado" en el log pero sigue en disco). Si el vacuum elimina el archivo
mientras el lector aÃºn lo estÃ¡ usando, el lector recibirÃ¡ un error de
"file not found". Los 7 dÃ­as es una heurÃ­stica conservadora para asegurar
que ninguna transacciÃ³n legÃ­tima dure mÃ¡s de eso. `OPTIMIZE` reorganiza
los archivos pequeÃ±os en archivos mÃ¡s grandes (compactaciÃ³n) â€” no elimina
datos, solo los reescribe. VACUUM elimina archivos huÃ©rfanos.

---

### Ejercicio 8.1.5 â€” Diagnosticar: la tabla Delta que creciÃ³ sin control

**Tipo: Diagnosticar**

Una tabla Delta Lake en producciÃ³n ocupa 50 TB en S3 pero los datos
reales son solo 8 TB. El costo mensual es $1,150 en lugar de $184.

```python
# InvestigaciÃ³n:
tabla = DeltaTable.forPath(spark, "s3://mi-tabla/ventas/")

historial = tabla.history()
historial.show()
# version  operationName   rowsAdded  rowsRemoved  numFiles
# 7,432    WRITE           1,200,000  0            120
# 7,431    DELETE          0          45,000       12 (archivos removidos del log)
# 7,430    WRITE           1,100,000  0            110
# ...

# Ver los archivos fÃ­sicos:
import subprocess
resultado = subprocess.run(
    ["aws", "s3", "ls", "--recursive", "s3://mi-tabla/ventas/"],
    capture_output=True, text=True
)
# Hay 7,432 versiones Ã— ~120 archivos = ~891,840 archivos en S3
# La mayorÃ­a son "remove" en el log pero siguen en disco
```

**Preguntas:**

1. Â¿Por quÃ© la tabla tiene 50 TB si los datos son 8 TB?

2. Â¿CuÃ¡ntos archivos hay en total? Â¿Por quÃ© tantos?

3. Â¿QuÃ© operaciÃ³n se deberÃ­a haber ejecutado regularmente?
   Â¿Con quÃ© frecuencia?

4. Â¿CuÃ¡nto tarda el vacuum sobre 50 TB con ~900,000 archivos en S3?
   Â¿CÃ³mo estimarlo?

5. Â¿CÃ³mo prevenirÃ­as este problema en el futuro?

**Pista:** El problema: nunca se ejecutÃ³ `VACUUM`. Con 7,432 escrituras
y un DELETE diario, hay ~7,432 versiones en el log y la mayorÃ­a de los
archivos son "huÃ©rfanos" (marcados como `remove` en el log pero no eliminados).
El vacuum debe ejecutarse regularmente â€” tÃ­picamente despuÃ©s de cada batch
o al menos semanalmente. En producciÃ³n, es comÃºn configurar un job de
mantenimiento diario que ejecuta `OPTIMIZE` (para compactar small files)
y `VACUUM` (para eliminar archivos huÃ©rfanos).
Tiempo de vacuum: S3 limita el rate de operaciones DELETE â€” para 900,000 archivos
a ~3,500 deletes/segundo, serÃ­an ~4 minutos mÃ­nimo, probablemente 15-30 minutos
con throttling.

---

## SecciÃ³n 8.2 â€” ACID en el Lakehouse: CÃ³mo Funciona Realmente

### Ejercicio 8.2.1 â€” Atomicidad: todo o nada

```python
from pyspark.sql import SparkSession
from delta import DeltaTable
import time

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .getOrCreate()

# Sin Delta Lake: si el job falla a mitad, quedan datos parciales
def escribir_sin_transaccion(ruta: str, n_particiones: int):
    for i in range(n_particiones):
        df = spark.createDataFrame(
            [(i * 1000 + j, f"data_{i}_{j}") for j in range(1000)],
            ["id", "valor"]
        )
        df.write.mode("append").parquet(ruta + f"/part_{i}/")
        if i == 3:
            raise Exception("Fallo simulado a mitad del job")
    # Si falla en particiÃ³n 3 â†’ quedan 3 particiones escritas (datos parciales)

# Con Delta Lake: todo el write es atÃ³mico
def escribir_con_delta(ruta: str, n_particiones: int):
    dfs = []
    for i in range(n_particiones):
        df = spark.createDataFrame(
            [(i * 1000 + j, f"data_{i}_{j}") for j in range(1000)],
            ["id", "valor"]
        )
        dfs.append(df)

    # Union y write en una sola operaciÃ³n atÃ³mica:
    from functools import reduce
    df_total = reduce(lambda a, b: a.union(b), dfs)

    try:
        # Si el job falla durante la escritura, ningÃºn dato es visible:
        df_total.write.format("delta").mode("overwrite").save(ruta)
        raise Exception("Fallo simulado durante el write")
    except Exception:
        pass

    # Un lector que lee la tabla durante el write solo ve el estado anterior
    lector_df = spark.read.format("delta").load(ruta)
    # No ve datos parciales â€” atomicidad garantizada
```

**Preguntas:**

1. Â¿CÃ³mo Delta Lake garantiza la atomicidad si los archivos Parquet
   se escriben individualmente antes del commit?

2. Si el job falla despuÃ©s de escribir todos los archivos Parquet pero
   ANTES de escribir el commit JSON al `_delta_log`, Â¿quÃ© pasa?

3. Â¿Un lector que lee durante un write ve datos del write en progreso?

4. Â¿La atomicidad de Delta Lake funciona si el `_delta_log` estÃ¡ en S3?
   Â¿S3 garantiza operaciones atÃ³micas?

**Pista:** Delta Lake escribe primero todos los archivos Parquet en ubicaciones
temporales (sin registrarlos en el log). Solo despuÃ©s escribe el archivo
JSON de commit al `_delta_log/`. Si el job falla antes del commit, los archivos
Parquet existen en S3 pero ningÃºn lector los conoce â€” son "fantasmas" que
el vacuum eliminarÃ¡ eventualmente. La "atomicidad" del commit en S3 se logra
con `put-if-absent`: el writer intenta crear el archivo `00006.json` â€” si ya
existe (otro writer ganÃ³ la carrera), falla y reintenta con `00007.json`.

---

### Ejercicio 8.2.2 â€” Aislamiento: lectores no afectados por escritores

```python
# Demostrar snapshot isolation de Delta Lake:
# Los lectores ven siempre un snapshot consistente de la tabla,
# independientemente de las escrituras concurrentes.

import threading
import time
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .getOrCreate()

# Crear tabla inicial:
spark.createDataFrame([(1, 100.0), (2, 200.0), (3, 300.0)], ["id", "monto"]) \
    .write.format("delta").save("/tmp/tabla_isolation/")

lecturas_durante_write = []

def leer_continuamente():
    """Lee la tabla repetidamente durante la escritura."""
    for _ in range(10):
        df = spark.read.format("delta").load("/tmp/tabla_isolation/")
        conteo = df.count()
        suma = df.agg({"monto": "sum"}).collect()[0][0]
        lecturas_durante_write.append({"conteo": conteo, "suma": suma})
        time.sleep(0.1)

def escribir_datos_nuevos():
    """Escribe datos nuevos mientras se lee."""
    time.sleep(0.3)  # dar tiempo al lector de arrancar
    spark.createDataFrame([(4, 400.0), (5, 500.0)], ["id", "monto"]) \
        .write.format("delta").mode("append").save("/tmp/tabla_isolation/")

# Ejecutar lectura y escritura concurrentemente:
t_lector = threading.Thread(target=leer_continuamente)
t_escritor = threading.Thread(target=escribir_datos_nuevos)

t_lector.start()
t_escritor.start()
t_lector.join()
t_escritor.join()

print("Lecturas durante el write:")
for lectura in lecturas_durante_write:
    print(f"  conteo={lectura['conteo']}, suma={lectura['suma']}")
# Las lecturas antes del write ven 3 filas, suma=600
# Las lecturas despuÃ©s del write ven 5 filas, suma=1500
# NUNCA ven 4 filas o suma=1000 (estado inconsistente)
```

**Preguntas:**

1. Â¿QuÃ© nivel de aislamiento ofrece Delta Lake por defecto?
   (Serializable, Snapshot Isolation, Read Committed, Read Uncommitted)

2. Â¿Es posible que un lector vea 4 filas (2 originales + 2 nuevas incompletos)?
   Â¿Por quÃ© no?

3. Â¿El aislamiento de Delta Lake tiene algÃºn costo de rendimiento?

4. Â¿QuÃ© es "dirty read" y Delta Lake lo previene?

---

### Ejercicio 8.2.3 â€” Consistencia: schema enforcement

```python
from delta import DeltaTable
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .getOrCreate()

# Crear tabla con schema definido:
spark.createDataFrame(
    [(1, "norte", 100.0)],
    ["id", "region", "monto"]
).write.format("delta").save("/tmp/tabla_schema/")

# Intento 1: escribir con schema diferente (columna extra)
try:
    spark.createDataFrame(
        [(2, "sur", 200.0, "extra_columna")],
        ["id", "region", "monto", "nueva_columna"]
    ).write.format("delta").mode("append").save("/tmp/tabla_schema/")
except Exception as e:
    print(f"Error esperado: {e}")
    # AnalysisException: A schema mismatch detected when writing to...

# Intento 2: escribir con tipo incorrecto
try:
    spark.createDataFrame(
        [(3, "este", "no_es_numero")],
        ["id", "region", "monto"]  # monto deberÃ­a ser float
    ).write.format("delta").mode("append").save("/tmp/tabla_schema/")
except Exception as e:
    print(f"Error de tipo: {e}")

# Permitir evoluciÃ³n del schema:
spark.createDataFrame(
    [(4, "oeste", 300.0, "nuevo_valor")],
    ["id", "region", "monto", "columna_nueva"]
).write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save("/tmp/tabla_schema/")
```

**Preguntas:**

1. Â¿QuÃ© operaciones de schema estÃ¡n permitidas con `mergeSchema=true`
   y cuÃ¡les no?

2. Si alguien escribe accidentalmente con `mergeSchema=true` y aÃ±ade
   una columna con nombre incorrecto, Â¿cÃ³mo lo corriges?

3. Â¿Schema enforcement protege contra datos invÃ¡lidos dentro de una columna
   (ej: `monto = -1` cuando el negocio dice que debe ser positivo)?

4. Â¿CÃ³mo aÃ±adirÃ­as constraints de negocio (CHECK constraints) a Delta Lake?

**Pista:** Delta Lake 2.0+ soporta constraints con `ALTER TABLE ADD CONSTRAINT`:
```sql
ALTER TABLE ventas ADD CONSTRAINT monto_positivo CHECK (monto > 0);
```
Si intentas insertar una fila con `monto = -1`, Delta Lake rechaza el commit.
Los constraints se almacenan en el `_delta_log` como metadatos y se verifican
en cada write. Para constraints mÃ¡s complejos que SQL no puede expresar,
la alternativa es validar en el pipeline antes de escribir.

---

### Ejercicio 8.2.4 â€” Durabilidad: quÃ© pasa si S3 pierde un archivo

```python
# Delta Lake usa S3 como almacenamiento â€” S3 garantiza:
# - 99.999999999% (11 nines) de durabilidad
# - ReplicaciÃ³n automÃ¡tica en mÃºltiples AZs
# - Consistencia eventual en lectura tras escritura (corregido en S3 strong consistency desde 2020)

# Â¿QuÃ© pasa si S3 pierde un archivo de datos (probabilidad ~0 pero teÃ³rico)?
# 1. El _delta_log tiene el registro del archivo (add action)
# 2. Los lectores intentan leer el archivo â†’ error 404
# 3. Delta Lake NO puede recuperar el dato automÃ¡ticamente
# 4. La soluciÃ³n: backups del _delta_log y de los archivos de datos

# Estrategia de backup con Delta Lake:
# OpciÃ³n A: S3 Versioning (mantiene versiones de cada objeto)
# OpciÃ³n B: S3 Replication (copia a otro bucket/regiÃ³n)
# OpciÃ³n C: Export periÃ³dico a otro sistema
```

**Preguntas:**

1. Â¿La "durabilidad" de Delta Lake depende 100% de la durabilidad de S3?

2. Â¿Si el `_delta_log` se corrompe pero los archivos Parquet estÃ¡n intactos,
   puedes recuperar los datos? Â¿CÃ³mo?

3. Â¿QuÃ© es "Delta Log checkpointing" y cÃ³mo ayuda a la recuperaciÃ³n?

4. Â¿QuÃ© diferencia hay entre la durabilidad de HDFS (replicaciÃ³n interna)
   y la de S3 (replicaciÃ³n gestionada por AWS)?

---

### Ejercicio 8.2.5 â€” Leer: ACID en un lakehouse vs en PostgreSQL

**Tipo: Comparar**

Delta Lake ofrece ACID pero con garantÃ­as diferentes a PostgreSQL:

```
PostgreSQL:
  Transacciones a nivel de fila: puedes actualizar una sola fila
  Aislamiento: Serializable (el mÃ¡s fuerte)
  Granularidad del lock: fila, pÃ¡gina, tabla
  Latencia de commit: milisegundos
  Uso: OLTP (miles de transacciones pequeÃ±as por segundo)

Delta Lake:
  Transacciones a nivel de commit: un write es atÃ³mico
  Aislamiento: Snapshot Isolation (no Serializable por defecto)
  Granularidad: archivo Parquet completo
  Latencia de commit: segundos (el commit al log + S3)
  Uso: OLAP (pocos commits grandes por hora)
```

**Preguntas:**

1. Â¿Por quÃ© Delta Lake no ofrece transacciones a nivel de fila como PostgreSQL?

2. Â¿Puedes usar Delta Lake para un sistema de pagos donde cada transacciÃ³n
   actualiza el saldo de una cuenta? Â¿Por quÃ© serÃ­a una mala idea?

3. Â¿QuÃ© workload de base de datos es claramente mejor en PostgreSQL?
   Â¿CuÃ¡l es claramente mejor en Delta Lake?

4. Â¿"Snapshot Isolation" de Delta Lake puede causar anomalÃ­as que
   Serializable previene? Â¿CuÃ¡les?

**Pista:** La anomalÃ­a clÃ¡sica de Snapshot Isolation que Serializable previene
es el "write skew": dos transacciones leen el mismo dato, calculan algo basado
en Ã©l, y ambas escriben sin ver la escritura de la otra.
Ejemplo en Delta Lake: dos jobs calculan "el monto total de la tabla" y ambos
ven $1000. Job A escribe "nuevo total = 1000 + mi_incremento". Job B escribe
"nuevo total = 1000 + mi_incremento". El resultado final ignora uno de los
incrementos. Para analytics batch donde esto no ocurre (cada job escribe
particiones distintas), Snapshot Isolation es suficiente.

---

## SecciÃ³n 8.3 â€” Time Travel y AuditorÃ­a

### Ejercicio 8.3.1 â€” Leer versiones anteriores

```python
from delta import DeltaTable
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .getOrCreate()

# Crear tabla con mÃºltiples versiones:
spark.createDataFrame([(1, 100.0), (2, 200.0)], ["id", "monto"]) \
    .write.format("delta").save("/tmp/tabla_tt/")   # versiÃ³n 0

spark.createDataFrame([(3, 300.0)], ["id", "monto"]) \
    .write.format("delta").mode("append").save("/tmp/tabla_tt/")  # versiÃ³n 1

DeltaTable.forPath(spark, "/tmp/tabla_tt/").delete("id = 1")  # versiÃ³n 2

# Time travel por versiÃ³n:
df_v0 = spark.read.format("delta") \
    .option("versionAsOf", 0) \
    .load("/tmp/tabla_tt/")
# Contiene: [(1, 100.0), (2, 200.0)]

df_v1 = spark.read.format("delta") \
    .option("versionAsOf", 1) \
    .load("/tmp/tabla_tt/")
# Contiene: [(1, 100.0), (2, 200.0), (3, 300.0)]

df_actual = spark.read.format("delta").load("/tmp/tabla_tt/")
# Contiene: [(2, 200.0), (3, 300.0)]  â€” id=1 fue eliminado en v2

# Time travel por timestamp:
df_ayer = spark.read.format("delta") \
    .option("timestampAsOf", "2024-01-14") \
    .load("/tmp/tabla_tt/")
```

**Restricciones:**
1. Verificar que cada versiÃ³n contiene los datos correctos
2. Â¿El time travel por timestamp usa la hora UTC o local?
3. Â¿Puedes hacer time travel a un momento entre dos commits?
4. Implementar una query de "cambios entre versiones" (quÃ© se aÃ±adiÃ³ y quÃ© se eliminÃ³)

---

### Ejercicio 8.3.2 â€” CDF (Change Data Feed): capturar cambios

```python
# Change Data Feed: expone los cambios (inserts, updates, deletes)
# como filas en una tabla especial â€” Ãºtil para CDC (Change Data Capture)

# Habilitar CDF en la tabla:
spark.sql("""
    ALTER TABLE delta.`/tmp/tabla_cdf/`
    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
""")

# Hacer operaciones:
spark.createDataFrame([(1, 100.0), (2, 200.0)], ["id", "monto"]) \
    .write.format("delta").save("/tmp/tabla_cdf/")  # v0: inserts

DeltaTable.forPath(spark, "/tmp/tabla_cdf/").update(
    condition="id = 1",
    set={"monto": "150.0"}
)  # v1: update

DeltaTable.forPath(spark, "/tmp/tabla_cdf/").delete("id = 2")  # v2: delete

# Leer los cambios entre versiones:
cambios = spark.read.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingVersion", 0) \
    .option("endingVersion", 2) \
    .load("/tmp/tabla_cdf/")

cambios.show()
# id  monto  _change_type  _commit_version  _commit_timestamp
# 1   100.0  insert        0                2024-01-15 14:00
# 2   200.0  insert        0                2024-01-15 14:00
# 1   100.0  update_preimage  1             2024-01-15 14:01
# 1   150.0  update_postimage 1             2024-01-15 14:01
# 2   200.0  delete        2                2024-01-15 14:02
```

**Preguntas:**

1. Â¿QuÃ© es `update_preimage` y `update_postimage`?

2. Â¿CDF tiene overhead en el rendimiento de escritura?

3. Â¿Para quÃ© casos de uso es Ãºtil CDF?

4. Â¿CDF puede usarse para sincronizar Delta Lake con una base de datos operacional?

5. Â¿CDF de Delta Lake es equivalente a un Kafka topic de cambios?

**Pista:** CDF es Ãºtil para:
(1) Sincronizar Delta Lake con sistemas downstream sin releer la tabla completa
(2) AuditorÃ­a: saber exactamente quÃ© cambiÃ³, cuÃ¡ndo, y cuÃ¡l era el valor anterior
(3) Pipelines de ML: entrenar incrementalmente solo con datos nuevos/cambiados
(4) Data mesh: propagar cambios a otras tablas o sistemas
La diferencia con Kafka: CDF es pull (tÃº decides cuÃ¡ndo leer los cambios)
mientras Kafka es push (los consumidores se suscriben y reciben en tiempo real).

---

### Ejercicio 8.3.3 â€” Reproducibilidad: volver a calcular un resultado de hace 6 meses

**Tipo: Implementar**

Un auditor pide reproducir exactamente el reporte de revenue de julio 2024:

```python
from delta import DeltaTable
from pyspark.sql import SparkSession, functions as F

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .getOrCreate()

def calcular_revenue_julio_2024(tabla_ventas: str) -> float:
    """
    Calcula el revenue de julio 2024 usando los datos exactos de esa fecha.
    La tabla puede haber sido actualizada desde entonces (correcciones, etc.)
    """
    # Sin time travel: usarÃ­a los datos actuales (posiblemente modificados)
    df_actual = spark.read.format("delta").load(tabla_ventas)
    revenue_actual = df_actual.filter(
        (F.col("fecha") >= "2024-07-01") & (F.col("fecha") < "2024-08-01")
    ).agg(F.sum("monto")).collect()[0][0]

    # Con time travel: datos exactos de julio 2024
    df_julio = spark.read.format("delta") \
        .option("timestampAsOf", "2024-07-31 23:59:59") \
        .load(tabla_ventas)
    revenue_julio = df_julio.filter(
        (F.col("fecha") >= "2024-07-01") & (F.col("fecha") < "2024-08-01")
    ).agg(F.sum("monto")).collect()[0][0]

    print(f"Revenue actual (con correcciones):  ${revenue_actual:,.2f}")
    print(f"Revenue julio (datos originales):   ${revenue_julio:,.2f}")
    print(f"Diferencia (correcciones):          ${revenue_actual - revenue_julio:,.2f}")

    return revenue_julio
```

**Restricciones:**
1. Â¿El time travel funciona si el VACUUM eliminÃ³ los archivos de julio 2024?
2. Â¿CÃ³mo defines una polÃ­tica de retenciÃ³n que garantiza la reproducibilidad?
3. Â¿CuÃ¡nto espacio adicional ocupa mantener 12 meses de historial?
4. Implementar una funciÃ³n de "auditorÃ­a de cambios" entre dos fechas

---

### Ejercicio 8.3.4 â€” Restaurar una tabla a una versiÃ³n anterior

```python
from delta import DeltaTable

tabla = DeltaTable.forPath(spark, "/tmp/tabla_restaurar/")

# Restaurar a una versiÃ³n anterior (operaciÃ³n destructiva para las versiones mÃ¡s recientes):
tabla.restoreToVersion(5)
# La tabla vuelve al estado de la versiÃ³n 5
# Las versiones 6, 7, 8... todavÃ­a existen en el log (para auditorÃ­a)
# pero la "versiÃ³n actual" es ahora el estado de la v5 restaurado

# Restaurar a un timestamp:
tabla.restoreToTimestamp("2024-01-01")

# Â¿La restauraciÃ³n crea una nueva versiÃ³n o sobrescribe?
tabla.history(5).show()
# version  operationName
# 9        RESTORE  â† nueva versiÃ³n 9 que representa el estado de v5
# 8        ...
# 7        ...
# 6        ...
# 5        ...  â† el estado al que volvimos
```

**Preguntas:**

1. `restoreToVersion(5)` crea una nueva versiÃ³n 9 (si estÃ¡bamos en v8)
   o sobrescribe? Â¿Por quÃ©?

2. Â¿Puedes hacer un "restore parcial" (restaurar solo algunas filas)?

3. Â¿CuÃ¡l es la diferencia entre `RESTORE` y leer con time travel + reescribir?

4. Â¿`RESTORE` respeta el schema actual si cambiÃ³ desde la versiÃ³n restaurada?

---

### Ejercicio 8.3.5 â€” Leer: time travel en producciÃ³n â€” cuÃ¡ndo es indispensable

**Tipo: Analizar**

Para cada escenario, evaluar si el time travel de Delta Lake resuelve el problema:

```
Escenario 1:
  Un pipeline de ML entrenÃ³ un modelo hace 3 meses y quiere reproducir
  exactamente las features que usÃ³ para debugging.
  
Escenario 2:
  Un bug en el pipeline de ingesta escribiÃ³ datos incorrectos durante 2 horas.
  Necesitas revertir esas 2 horas de datos sin afectar los datos anteriores.
  
Escenario 3:
  La tabla tiene 500 GB. Hace 7 dÃ­as se ejecutÃ³ VACUUM con retentionHours=0.
  Un auditor pide los datos de hace 30 dÃ­as.
  
Escenario 4:
  Dos pipelines calculan el mismo KPI con lÃ³gica diferente.
  Quieres comparar sus resultados histÃ³ricos semana a semana.
  
Escenario 5:
  Un sistema de recomendaciÃ³n usa features de una tabla Delta Lake
  actualizada cada hora. Quieres saber quÃ© features usÃ³ para
  recomendar un producto especÃ­fico hace 2 dÃ­as.
```

Para cada escenario: Â¿el time travel resuelve el problema?
Si no, Â¿quÃ© soluciÃ³n alternativa existe?

---

## SecciÃ³n 8.4 â€” Schema Evolution y Enforcement

### Ejercicio 8.4.1 â€” Los modos de schema evolution de Delta Lake

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .getOrCreate()

# Tabla original:
spark.createDataFrame(
    [(1, "norte", 100.0)],
    ["id", "region", "monto"]
).write.format("delta").save("/tmp/tabla_schema_evo/")

# Modo 1: mergeSchema â€” aÃ±adir columnas nuevas
spark.createDataFrame(
    [(2, "sur", 200.0, "USD")],
    ["id", "region", "monto", "moneda"]
).write.format("delta") \
    .mode("append") \
    .option("mergeSchema", "true") \
    .save("/tmp/tabla_schema_evo/")
# Resultado: columna "moneda" aÃ±adida, filas anteriores tienen moneda=null

# Modo 2: overwriteSchema â€” cambiar el schema completamente
spark.createDataFrame(
    [(3, "este", 300.0, "EUR", "premium")],
    ["id", "region", "monto", "moneda", "segmento"]
).write.format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .save("/tmp/tabla_schema_evo/")
# CUIDADO: esto borra los datos anteriores y cambia el schema

# Verificar el schema actual:
print(spark.read.format("delta").load("/tmp/tabla_schema_evo/").schema)
```

**Restricciones:**
1. Documentar el schema en cada versiÃ³n usando time travel
2. Â¿QuÃ© versiones son accesibles con time travel despuÃ©s del overwriteSchema?
3. Implementar la estrategia de "schema versionado" para producciÃ³n
4. Â¿CuÃ¡ndo usar mergeSchema vs overwriteSchema?

---

### Ejercicio 8.4.2 â€” Column mapping: renombrar columnas sin reescribir datos

Delta Lake 2.0+ soporta renombrar y eliminar columnas sin reescribir los archivos:

```python
# Habilitar column mapping:
spark.sql("""
    ALTER TABLE delta.`/tmp/tabla_mapping/`
    SET TBLPROPERTIES (
        'delta.columnMapping.mode' = 'name',
        'delta.minReaderVersion' = '2',
        'delta.minWriterVersion' = '5'
    )
""")

# Renombrar una columna (sin reescribir los datos):
spark.sql("""
    ALTER TABLE delta.`/tmp/tabla_mapping/`
    RENAME COLUMN monto TO precio_total
""")

# Eliminar una columna (sin reescribir los datos):
spark.sql("""
    ALTER TABLE delta.`/tmp/tabla_mapping/`
    DROP COLUMN descripcion_interna
""")
```

```
Sin column mapping:
  Renombrar "monto" a "precio_total":
  - Reescribir TODOS los archivos Parquet (cambia el nombre de la columna en cada uno)
  - Para 1 TB de datos: proceso de horas, costo significativo

Con column mapping:
  - Solo actualizar el metadata en el _delta_log
  - Los archivos Parquet siguen teniendo "monto" internamente
  - El lector traduce "precio_total" â†’ "monto" usando el mapping
  - Para 1 TB de datos: proceso de segundos, costo mÃ­nimo
```

**Preguntas:**

1. Â¿Hay algÃºn costo de rendimiento en lectura al usar column mapping?

2. Â¿Column mapping es compatible con todos los lectores de Delta Lake?

3. Â¿Puedes usar column mapping con `versionAsOf` para leer una versiÃ³n
   anterior que tenÃ­a el nombre de columna original?

4. Â¿QuÃ© limitaciones tiene column mapping? Â¿Puedo renombrar a un nombre
   que ya usÃ© antes?

---

### Ejercicio 8.4.3 â€” Schema evolution en un pipeline de producciÃ³n

**Tipo: DiseÃ±ar**

Un sistema de e-commerce tiene una tabla Delta Lake de eventos que recibe
100,000 eventos/hora de mÃºltiples microservicios. El equipo quiere
aÃ±adir nuevas columnas sin interrumpir el servicio:

```
Schema actual (v1):
  user_id: int
  evento: string
  timestamp: timestamp
  monto: double

Schema nuevo (v2) â€” cambios propuestos:
  user_id: int
  evento: string
  timestamp: timestamp
  monto: double
  moneda: string         â† nueva, requerida
  region: string         â† nueva, opcional (algunos microservicios no la tienen)
  metadata: struct<...>  â† nueva, para datos adicionales del microservicio
```

**Preguntas:**

1. Â¿Puedes aÃ±adir `moneda` como columna requerida sin romper los microservicios
   que ya estÃ¡n en producciÃ³n?

2. Â¿CuÃ¡l es el orden seguro para desplegar el cambio de schema?

3. Â¿CÃ³mo manejas los eventos histÃ³ricos que no tienen `moneda`?

4. Â¿Si un microservicio envÃ­a `moneda = null`, es un error o aceptable?

5. Proponer la estrategia completa de migraciÃ³n (con etapas y rollback plan).

**Pista:** El orden seguro para aÃ±adir columnas a una tabla activa:
1. AÃ±adir la columna con `mergeSchema=true` como nullable (no rompe los writers actuales)
2. Actualizar los microservicios uno a uno para enviar la nueva columna
3. Solo despuÃ©s de que todos los writers envÃ­en la columna, marcarlo como NOT NULL
   (si es necesario)
4. Rellenar los valores histÃ³ricos (backfill) con un valor por defecto si se necesitan
El rollback: simplemente no hacer el paso 3 â€” los writers nuevos envÃ­an la columna,
los viejos envÃ­an null, y el lector maneja ambos casos.

---

### Ejercicio 8.4.4 â€” Detectar y alertar sobre cambios de schema inesperados

```python
from delta import DeltaTable
from pyspark.sql.types import StructType
import json

def verificar_schema_compatible(
    tabla: str,
    schema_esperado: StructType,
    alerta_fn=None,
) -> bool:
    """
    Verifica que el schema actual de la tabla Delta Lake es compatible
    con el schema esperado. Alerta si hay cambios incompatibles.
    """
    tabla_delta = DeltaTable.forPath(spark, tabla)
    schema_actual = spark.read.format("delta").load(tabla).schema

    # Columnas esperadas que faltan:
    campos_faltantes = set(schema_esperado.fieldNames()) - set(schema_actual.fieldNames())

    # Columnas extra no esperadas:
    campos_extra = set(schema_actual.fieldNames()) - set(schema_esperado.fieldNames())

    # Columnas con tipo diferente:
    tipos_diferentes = []
    for campo in schema_esperado:
        if campo.name in schema_actual.fieldNames():
            campo_actual = schema_actual[campo.name]
            if campo_actual.dataType != campo.dataType:
                tipos_diferentes.append({
                    "columna": campo.name,
                    "esperado": str(campo.dataType),
                    "actual": str(campo_actual.dataType),
                })

    problemas = {
        "campos_faltantes": list(campos_faltantes),
        "campos_extra": list(campos_extra),
        "tipos_diferentes": tipos_diferentes,
    }

    hay_problemas = any(len(v) > 0 for v in problemas.values())
    if hay_problemas and alerta_fn:
        alerta_fn(f"Schema incompatible en {tabla}: {json.dumps(problemas)}")

    return not hay_problemas
```

**Restricciones:**
1. Implementar la funciÃ³n completa
2. Integrarla en el pipeline de ingesta para verificar el schema de cada batch
3. Â¿CÃ³mo distingues "cambio incompatible" de "cambio compatible" (nueva columna nullable)?
4. Â¿DÃ³nde en el pipeline deberÃ­a ejecutarse esta verificaciÃ³n?

---

### Ejercicio 8.4.5 â€” Leer: schema evolution en Iceberg vs Delta Lake

**Tipo: Comparar**

Delta Lake y Apache Iceberg tienen enfoques distintos para schema evolution:

```
Delta Lake schema evolution:
  - mergeSchema: aÃ±adir columnas compatibles
  - overwriteSchema: cambiar el schema completamente (destructivo)
  - Column mapping: renombrar/eliminar sin reescribir
  - Constraints: CHECK constraints en columnas

Apache Iceberg schema evolution:
  - ADD COLUMN: aÃ±adir columna en cualquier posiciÃ³n
  - DROP COLUMN: eliminar columna (los datos siguen, solo se ocultan)
  - RENAME COLUMN: renombrar sin reescribir
  - ALTER COLUMN: cambiar tipo (con reglas de compatibilidad)
  - ParticiÃ³n evolution: cambiar la particiÃ³n sin reescribir
  - Hidden partitioning: las particiones no son visibles en el schema del usuario
```

**Preguntas:**

1. Â¿Iceberg puede renombrar columnas sin reescribir datos?
   Â¿Tiene algo como column mapping de Delta Lake?

2. Â¿QuÃ© es "partition evolution" de Iceberg y por quÃ© no existe en Delta Lake?

3. Â¿CuÃ¡l de los dos tiene mejor soporte para cambios de tipo de columna
   (ej: `int` â†’ `long`)?

4. Â¿El "hidden partitioning" de Iceberg simplifica los queries para el usuario?
   Â¿Tiene desventajas?

> ðŸ“– Profundizar: el paper *Apache Iceberg: An Architectural Look Under the Covers*
> (Russell, VLDB 2022) explica el diseÃ±o del Ã¡rbol de metadata de Iceberg
> y las ventajas de su approach vs el log de Delta Lake. Especialmente relevante
> la SecciÃ³n 3 sobre schema evolution y la SecciÃ³n 4 sobre partition evolution.

---

## SecciÃ³n 8.5 â€” Operaciones DML: UPDATE, DELETE y MERGE

### Ejercicio 8.5.1 â€” La mecÃ¡nica de UPDATE en Delta Lake

```python
from delta import DeltaTable

tabla = DeltaTable.forPath(spark, "/tmp/tabla_update/")

# UPDATE: modificar filas existentes
tabla.update(
    condition="region = 'norte'",
    set={"monto": "monto * 1.1"}  # 10% de aumento para regiÃ³n norte
)

# Â¿QuÃ© hace Delta Lake internamente?
# 1. Leer todos los archivos Parquet que contienen filas de regiÃ³n='norte'
# 2. Escribir NUEVOS archivos Parquet con las filas modificadas
# 3. AÃ±adir las filas NUEVAS al log (add actions)
# 4. Marcar las filas VIEJAS como removed (remove actions)
# 5. Commit atÃ³mico del log

# Con Z-ordering (para UPDATE eficientes):
tabla.optimize().executeZOrderBy("region")
# Si los datos estÃ¡n Z-ordenados por region, el UPDATE solo necesita
# leer los archivos que contienen regiÃ³n='norte' â†’ menos I/O
```

**Preguntas:**

1. Â¿Delta Lake puede actualizar una sola fila sin reescribir el archivo entero?

2. Si un archivo Parquet de 1 GB tiene 100,000 filas y quieres actualizar
   solo 1 fila, Â¿cuÃ¡ntos datos se reescriben?

3. Â¿QuÃ© es Z-ordering y cÃ³mo reduce el costo de los UPDATEs?

4. Â¿CuÃ¡nto mÃ¡s costoso es un UPDATE en Delta Lake vs en PostgreSQL?

5. Â¿CuÃ¡ndo es preferible "actualizar" datos en Delta Lake vs simplemente
   aÃ±adir una nueva fila con los valores actualizados (SCD Type 2)?

**Pista:** Delta Lake escribe Copy-on-Write (CoW) por defecto â€” para actualizar
1 fila en un archivo de 1 GB, reescribe el archivo completo de 1 GB con la fila
modificada. Esto es costoso para actualizaciones frecuentes pero eficiente para
lectura (no hay merge necesario). La alternativa para workloads con muchas
actualizaciones pequeÃ±as es Hudi con Merge-on-Read (MoR) â€” escribe solo el delta
y hace el merge al leer. MÃ¡s rÃ¡pido para escribir, mÃ¡s costoso para leer.

---

### Ejercicio 8.5.2 â€” MERGE: upsert eficiente

```python
from delta import DeltaTable
from pyspark.sql import SparkSession, functions as F

spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
    .getOrCreate()

# MERGE: la operaciÃ³n mÃ¡s versÃ¡til del lakehouse
# Permite: INSERT nuevas filas, UPDATE filas existentes, DELETE filas

tabla_delta = DeltaTable.forPath(spark, "/tmp/tabla_clientes/")

# Datos de actualizaciÃ³n:
actualizaciones = spark.createDataFrame([
    (1, "Alice", "premium", 1500.0),    # existente â†’ actualizar
    (2, "Bob",   "standard", 200.0),    # existente â†’ actualizar
    (5, "Eve",   "new", 0.0),           # nuevo â†’ insertar
], ["id", "nombre", "segmento", "gasto"])

# MERGE INTO:
tabla_delta.alias("t").merge(
    actualizaciones.alias("s"),
    "t.id = s.id"
).whenMatchedUpdate(set={
    "nombre": "s.nombre",
    "segmento": "s.segmento",
    "gasto": "s.gasto",
}).whenNotMatchedInsert(values={
    "id": "s.id",
    "nombre": "s.nombre",
    "segmento": "s.segmento",
    "gasto": "s.gasto",
}).execute()

# MERGE con DELETE:
tabla_delta.alias("t").merge(
    actualizaciones.alias("s"),
    "t.id = s.id"
).whenMatchedUpdate(
    condition="s.segmento != 'deleted'",
    set={"segmento": "s.segmento"}
).whenMatchedDelete(
    condition="s.segmento = 'deleted'"
).whenNotMatchedInsert(
    values={"id": "s.id", "nombre": "s.nombre", "segmento": "s.segmento"}
).execute()
```

**Restricciones:**
1. Implementar un pipeline de SCD Type 2 usando MERGE (mantener historial de cambios)
2. Medir el rendimiento del MERGE para 1M de actualizaciones sobre 100M de filas
3. Â¿El MERGE genera un shuffle? Â¿CuÃ¡nto shuffle?
4. Â¿CuÃ¡ndo es mÃ¡s eficiente un MERGE que un batch de DELETEs + INSERTs?

---

### Ejercicio 8.5.3 â€” Implementar CDC con MERGE

CDC (Change Data Capture) es el patrÃ³n de sincronizar cambios de una fuente
(base de datos operacional) a un destino (lakehouse):

```python
def aplicar_cambios_cdc(
    spark: SparkSession,
    tabla_delta: str,
    cambios_kafka: "DataFrame",  # columnas: id, operacion (I/U/D), timestamp, datos...
) -> dict:
    """
    Aplica un batch de cambios CDC a una tabla Delta Lake.
    
    cambios_kafka contiene:
      - operacion: 'I' (insert), 'U' (update), 'D' (delete)
      - id: clave primaria
      - resto de columnas: datos actualizados
    
    Para cada operaciÃ³n, el MERGE hace lo correcto:
      - 'I': INSERT si no existe, UPDATE si ya existe (idempotente)
      - 'U': UPDATE
      - 'D': DELETE
    """
    tabla = DeltaTable.forPath(spark, tabla_delta)

    # Deduplicar: si el mismo id tiene mÃºltiples cambios, quedarse con el Ãºltimo
    cambios_dedup = cambios_kafka \
        .withColumn("rank",
            F.row_number().over(
                Window.partitionBy("id")
                      .orderBy(F.col("timestamp").desc())
            )
        ).filter(F.col("rank") == 1).drop("rank")

    # MERGE diferenciado por tipo de operaciÃ³n:
    tabla.alias("t").merge(
        cambios_dedup.alias("s"),
        "t.id = s.id"
    ).whenMatchedUpdate(
        condition="s.operacion IN ('I', 'U')",
        set={col: f"s.{col}" for col in tabla.toDF().columns if col != "id"}
    ).whenMatchedDelete(
        condition="s.operacion = 'D'"
    ).whenNotMatchedInsert(
        condition="s.operacion IN ('I', 'U')",
        values={col: f"s.{col}" for col in tabla.toDF().columns}
    ).execute()

    return {
        "cambios_aplicados": cambios_dedup.count(),
        "operacion": "cdc_merge",
    }
```

**Restricciones:**
1. Implementar la funciÃ³n completa
2. Â¿QuÃ© pasa si llegan eventos CDC fuera de orden? Â¿El pipeline es correcto?
3. Medir el rendimiento para 100K cambios/batch sobre una tabla de 1 TB
4. Â¿CÃ³mo garantizas exactly-once processing con este patrÃ³n?

---

### Ejercicio 8.5.4 â€” OPTIMIZE y Z-ordering: compactar para queries eficientes

```python
from delta import DeltaTable

# OPTIMIZE: compactar archivos pequeÃ±os en archivos grandes
tabla = DeltaTable.forPath(spark, "/tmp/tabla_fragmentada/")

# Ver el estado actual:
tabla.detail().show()
# numFiles: 10,000 (!) â€” muchos archivos pequeÃ±os

# Compactar:
tabla.optimize().executeCompaction()
# numFiles: 100 â€” archivos de ~128 MB

# Z-ordering: organizar los datos para acceso eficiente por columnas frecuentes
tabla.optimize().executeZOrderBy("region", "fecha")
# Ahora las filas de "norte" + "2024-01" estÃ¡n en los mismos archivos
# â†’ queries con WHERE region='norte' AND fecha='2024-01' leen mucho menos

# Verificar el resultado:
tabla.detail().show()
# numFiles: 100 (compactado)
# clusteringColumns: [region, fecha] (z-ordenado)
```

**Preguntas:**

1. Â¿Por quÃ© OPTIMIZE no cambia los datos, solo los reorganiza?

2. Â¿Z-ordering es equivalente a particionamiento? Â¿En quÃ© se diferencia?

3. Â¿OPTIMIZE tiene que ejecutarse periÃ³dicamente? Â¿CuÃ¡ndo se "deshace"?

4. Si tienes `region` con 4 valores y `fecha` con 365 valores,
   Â¿cuÃ¡ntas combinaciones hay y cÃ³mo afecta al Z-ordering?

5. Â¿CuÃ¡nto tiempo tarda OPTIMIZE sobre 1 TB de datos fragmentados?

**Pista:** Z-ordering vs particionamiento: el particionamiento crea directorios
separados por valor (`region=norte/`, `region=sur/`) â€” queries que filtran
por regiÃ³n solo leen el directorio correspondiente. Z-ordering organiza los datos
DENTRO de los archivos para que los registros similares (mismo region+fecha) estÃ©n
fÃ­sicamente cerca. Puedes combinar ambos: particionar por aÃ±o/mes y Z-ordenar
por region+user_id. El resultado: queries que filtran por aÃ±o/mes+region+user_id
son muy eficientes.

---

### Ejercicio 8.5.5 â€” Diagnosticar: el pipeline con demasiados archivos pequeÃ±os

**Tipo: Diagnosticar**

Un pipeline de streaming escribe 1 archivo Parquet por micro-batch en Delta Lake.
El micro-batch procesa 10,000 filas cada 5 minutos.
DespuÃ©s de 30 dÃ­as, el reporte diario de analytics tarda 3 horas:

```python
tabla = DeltaTable.forPath(spark, "s3://lakehouse/eventos/")
detalle = tabla.detail().collect()[0]

print(f"NÃºmero de archivos: {detalle['numFiles']}")
# 30 dÃ­as Ã— 24 horas Ã— 12 batches/hora = 8,640 archivos

print(f"TamaÃ±o promedio por archivo: {detalle['sizeInBytes'] / detalle['numFiles'] / 1024:.0f} KB")
# ~100 KB por archivo (!!)

print(f"TamaÃ±o total: {detalle['sizeInBytes'] / 1024**3:.1f} GB")
# ~860 MB de datos reales â€” pero 8,640 archivos de 100 KB cada uno
```

**Preguntas:**

1. Â¿Por quÃ© el reporte de analytics tarda 3 horas si solo hay 860 MB de datos?

2. Â¿CuÃ¡ntos archivos Parquet se esperan para 860 MB de datos bien empaquetados?

3. Â¿CÃ³mo arreglas el problema sin detener el pipeline de streaming?

4. Â¿CÃ³mo prevenirÃ­as el problema en el diseÃ±o inicial?

5. Â¿OPTIMIZE puede ejecutarse mientras el pipeline de streaming escribe activamente?

**Pista:** 8,640 archivos de 100 KB cada uno: cuando Spark lee esto,
crea 8,640 tasks de lectura (una por archivo). El overhead de scheduling
de 8,640 tasks (50ms cada una) = 7 minutos solo en scheduling.
MÃ¡s el overhead de S3 LIST para descubrir los archivos y el overhead de
abrir 8,640 archivos Parquet (cada uno tiene su propio footer que se lee
para obtener las estadÃ­sticas). La soluciÃ³n: OPTIMIZE periÃ³dico (cada hora o dÃ­a).
La prevenciÃ³n: usar `trigger(once=True)` con acumulaciÃ³n de mÃ¡s datos por batch,
o configurar el streaming job para hacer micro-batches menos frecuentes pero mÃ¡s grandes.

---

## SecciÃ³n 8.6 â€” Apache Iceberg: el Modelo de Ãrbol

### Ejercicio 8.6.1 â€” La estructura de metadata de Iceberg

```
Apache Iceberg organiza el metadata en un Ã¡rbol:

Table metadata file (JSON):
  {
    "format-version": 2,
    "table-uuid": "abc-123",
    "location": "s3://bucket/tabla/",
    "schemas": [schema_v1, schema_v2],
    "current-schema-id": 1,
    "partition-specs": [spec_v1, spec_v2],
    "current-spec-id": 1,
    "snapshots": [...],
    "current-snapshot-id": 9876543210
  }

Snapshot:
  {
    "snapshot-id": 9876543210,
    "timestamp-ms": 1705000000000,
    "manifest-list": "s3://bucket/tabla/metadata/snap-9876543210.avro"
  }

Manifest List (Avro):
  [{
    "manifest-path": "s3://bucket/tabla/metadata/manifest-abc.avro",
    "added-data-files-count": 5,
    "existing-data-files-count": 100,
    "deleted-data-files-count": 0,
  }]

Manifest File (Avro):
  [{
    "status": "ADDED",
    "data-file": {
      "file-path": "s3://bucket/tabla/data/0001.parquet",
      "record-count": 100000,
      "column-sizes": {...},
      "value-counts": {...},
      "lower-bounds": {"monto": 10.0},
      "upper-bounds": {"monto": 9999.0},
    }
  }]
```

**Preguntas:**

1. Â¿Por quÃ© Iceberg usa un Ã¡rbol de metadata en lugar del log lineal de Delta Lake?

2. Â¿CuÃ¡ntas lecturas de S3 necesita Iceberg para responder a la pregunta
   "Â¿quÃ© archivos contienen datos del mes de enero 2024?"

3. Â¿DÃ³nde estÃ¡n las estadÃ­sticas por columna en Iceberg?
   (min, max, null count)

4. Â¿QuÃ© ventaja tiene Iceberg sobre Delta Lake para tablas con miles
   de particiones?

5. Â¿El modelo de Ã¡rbol de Iceberg hace el time travel mÃ¡s rÃ¡pido o mÃ¡s
   lento que el log de Delta Lake?

**Pista:** La ventaja del Ã¡rbol de Iceberg para tablas grandes: Delta Lake
necesita leer todos los archivos JSON del log para construir el estado actual
(mitigado por los checkpoints). Iceberg apunta directamente al snapshot actual â€”
una lectura de la metadata file te da el snapshot, una lectura del manifest list
te da los manifests, y puedes hacer predicate pushdown sobre los manifest files
para saltarte los que no contienen datos relevantes. Para tablas con 100,000
particiones y 1M de archivos, Iceberg puede planificar la query en segundos;
Delta Lake necesita procesar el log completo (o el Ãºltimo checkpoint).

---

### Ejercicio 8.6.2 â€” Partition evolution en Iceberg

Una de las capacidades mÃ¡s valoradas de Iceberg:

```python
# pip install pyiceberg

from pyiceberg.catalog import load_catalog
from pyiceberg.schema import Schema
from pyiceberg.types import (NestedField, LongType, StringType, 
                              DoubleType, TimestampType)
from pyiceberg.partitioning import PartitionSpec, PartitionField
from pyiceberg.transforms import DayTransform, MonthTransform, IdentityTransform

# Crear tabla con particiÃ³n por dÃ­a:
catalog = load_catalog("local", **{"type": "sql",
                                    "uri": "sqlite:///catalog.db"})

schema = Schema(
    NestedField(1, "id", LongType()),
    NestedField(2, "timestamp", TimestampType()),
    NestedField(3, "region", StringType()),
    NestedField(4, "monto", DoubleType()),
)

# ParticiÃ³n inicial: por dÃ­a
spec_v1 = PartitionSpec(
    PartitionField(source_id=2, field_id=1000,
                   transform=DayTransform(), name="day")
)

tabla = catalog.create_table(
    identifier="ventas",
    schema=schema,
    partition_spec=spec_v1,
)

# ... escribir datos con particiÃ³n por dÃ­a ...

# Evolucionar la particiÃ³n a mes (sin reescribir datos existentes!):
with tabla.update_spec() as update:
    update.remove_field("day")
    update.add_identity("month")  # ahora particiona por mes

# Los datos VIEJOS siguen con particiÃ³n por dÃ­a
# Los datos NUEVOS se escribirÃ¡n con particiÃ³n por mes
# Las queries funcionan correctamente en ambos
```

**Preguntas:**

1. Â¿Por quÃ© la partition evolution de Iceberg es especialmente valiosa?
   Â¿QuÃ© problema resuelve que Delta Lake no puede resolver fÃ¡cilmente?

2. Si la tabla tiene datos con particiÃ³n por dÃ­a Y datos con particiÃ³n por mes,
   Â¿cÃ³mo planifica Iceberg una query que filtra por un rango de fechas?

3. Â¿CuÃ¡l es el "hidden partitioning" de Iceberg y por quÃ© simplifica
   los queries del usuario?

4. Â¿Delta Lake puede cambiar la particiÃ³n de una tabla existente?

**Pista:** El "hidden partitioning" de Iceberg: cuando usas
`PartitionField(source_id=timestamp, transform=DayTransform())`, Iceberg
genera automÃ¡ticamente el valor de particiÃ³n a partir del timestamp â€”
el usuario no necesita aÃ±adir una columna `date` calculada explÃ­citamente.
En Delta Lake, tienes que aÃ±adir una columna `date = date(timestamp)` y
usar `.partitionBy("date")`. En Iceberg, el usuario simplemente escribe el
timestamp y Iceberg gestiona la particiÃ³n internamente. Las queries tambiÃ©n
son mÃ¡s limpias: `WHERE timestamp BETWEEN '2024-01-01' AND '2024-01-31'`
automÃ¡ticamente hace predicate pushdown sobre las particiones de dÃ­a.

---

### Ejercicio 8.6.3 â€” Multi-engine: Iceberg con Spark, Trino y Polars

```python
# La misma tabla Iceberg accesible desde mÃºltiples engines:

# Desde PySpark:
from pyspark.sql import SparkSession
spark = SparkSession.builder \
    .config("spark.jars.packages",
            "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0") \
    .config("spark.sql.extensions",
            "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.mi_catalog",
            "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.mi_catalog.type", "hadoop") \
    .config("spark.sql.catalog.mi_catalog.warehouse", "s3://mi-bucket/iceberg/") \
    .getOrCreate()

spark.table("mi_catalog.default.ventas").show()

# Desde Python con PyIceberg (sin Spark):
from pyiceberg.catalog import load_catalog
catalog = load_catalog("mi_catalog", **{
    "type": "glue",  # AWS Glue como catalog
    "warehouse": "s3://mi-bucket/iceberg/",
})
tabla = catalog.load_table("default.ventas")

# Leer como Arrow y procesar con Polars:
import polars as pl
arrow_scan = tabla.scan(row_filter="region = 'norte'").to_arrow()
df_polars = pl.from_arrow(arrow_scan)
```

**Preguntas:**

1. Â¿QuÃ© es el "catalog" en Iceberg y por quÃ© es necesario?

2. Â¿CuÃ¡ntos catalogs soporta Iceberg? Â¿CuÃ¡les son los mÃ¡s comunes?

3. Â¿Delta Lake tambiÃ©n soporta mÃºltiples engines (Trino, DuckDB, Polars)?

4. Â¿QuÃ© es el "Open Table Format" y cÃ³mo se relaciona con Iceberg?

5. Â¿CuÃ¡ndo el soporte multi-engine de Iceberg es una ventaja real
   vs ser sobre-ingenierÃ­a?

> ðŸ”— Ecosistema: el catÃ¡logo Iceberg mÃ¡s comÃºn en producciÃ³n es:
> AWS Glue (para AWS), Hive Metastore (para Hadoop/on-prem),
> y Nessie (open-source, git-like versioning para tablas).
> Unity Catalog de Databricks soporta tanto Delta Lake como Iceberg.

---

### Ejercicio 8.6.4 â€” Row-level deletes en Iceberg v2

Iceberg v2 aÃ±ade soporte para deletes a nivel de fila sin reescribir archivos:

```
Iceberg v1 (Copy-on-Write como Delta Lake):
  DELETE de 1 fila â†’ reescribir el archivo completo
  Eficiente para lectura, costoso para DELETE frecuentes

Iceberg v2 (Merge-on-Read con delete files):
  DELETE de 1 fila â†’ escribir un "delete file" con la posiciÃ³n de la fila
  El archivo original NO se modifica
  Al leer: merge del archivo de datos + delete files
  Eficiente para DELETE frecuentes, mÃ¡s costoso para lectura

Tipo de delete files:
  - Position deletes: "eliminar fila en posiciÃ³n 1,234 del archivo X"
  - Equality deletes: "eliminar todas las filas donde id = 12345"
```

**Preguntas:**

1. Â¿CuÃ¡ndo es preferible Merge-on-Read (Iceberg v2) sobre Copy-on-Write (Delta Lake)?

2. Â¿El Merge-on-Read tiene impacto en el rendimiento de las queries de lectura?

3. Â¿Hay un comando equivalente al `OPTIMIZE` de Delta Lake en Iceberg para
   compactar los delete files?

4. Â¿Los delete files de Iceberg son compatibles con todos los engines
   (Spark, Trino, Flink)?

---

### Ejercicio 8.6.5 â€” Leer: cuÃ¡ndo elegir Iceberg sobre Delta Lake

**Tipo: Comparar**

Para cada criterio, determinar si Iceberg o Delta Lake es superior,
o si son equivalentes:

```
Criterio                          Iceberg    Delta Lake
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Partition evolution              âœ“ mejor    âœ— manual
Multi-engine por defecto         âœ“ mejor    âœ“ (mejorando)
Schema evolution                 âœ“          âœ“ (similar)
Madurez del ecosistema           âœ“ (+ aÃ±os) âœ“ (+ Databricks)
IntegraciÃ³n con Spark            âœ“          âœ“âœ“ (Delta es de Databricks)
Operaciones DML (UPDATE/MERGE)   âœ“          âœ“âœ“ (mejor implementado)
Time travel                      âœ“          âœ“ (similar)
GestiÃ³n de metadata a escala     âœ“ mejor    âœ“ (checkpoints)
Comet (aceleraciÃ³n Spark)        âœ— (solo Î´) âœ“
```

Completar la tabla y aÃ±adir 3 criterios propios.

---

## SecciÃ³n 8.7 â€” Comparativa y DecisiÃ³n: Delta Lake vs Iceberg vs Hudi

### Ejercicio 8.7.1 â€” Apache Hudi: el especialista en CDC

```python
# Apache Hudi: diseÃ±ado para CDC y workloads con muchos updates/deletes

# Dos storage types en Hudi:
# Copy-on-Write (CoW): como Delta Lake, actualiza copiando archivos
# Merge-on-Read (MoR): escribe deltas, merge al leer
#   - escritura mÃ¡s rÃ¡pida
#   - lectura mÃ¡s costosa (merge de base + deltas)

# En PySpark:
df_nuevos = spark.createDataFrame(
    [(1, "norte", 100.0), (2, "sur", 200.0)],
    ["id", "region", "monto"]
)

df_nuevos.write.format("hudi") \
    .option("hoodie.table.name", "mi_tabla_hudi") \
    .option("hoodie.datasource.write.recordkey.field", "id") \
    .option("hoodie.datasource.write.precombine.field", "timestamp") \
    .option("hoodie.datasource.write.operation", "upsert") \
    .option("hoodie.datasource.write.table.type", "MERGE_ON_READ") \
    .mode("append") \
    .save("/tmp/tabla_hudi/")
```

**Preguntas:**

1. Â¿CuÃ¡l es el caso de uso principal de Hudi que lo diferencia de Delta e Iceberg?

2. Â¿QuÃ© son los "base files" y "delta files" en Hudi MoR?

3. Â¿Hudi tiene algo equivalente al time travel de Delta Lake?

4. Â¿Por quÃ© Hudi es popular en Uber, LinkedIn, y otras empresas con
   workloads de streaming + updates frecuentes?

5. Â¿Hudi soporta mÃºltiples engines como Iceberg?

> âš™ï¸ VersiÃ³n: Hudi 0.14+ tiene cambios significativos en la API de Python.
> Las opciones de configuraciÃ³n (`hoodie.*`) han cambiado en versiones recientes.
> Para proyectos nuevos en 2024, verificar la documentaciÃ³n oficial de Apache Hudi.

---

### Ejercicio 8.7.2 â€” La matriz de decisiÃ³n final

**Tipo: Construir**

```
                     Delta Lake    Iceberg    Hudi
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Mejor para:
  Batch analytics     âœ“âœ“           âœ“          âœ“
  CDC/streaming       âœ“            âœ“          âœ“âœ“
  Multi-engine        âœ“            âœ“âœ“         âœ“
  Databricks          âœ“âœ“           âœ“          âœ“
  AWS Glue            âœ“            âœ“âœ“         âœ“
  Updates frecuentes  âœ“            âœ“          âœ“âœ“

Ecosistema:
  Maduro              âœ“âœ“           âœ“âœ“         âœ“
  Open source         âœ“            âœ“âœ“         âœ“âœ“
  Databricks-backed   âœ“âœ“           âœ“(neutral) âœ“

CaracterÃ­sticas:
  Partition evolution âœ—            âœ“âœ“         âœ“
  Row-level deletes   âœ“(CoW)       âœ“(v2 MoR)  âœ“âœ“(MoR)
  Schema evolution    âœ“âœ“           âœ“âœ“         âœ“
  Time travel         âœ“âœ“           âœ“âœ“         âœ“
  VACUUM/Compaction   âœ“âœ“           âœ“âœ“         âœ“âœ“
```

**Restricciones:**
1. Completar las celdas marcadas con `?`
2. Â¿Hay un "ganador claro" en alguna categorÃ­a?
3. Â¿La elecciÃ³n es siempre tÃ©cnica o tambiÃ©n polÃ­tica (vendor lock-in)?

---

### Ejercicio 8.7.3 â€” DiseÃ±ar el lakehouse del sistema de e-commerce

**Tipo: DiseÃ±ar**

Para el sistema de e-commerce del repositorio, diseÃ±ar el lakehouse completo:

```
Fuentes:
  - Eventos de click/vista/compra: 50M eventos/dÃ­a, streaming desde Kafka
  - CatÃ¡logo de productos: 1M productos, actualizaciÃ³n por batch desde PostgreSQL
  - Clientes: 10M usuarios, actualizaciones frecuentes (GDPR: derecho al olvido)
  - Inventario: 50K SKUs, actualizaciones en tiempo real (10K updates/hora)

Capas del lakehouse:
  Bronze: datos crudos tal como llegan (sin transformaciÃ³n)
  Silver: datos limpios y normalizados (join, dedup, enrich)
  Gold:   mÃ©tricas de negocio (revenue, conversiÃ³n, stock)

SLA:
  - Datos disponibles para BI en < 1 hora desde su ingesta
  - Historial de 2 aÃ±os para auditorÃ­a
  - Derecho al olvido (GDPR): eliminar datos de un usuario en < 24 horas
```

Para cada tabla y capa, especificar:
1. Â¿Delta Lake, Iceberg, o Hudi? Â¿Por quÃ©?
2. Estrategia de particionamiento
3. Operaciones DML necesarias (INSERT, UPDATE, MERGE, DELETE)
4. PolÃ­tica de retenciÃ³n y vacuum
5. Estrategia de schema evolution

---

### Ejercicio 8.7.4 â€” El problema del GDPR en el lakehouse

**Tipo: Implementar**

GDPR requiere que puedas eliminar todos los datos de un usuario en 24 horas.
En un lakehouse con 2 aÃ±os de historial y time travel, esto es un desafÃ­o:

```python
def eliminar_usuario_gdpr(
    spark: SparkSession,
    user_id: int,
    tablas: list[str],
) -> dict:
    """
    Elimina todos los datos de un usuario de todas las tablas del lakehouse.
    DesafÃ­o: el time travel mantiene versiones antiguas con esos datos.
    """
    resultados = {}

    for tabla in tablas:
        delta_tabla = DeltaTable.forPath(spark, tabla)

        # Paso 1: eliminar del estado actual
        filas_antes = spark.read.format("delta").load(tabla).count()
        delta_tabla.delete(f"user_id = {user_id}")
        filas_despues = spark.read.format("delta").load(tabla).count()

        resultados[tabla] = {
            "filas_eliminadas": filas_antes - filas_despues,
        }

        # Paso 2: eliminar el historial (time travel)
        # PROBLEMA: vacuum elimina archivos mÃ¡s viejos que el retention period
        # Si el retention period es 7 dÃ­as y el usuario tiene datos de hace 2 aÃ±os,
        # esos datos siguen accesibles via time travel hasta que vacuum los elimine

        # Â¿Hay una forma de eliminar inmediatamente sin esperar el vacuum?
        # ...

    return resultados
```

**Preguntas:**

1. Â¿Delta Lake puede eliminar datos de versiones histÃ³ricas (time travel)
   de forma inmediata?

2. Â¿VACUUM resuelve el problema del GDPR? Â¿Con quÃ© `retentionHours`?

3. Â¿Hay tensiÃ³n entre el time travel (retener historial) y el GDPR (eliminar datos)?
   Â¿CÃ³mo la resuelves en el diseÃ±o?

4. Â¿Delta Lake tiene alguna operaciÃ³n especÃ­fica para "borrado permanente"?
   Â¿Y Apache Iceberg?

5. Â¿QuÃ© tÃ©cnica criptogrÃ¡fica permite el "right to be forgotten" sin eliminar
   datos fÃ­sicamente?

**Pista:** El patrÃ³n criptogrÃ¡fico para GDPR con time travel: en lugar de
almacenar datos personales directamente, almacenar `encrypt(datos_personales, clave_usuario)`.
Para "olvidar" al usuario, eliminar su clave de cifrado â€” los datos cifrados
en el historial siguen existiendo pero son indescifrables sin la clave.
Este patrÃ³n se llama "crypto-shredding". Delta Lake Deletion Vectors (DV, v3.0+)
permiten marcar filas como eliminadas sin reescribir los archivos â€” similar
a Iceberg v2 equality deletes, pero la fila puede recuperarse con time travel
si el DV se elimina. Para eliminaciÃ³n verdaderamente permanente, VACUUM sigue
siendo necesario.

---

### Ejercicio 8.7.5 â€” El repositorio a mitad de camino: reflexiÃ³n

**Tipo: ReflexiÃ³n/integrar**

Llegamos al final de la Parte 2 (batch processing).
Los capÃ­tulos 04-08 cubrieron: Spark (modelo + optimizaciÃ³n), Polars, DataFusion,
y el Lakehouse.

Volviendo a la pregunta del Ejercicio 1.5.5 del Cap.01:

> *"El 80% de los problemas de performance son skew, shuffles, y formatos.
> Â¿EstÃ¡s de acuerdo?"*

Ahora que conoces Spark, Polars, DataFusion, y el Lakehouse:

1. Â¿CambiarÃ­a tu respuesta? Â¿AÃ±adirÃ­as o quitarÃ­as algo de la lista?

2. Â¿QuÃ© problema de los cinco capÃ­tulos te resultÃ³ mÃ¡s sorprendente?
   (algo que no esperabas que fuera asÃ­)

3. Â¿QuÃ© herramienta del batch stack (Spark, Polars, DataFusion, Delta Lake)
   usarÃ­as en tu trabajo actual? Â¿Por quÃ©?

4. La "cadena de causalidad" del Cap.03:
   ```
   Cap.01: framework controla el paralelismo
   Cap.02: formato determina el I/O
   Cap.03: Map/Reduce + shuffle es el modelo
   Cap.04: Spark implementa ese modelo
   Cap.05: optimizar = minimizar shuffles, evitar skew
   Cap.06: Polars evita el problema distribuido cuando los datos caben
   Cap.07: DataFusion es el motor embebible
   Cap.08: el Lakehouse aÃ±ade ACID y gestiÃ³n sobre el almacenamiento
   ```
   Â¿Esta cadena es coherente? Â¿QuÃ© eslabÃ³n falta?

5. La Parte 3 cubre streaming (Kafka, Beam, Spark Streaming, Flink).
   BasÃ¡ndote en lo aprendido en batch, Â¿quÃ© esperas que sea diferente
   en streaming? Â¿QuÃ© esperas que sea igual?

---

## Resumen del capÃ­tulo

**Las cuatro garantÃ­as del lakehouse y su costo:**

```
1. Atomicidad
   QuÃ© da: un write es todo-o-nada
   Costo: overhead de escritura en dos fases (archivos + commit al log)
   Sin esto: datos parciales despuÃ©s de un fallo

2. Consistencia (schema enforcement)
   QuÃ© da: nadie puede escribir datos incorrectos
   Costo: overhead de verificaciÃ³n en cada write + evoluciÃ³n mÃ¡s rÃ­gida
   Sin esto: corrupciÃ³n silenciosa de datos

3. Aislamiento (snapshot isolation)
   QuÃ© da: lectores no ven escrituras en progreso
   Costo: overhead de gestionar versiones concurrentes
   Sin esto: dirty reads (leer datos a medio escribir)

4. Durabilidad
   QuÃ© da: los datos sobreviven fallos del sistema
   Costo: dependencia de la durabilidad del almacenamiento subyacente (S3)
   Sin esto: perder datos en fallos de hardware
```

**La decisiÃ³n prÃ¡ctica en una oraciÃ³n por formato:**

```
Delta Lake: si estÃ¡s en Databricks o en el ecosistema Spark, y quieres
            la integraciÃ³n mÃ¡s profunda y el tooling mÃ¡s maduro.

Iceberg: si necesitas multi-engine sin vendor lock-in, o si necesitas
         partition evolution, o si usas AWS con Glue catalog.

Hudi: si tienes workloads con CDC/streaming y muchos updates frecuentes
      (Uber, LinkedIn style) y no te importa la mayor complejidad.
```

**Lo que conecta este capÃ­tulo con la Parte 3 (streaming):**

> El lakehouse que construimos en la Parte 2 es el destino de los pipelines
> de streaming de la Parte 3. Kafka (Cap.09) produce eventos. Flink (Cap.12)
> los procesa. Delta Lake / Iceberg es donde aterrizan.
> La Parte 3 cierra el ciclo: de la fuente de datos al lakehouse,
> pasando por el procesamiento en tiempo real.
