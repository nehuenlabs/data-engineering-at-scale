# GuÃ­a de Ejercicios â€” Cap.03: El Modelo Map/Reduce

> Este capÃ­tulo no es sobre Hadoop.
>
> Es sobre el paradigma que subyace a Spark, Beam, Flink, y Kafka Streams â€”
> y que apareciÃ³ dÃ©cadas antes que cualquiera de ellos.
> Entender Map/Reduce como abstracciÃ³n (no como tecnologÃ­a) es entender
> por quÃ© todos esos frameworks tienen la forma que tienen.
>
> Al final del capÃ­tulo, el cÃ³digo de Spark que escribes en el resto
> del repositorio dejarÃ¡ de parecer una API que hay que memorizar
> y empezarÃ¡ a parecer una consecuencia inevitable de un modelo matemÃ¡tico.

---

## La idea en una oraciÃ³n

Map/Reduce es la observaciÃ³n de que la mayorÃ­a de las computaciones
sobre grandes colecciones de datos puede expresarse como:

```
1. MAP:    transformar cada elemento independientemente
2. REDUCE: combinar los resultados de elementos relacionados
```

Y que esa separaciÃ³n tiene una propiedad extraordinaria:
el MAP puede ejecutarse en paralelo sin coordinaciÃ³n,
y el REDUCE puede ejecutarse en paralelo por grupos.

```
Sin Map/Reduce (procesamiento secuencial):
  [e1, e2, e3, ..., e1M] â†’ procesar uno a uno â†’ resultado
  Tiempo: proporcional a N

Con Map/Reduce (paralelo):
  [e1, e2, e3] â†’ map â†’ [r1, r2, r3] â†’ reduce â†’ resultado parcial 1
  [e4, e5, e6] â†’ map â†’ [r4, r5, r6] â†’ reduce â†’ resultado parcial 2
  [resultado parcial 1, resultado parcial 2] â†’ reduce final â†’ resultado
  Tiempo: proporcional a N/workers + log(workers) para el reduce final
```

El costo: el REDUCE requiere que todos los elementos del mismo grupo
estÃ©n juntos. Eso requiere comunicaciÃ³n entre workers. En sistemas
distribuidos, esa comunicaciÃ³n se llama **shuffle** â€” el concepto
mÃ¡s importante del Cap.04.

---

## El origen: el paper de Google (2004)

El modelo Map/Reduce fue descrito por Jeffrey Dean y Sanjay Ghemawat
en el paper *MapReduce: Simplified Data Processing on Large Clusters* (2004).
El paper describe cÃ³mo Google procesaba cientos de terabytes de datos
para construir el Ã­ndice web.

La observaciÃ³n central del paper:

> *"Most of our computations involve applying a map operation to each logical
> record in our input in order to compute a set of intermediate key/value pairs,
> and then applying a reduce operation to all values that shared the same key."*

> ðŸ“– Profundizar: el paper original (Dean & Ghemawat, OSDI 2004) tiene 13 pÃ¡ginas
> y es uno de los papers mÃ¡s leÃ­dos en sistemas distribuidos. La SecciÃ³n 3
> (Implementation) explica el shuffle en detalle. Disponible en research.google.com.

---

## Tabla de contenidos

- [SecciÃ³n 3.1 â€” Map: transformaciÃ³n independiente](#secciÃ³n-31--map-transformaciÃ³n-independiente)
- [SecciÃ³n 3.2 â€” Reduce: combinaciÃ³n por clave](#secciÃ³n-32--reduce-combinaciÃ³n-por-clave)
- [SecciÃ³n 3.3 â€” El shuffle: el costo de la coordinaciÃ³n](#secciÃ³n-33--el-shuffle-el-costo-de-la-coordinaciÃ³n)
- [SecciÃ³n 3.4 â€” Combiners: optimizar antes del shuffle](#secciÃ³n-34--combiners-optimizar-antes-del-shuffle)
- [SecciÃ³n 3.5 â€” MÃ¡s allÃ¡ de Map/Reduce: el modelo generalizado](#secciÃ³n-35--mÃ¡s-allÃ¡-de-mapreduce-el-modelo-generalizado)
- [SecciÃ³n 3.6 â€” Map/Reduce en los frameworks modernos](#secciÃ³n-36--mapreduce-en-los-frameworks-modernos)
- [SecciÃ³n 3.7 â€” CuÃ¡ndo Map/Reduce no es suficiente](#secciÃ³n-37--cuÃ¡ndo-mapreduce-no-es-suficiente)

---

## SecciÃ³n 3.1 â€” Map: TransformaciÃ³n Independiente

### Ejercicio 3.1.1 â€” La propiedad fundamental del map

**Tipo: Leer/analizar**

El **map** tiene una propiedad que lo hace trivialmente paralelizable:
cada elemento se transforma **sin depender de ningÃºn otro elemento**.

```python
# Map: aplicar una funciÃ³n a cada elemento independientemente
def map_fn(elemento):
    return elemento * 2

datos = [1, 2, 3, 4, 5, 6, 7, 8]

# Secuencial:
resultado_seq = [map_fn(x) for x in datos]
# â†’ [2, 4, 6, 8, 10, 12, 14, 16]

# Paralelo â€” mismo resultado, cualquier orden de ejecuciÃ³n:
# Worker 1: map_fn(1), map_fn(2) â†’ [2, 4]
# Worker 2: map_fn(3), map_fn(4) â†’ [6, 8]
# Worker 3: map_fn(5), map_fn(6) â†’ [10, 12]
# Worker 4: map_fn(7), map_fn(8) â†’ [14, 16]
# Resultado combinado: [2, 4, 6, 8, 10, 12, 14, 16]
```

La propiedad clave: `map_fn(3)` no necesita saber el resultado de `map_fn(1)`.
Cada aplicaciÃ³n es completamente independiente.

**Preguntas:**

1. Â¿CuÃ¡l de las siguientes funciones puede usarse como `map_fn`
   en un map paralelo? Â¿CuÃ¡l no puede y por quÃ©?

```python
# OpciÃ³n A:
def normalizar(precio):
    return precio / max_precio_global  # max_precio_global es una variable externa

# OpciÃ³n B:
def calcular_descuento(transaccion):
    return transaccion["monto"] * 0.1

# OpciÃ³n C:
contador_global = 0
def contar_y_transformar(elemento):
    global contador_global
    contador_global += 1  # modifica estado global
    return elemento * contador_global

# OpciÃ³n D:
historial = []
def con_historial(elemento):
    historial.append(elemento)
    return sum(historial) / len(historial)  # promedio acumulado

# OpciÃ³n E:
cache = {}
def con_cache(elemento):
    if elemento not in cache:
        cache[elemento] = calcular_costoso(elemento)
    return cache[elemento]
```

2. La OpciÃ³n A accede a `max_precio_global`. Â¿CuÃ¡ndo esto es seguro
   en un map paralelo y cuÃ¡ndo no lo es?

3. La OpciÃ³n E usa una cachÃ© compartida. Â¿QuÃ© problema tiene en un
   entorno distribuido donde cada worker tiene su propio proceso?

4. Â¿QuÃ© significa que una funciÃ³n de map sea **pura**?
   Â¿CuÃ¡l de las opciones anteriores es pura?

5. En Spark, `df.withColumn("nuevo", F.col("monto") * 0.1)` es un map.
   Â¿Por quÃ© Spark puede ejecutarlo en paralelo sin coordinaciÃ³n?

**Pista:** La OpciÃ³n E es segura en un solo proceso (la cachÃ© se comparte
entre threads del mismo proceso) pero no en un sistema distribuido donde
cada worker tiene su propia memoria. En Spark, la cachÃ© de la OpciÃ³n E
existirÃ­a de forma independiente en cada executor â€” lo que significa que
cada executor recalcularÃ­a `calcular_costoso` para los elementos que procesa,
sin beneficiarse de los cÃ¡lculos de otros executors.
La soluciÃ³n distribuida: broadcast variable (Cap.04 Â§4.4).

---

### Ejercicio 3.1.2 â€” Implementar map paralelo desde cero

```python
from multiprocessing import Pool
from typing import Callable, TypeVar, Iterator
import time

T = TypeVar("T")
R = TypeVar("R")

def map_paralelo(
    datos: list[T],
    fn: Callable[[T], R],
    num_workers: int = 4,
    tamaÃ±o_chunk: int = None,
) -> list[R]:
    """
    Aplica fn a cada elemento de datos en paralelo.
    
    Args:
        datos: la colecciÃ³n de entrada
        fn: funciÃ³n pura a aplicar a cada elemento
        num_workers: nÃºmero de procesos paralelos
        tamaÃ±o_chunk: cuÃ¡ntos elementos por worker en cada batch
                      (None = dividir uniformemente)
    """
    if tamaÃ±o_chunk is None:
        tamaÃ±o_chunk = max(1, len(datos) // num_workers)

    with Pool(processes=num_workers) as pool:
        # imap_unordered: mÃ¡s eficiente pero no preserva el orden
        # imap: preserva el orden, ligeramente menos eficiente
        resultado = list(pool.imap(fn, datos, chunksize=tamaÃ±o_chunk))

    return resultado

# Comparar tiempos:
def calcular_costoso(n: int) -> float:
    """Simula una operaciÃ³n costosa (en producciÃ³n: llamada a una API, cÃ¡lculo ML)."""
    time.sleep(0.001)  # 1ms por elemento
    return n ** 2

datos = list(range(1000))

inicio = time.perf_counter()
resultado_seq = [calcular_costoso(x) for x in datos]
tiempo_seq = time.perf_counter() - inicio

inicio = time.perf_counter()
resultado_par = map_paralelo(datos, calcular_costoso, num_workers=8)
tiempo_par = time.perf_counter() - inicio

print(f"Secuencial: {tiempo_seq:.2f}s")
print(f"Paralelo (8 workers): {tiempo_par:.2f}s")
print(f"Speedup: {tiempo_seq/tiempo_par:.1f}Ã—")
```

**Restricciones:**
1. Ejecutar y medir el speedup real vs el teÃ³rico (8 workers â†’ 8Ã— speedup ideal)
2. Â¿Por quÃ© el speedup real es menor que 8Ã—? Identificar las fuentes de overhead
3. Medir cÃ³mo cambia el speedup con `tamaÃ±o_chunk = 1, 10, 100, 1000`
4. Implementar una versiÃ³n con `concurrent.futures.ThreadPoolExecutor`
   y comparar con `multiprocessing.Pool`. Â¿CuÃ¡ndo threads > procesos para map?

**Pista:** Las fuentes de overhead en map paralelo:
(1) serializaciÃ³n de datos y resultados entre procesos (pickle),
(2) overhead de scheduling (crear y comunicarse con procesos),
(3) si `calcular_costoso` libera el GIL, threads pueden ser suficientes;
si no (cÃ³digo Python puro), necesitas procesos.
El `tamaÃ±o_chunk` grande reduce el overhead de serializaciÃ³n por elemento
pero puede crear desbalance si los elementos tardan distinto tiempo.

---

### Ejercicio 3.1.3 â€” FlatMap: cuando un elemento produce muchos

El map produce exactamente un resultado por elemento.
El **flatMap** produce cero, uno, o muchos resultados por elemento:

```python
# Map: 1 entrada â†’ 1 salida
["hola mundo", "foo bar"] â†’ map(split) â†’ [["hola", "mundo"], ["foo", "bar"]]

# FlatMap: 1 entrada â†’ N salidas (aplana el resultado)
["hola mundo", "foo bar"] â†’ flatMap(split) â†’ ["hola", "mundo", "foo", "bar"]
```

```python
from itertools import chain

def flat_map(datos: list, fn: Callable) -> list:
    """Aplica fn y aplana el resultado."""
    return list(chain.from_iterable(fn(x) for x in datos))

# Ejemplos de uso:
oraciones = [
    "el gato come pescado",
    "el perro come carne",
    "el gato bebe leche",
]

# Extraer palabras:
palabras = flat_map(oraciones, str.split)
# â†’ ["el", "gato", "come", "pescado", "el", "perro", ...]

# Generar pares (palabra, 1) para contar:
pares = flat_map(oraciones, lambda oracion: [(w, 1) for w in oracion.split()])
# â†’ [("el", 1), ("gato", 1), ("come", 1), ("pescado", 1), ...]
```

**Preguntas:**

1. En Spark, `df.select(F.explode(F.col("items")))` es un flatMap.
   Â¿CuÃ¡ndo necesitas explode en lugar de withColumn?

2. Â¿Un flatMap puede reducir el nÃºmero de elementos? Da un ejemplo.

3. Â¿Un flatMap puede producir 0 elementos para algunos inputs?
   Â¿CuÃ¡ndo es esto Ãºtil?

4. Â¿La propiedad de independencia del map se conserva en el flatMap?

5. En el ejemplo de generaciÃ³n de pares `(palabra, 1)`:
   Â¿ves el comienzo del algoritmo de wordcount?
   Â¿QuÃ© operaciÃ³n vendrÃ­a despuÃ©s para completarlo?

**Pista:** El flatMap que produce 0 elementos es equivalente a un filter â€”
para las entradas que quieres eliminar, retorna una lista vacÃ­a.
Muchos frameworks lo usan para combinar filter + transform en una sola
operaciÃ³n eficiente: "para cada elemento, si cumple la condiciÃ³n retorna
el elemento transformado, si no retorna vacÃ­o".
En Spark: `df.select(F.explode_outer(col))` vs `F.explode(col)` â€”
`explode` elimina filas con arrays vacÃ­os/null, `explode_outer` las conserva
como null.

---

### Ejercicio 3.1.4 â€” Map en el contexto de Spark: transformaciones narrow

En Spark, las transformaciones de tipo map son "narrow transformations":
cada particiÃ³n de output depende de exactamente una particiÃ³n de input.

```python
from pyspark.sql import SparkSession, functions as F

spark = SparkSession.builder.getOrCreate()

df = spark.read.parquet("transacciones.parquet")

# Todas estas son narrow transformations (map-like):
df_transformado = (df
    .withColumn("monto_con_iva", F.col("monto") * 1.19)          # map
    .withColumn("region_upper", F.upper(F.col("region")))         # map
    .filter(F.col("monto") > 100)                                  # filter
    .select("id", "monto_con_iva", "region_upper")                 # projection
    .withColumn("categoria",
        F.when(F.col("monto_con_iva") > 1000, "premium")
         .otherwise("standard"))                                   # map condicional
)

# Estas transformaciones NO crean un shuffle:
# Cada particiÃ³n de df_transformado se calcula independientemente
# de las otras particiones â†’ paralelismo perfecto
df_transformado.explain()
```

**Preguntas:**

1. Â¿Por quÃ© las narrow transformations no crean shuffles?

2. Â¿CuÃ¡ntas veces se lee el archivo de Parquet si ejecutas:
   ```python
   df_a = df.filter(F.col("monto") > 100)
   df_b = df.filter(F.col("region") == "norte")
   df_a.count()
   df_b.count()
   ```

3. Spark agrupa mÃºltiples narrow transformations en un solo "stage".
   Â¿CuÃ¡ntos stages tiene el plan del pipeline anterior?

4. Â¿QuÃ© es el "pipeline fusion" y cÃ³mo lo aprovechan Spark y Polars?

5. Si una funciÃ³n de map tarda 10ms por fila y tienes 100M filas con 100 cores,
   Â¿cuÃ¡nto tarda la operaciÃ³n? Â¿QuÃ© supuesto estÃ¡s haciendo?

**Pista:** Pipeline fusion: en lugar de materializar el resultado de cada
transformaciÃ³n en memoria, Spark encadena mÃºltiples transformaciones en
una sola pasada sobre los datos. En el pipeline anterior, la secuencia
`withColumn â†’ filter â†’ select â†’ withColumn` se ejecuta como una sola
funciÃ³n por fila, sin guardar resultados intermedios. En Polars esto se llama
"lazy evaluation" â€” el plan se optimiza antes de ejecutarse.

---

### Ejercicio 3.1.5 â€” Leer: el map que es mÃ¡s lento con mÃ¡s workers

**Tipo: Diagnosticar**

Un equipo experimenta con paralelismo para acelerar un map costoso:

```
Experimento: aplicar un modelo de ML a 1 millÃ³n de imÃ¡genes.
Cada imagen tarda ~50ms en procesarse.

Resultados:
  1 worker:    50,000s (13.9 horas) â€” baseline
  2 workers:  25,100s (expected: 25,000s) âœ“
  4 workers:  12,580s (expected: 12,500s) âœ“
  8 workers:  6,420s  (expected: 6,250s)  âœ“
  16 workers: 4,100s  (expected: 3,125s)  âœ— â† solo 1.56Ã— speedup vs 2Ã—
  32 workers: 4,350s  (expected: 1,563s)  âœ— â† Â¡mÃ¡s lento que 16!
  64 workers: 5,200s  (expected:   781s)  âœ— â† aÃºn mÃ¡s lento
```

**Preguntas:**

1. Â¿Por quÃ© el speedup es casi perfecto hasta 8 workers pero colapsa a partir de 16?

2. Â¿Por quÃ© 32 workers es mÃ¡s lento que 16?

3. Â¿QuÃ© recursos fÃ­sicos estÃ¡n siendo el cuello de botella a partir de 16 workers?

4. Â¿El modelo de ML es relevante para diagnosticar este problema?

5. PropÃ³n cÃ³mo encontrar el nÃºmero Ã³ptimo de workers para este workload
   de forma experimental.

**Pista:** A partir de cierto punto, aÃ±adir mÃ¡s workers no aumenta el throughput
porque hay un recurso compartido que se satura. Las causas mÃ¡s frecuentes:
(1) ancho de banda de I/O â€” si las imÃ¡genes estÃ¡n en disco y 16+ workers
compiten por leer, el I/O se satura antes que la CPU,
(2) memoria â€” cada worker necesita cargar el modelo de ML (~GB), 16 workers
pueden saturar la RAM disponible causando swap,
(3) la GPU â€” si el modelo usa GPU, solo hay N GPUs disponibles.
La forma de distinguir: medir el uso de CPU, memoria, I/O, y GPU durante
el experimento con cada configuraciÃ³n de workers.

---

## SecciÃ³n 3.2 â€” Reduce: CombinaciÃ³n por Clave

### Ejercicio 3.2.1 â€” La estructura del reduce

El **reduce** combina mÃºltiples valores en uno, respetando una clave de agrupaciÃ³n:

```python
from collections import defaultdict
from typing import Callable

def map_reduce_simple(
    datos: list,
    map_fn: Callable,           # elemento â†’ (clave, valor)
    reduce_fn: Callable,        # (valor_acumulado, valor_nuevo) â†’ valor_acumulado
    valor_inicial,              # el valor de partida del acumulador
) -> dict:
    """
    ImplementaciÃ³n simple de Map/Reduce en un solo proceso.
    """
    # Fase MAP: transformar cada elemento en (clave, valor)
    pares_clave_valor = [map_fn(elemento) for elemento in datos]
    
    # Fase SHUFFLE (implÃ­cita): agrupar por clave
    agrupados = defaultdict(list)
    for clave, valor in pares_clave_valor:
        agrupados[clave].append(valor)
    
    # Fase REDUCE: combinar los valores de cada clave
    resultado = {}
    for clave, valores in agrupados.items():
        acumulado = valor_inicial
        for valor in valores:
            acumulado = reduce_fn(acumulado, valor)
        resultado[clave] = acumulado
    
    return resultado

# Wordcount clÃ¡sico:
oraciones = [
    "el gato come pescado y el perro come carne",
    "el gato bebe leche y el perro bebe agua",
    "el gato y el perro son amigos",
]

conteo = map_reduce_simple(
    datos=oraciones,
    map_fn=lambda oracion: [(palabra, 1) for palabra in oracion.split()],
    reduce_fn=lambda acumulado, nuevo: acumulado + nuevo,
    valor_inicial=0,
)
# Pero map_fn retorna una lista â†’ necesitamos flatMap, no map
```

**Restricciones:**
1. Arreglar `map_reduce_simple` para que use flatMap en lugar de map
2. Implementar wordcount usando la funciÃ³n corregida
3. Implementar "suma de montos por regiÃ³n" para una lista de transacciones
4. Implementar "monto mÃ¡ximo por regiÃ³n"
5. Â¿QuÃ© tiene en comÃºn la estructura de los tres ejercicios?

**Pista:** La correcciÃ³n para flatMap:
```python
pares_clave_valor = []
for elemento in datos:
    pares_clave_valor.extend(map_fn(elemento))
```
O mÃ¡s elegante: `list(chain.from_iterable(map_fn(e) for e in datos))`.
Lo que tienen en comÃºn los tres ejercicios: todos son `(clave, valor)` â†’
agrupar por clave â†’ aplicar una funciÃ³n de reducciÃ³n a los valores.
La diferencia estÃ¡ solo en la funciÃ³n de reducciÃ³n: `+` para suma, `max` para mÃ¡ximo.

---

### Ejercicio 3.2.2 â€” La propiedad de asociatividad en el reduce

Para que el reduce pueda ejecutarse en paralelo, la funciÃ³n de reducciÃ³n
debe ser **asociativa** y, idealmente, **conmutativa**:

```
Asociativa:  reduce(reduce(a, b), c) == reduce(a, reduce(b, c))
Conmutativa: reduce(a, b) == reduce(b, a)

Si es asociativa: puedo reducir en Ã¡rbol (paralelo)
Si tambiÃ©n es conmutativa: puedo reducir en cualquier orden
```

```
Reduce en Ã¡rbol (paralelo):
  Nivel 0: [1, 2, 3, 4, 5, 6, 7, 8]
  Nivel 1: [sum(1,2), sum(3,4), sum(5,6), sum(7,8)] = [3, 7, 11, 15]
  Nivel 2: [sum(3,7), sum(11,15)]                   = [10, 26]
  Nivel 3: sum(10, 26)                               = 36
  
  Profundidad: log2(8) = 3 niveles en paralelo
  vs
  Reduce secuencial: 7 pasos
```

**Preguntas:**

1. Para cada funciÃ³n, determinar si es asociativa, conmutativa, o ambas:
   - `suma: (a, b) â†’ a + b`
   - `max: (a, b) â†’ max(a, b)`
   - `concatenar: (a, b) â†’ a + b` (para strings)
   - `promedio: (a, b) â†’ (a + b) / 2`
   - `append: (lista, elem) â†’ lista + [elem]`
   - `merge_dict: (d1, d2) â†’ {**d1, **d2}`

2. Â¿Por quÃ© el **promedio** no puede reducirse directamente en paralelo?
   Â¿CÃ³mo lo resolverÃ­as?

3. Implementar reduce en Ã¡rbol paralelo:
```python
def reduce_arbol(datos: list, fn: Callable, num_workers: int = 4) -> any:
    """
    Reduce en Ã¡rbol usando mÃºltiples workers.
    Requiere que fn sea asociativa.
    """
    # TODO: implementar
    # Hint: dividir datos en chunks, reducir cada chunk en paralelo,
    # luego reducir los resultados parciales
    pass
```

4. Â¿Para quÃ© tamaÃ±o de datos el reduce en Ã¡rbol es mÃ¡s rÃ¡pido que el secuencial?

**Pista:** El promedio no es asociativo: `avg(avg(1,2), 3) = avg(1.5, 3) = 2.25`,
pero `avg(1, avg(2,3)) = avg(1, 2.5) = 1.75`. La soluciÃ³n: reducir por suma
y count por separado (ambos sÃ­ son asociativos), luego dividir al final:
`sum([1,2,3]) / count([1,2,3]) = 6/3 = 2`. Esto es exactamente lo que hace
Spark cuando calculas `F.mean()` â€” internamente reduce `(sum, count)` tuplas.

---

### Ejercicio 3.2.3 â€” GroupByKey vs ReduceByKey: el tradeoff crucial

Este ejercicio introduce el concepto mÃ¡s importante antes del shuffle.

```python
from pyspark.sql import SparkSession
from pyspark import RDD

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

# Dataset: (regiÃ³n, monto) para 100M transacciones
transacciones_rdd = sc.parallelize([
    ("norte", 100.0), ("sur", 200.0), ("norte", 150.0),
    ("este", 50.0), ("norte", 300.0), ("sur", 75.0),
    # ... 100M elementos mÃ¡s
])

# OpciÃ³n A: groupByKey â†’ suma manual
# PROBLEMA: mueve TODOS los valores al reducer antes de reducir
suma_a = (transacciones_rdd
    .groupByKey()           # todos los valores de "norte" van al mismo nodo
    .mapValues(sum)         # luego suma
)

# OpciÃ³n B: reduceByKey â†’ combina localmente ANTES del shuffle
# MEJOR: combina los valores en cada worker antes de moverlos
suma_b = (transacciones_rdd
    .reduceByKey(lambda a, b: a + b)  # suma parcial en cada worker primero
)

# Ambas producen el mismo resultado:
# [("norte", 550.0), ("sur", 275.0), ("este", 50.0)]
```

```
groupByKey en un cluster de 3 workers:

Worker 1: ("norte", 100), ("norte", 150) â†’ shuffle "norte" â†’ Worker A
Worker 2: ("norte", 300), ("sur", 200)   â†’ shuffle "norte" â†’ Worker A,
                                                              "sur"   â†’ Worker B
Worker 3: ("sur", 75), ("este", 50)      â†’ shuffle "sur"   â†’ Worker B,
                                                              "este"  â†’ Worker C

Worker A recibe: [100, 150, 300] â†’ sum â†’ 550
Worker B recibe: [200, 75]       â†’ sum â†’ 275
Worker C recibe: [50]            â†’ sum â†’ 50

Total de datos shuffleados: 6 valores Ã— 8 bytes = 48 bytes

reduceByKey en el mismo cluster:

Worker 1: ("norte", 100+150=250) â†’ shuffle â†’ Worker A
Worker 2: ("norte", 300), ("sur", 200) â†’ reduce local â†’
          ("norte", 300), ("sur", 200) â†’ shuffle â†’ Workers A, B
Worker 3: ("sur", 75+0=75), ("este", 50) â†’ shuffle â†’ Workers B, C

Worker A recibe: [250, 300] â†’ sum â†’ 550
Worker B recibe: [200, 75]  â†’ sum â†’ 275
Worker C recibe: [50]       â†’ sum â†’ 50

Total de datos shuffleados: 4 valores Ã— 8 bytes = 32 bytes
(con mÃ¡s datos, la diferencia es mucho mayor)
```

**Restricciones:**
1. Medir el tiempo de `groupByKey` vs `reduceByKey` para 10M pares
2. Medir los bytes de shuffle en Spark UI para cada operaciÃ³n
3. Â¿La diferencia de rendimiento aumenta o disminuye con mÃ¡s datos?
4. Â¿CuÃ¡ndo `groupByKey` es inevitable? (es decir, cuando `reduceByKey` no funciona)

**Pista:** `groupByKey` es inevitable cuando necesitas acceder a todos los valores
del grupo simultÃ¡neamente y no puedes expresar la operaciÃ³n como un reduce asociativo.
Por ejemplo: calcular la mediana (necesitas ordenar todos los valores),
encontrar el top-N de cada grupo, o calcular percentiles exactos.
Para estas operaciones, el shuffle de todos los valores es necesario â€”
no hay "combinaciÃ³n previa" posible.

---

### Ejercicio 3.2.4 â€” Implementar reduce distribuido con multiprocessing

```python
from multiprocessing import Pool
from collections import defaultdict
from typing import Callable, TypeVar
import itertools

K = TypeVar("K")
V = TypeVar("V")

def map_reduce_distribuido(
    datos: list,
    map_fn: Callable,       # elemento â†’ list[(clave, valor)]
    reduce_fn: Callable,    # (acumulado, valor) â†’ acumulado
    valor_inicial,
    num_workers: int = 4,
) -> dict:
    """
    Map/Reduce distribuido con mÃºltiples procesos.
    Incluye un combiner local (pre-reduce) antes del shuffle.
    """
    tamaÃ±o_chunk = max(1, len(datos) // num_workers)
    chunks = [datos[i:i+tamaÃ±o_chunk] for i in range(0, len(datos), tamaÃ±o_chunk)]

    def map_y_combinar_local(chunk: list) -> dict:
        """
        MAP + COMBINER: aplicar map y combinar localmente antes del shuffle.
        Equivale al 'partial reduce' de Hadoop / combiner de Spark.
        """
        pares = list(itertools.chain.from_iterable(map_fn(e) for e in chunk))
        resultado_local = defaultdict(lambda: valor_inicial)
        for clave, valor in pares:
            resultado_local[clave] = reduce_fn(resultado_local[clave], valor)
        return dict(resultado_local)

    # Fase MAP + COMBINER (paralela):
    with Pool(processes=num_workers) as pool:
        resultados_parciales = pool.map(map_y_combinar_local, chunks)

    # Fase SHUFFLE + REDUCE FINAL (secuencial en este ejemplo simplificado):
    resultado_final = defaultdict(lambda: valor_inicial)
    for parcial in resultados_parciales:
        for clave, valor in parcial.items():
            resultado_final[clave] = reduce_fn(resultado_final[clave], valor)

    return dict(resultado_final)

# Wordcount:
textos = [f"el gato y el perro en el barrio {i}" for i in range(100_000)]

resultado = map_reduce_distribuido(
    datos=textos,
    map_fn=lambda texto: [(palabra, 1) for palabra in texto.split()],
    reduce_fn=lambda a, b: a + b,
    valor_inicial=0,
    num_workers=8,
)
```

**Restricciones:**
1. Ejecutar y verificar la correcciÃ³n del resultado
2. Comparar el tiempo con y sin el combiner local
3. Medir cuÃ¡ntos datos se "shufflean" (pasan de los workers al reduce final)
   con y sin combiner para 100,000 textos
4. Â¿La funciÃ³n `reduce_fn` que usas aquÃ­ debe ser asociativa?
   Â¿Y conmutativa? Â¿Por quÃ©?

---

### Ejercicio 3.2.5 â€” Leer: diagnosticar un reduce incorrecto

**Tipo: Diagnosticar**

Un pipeline calcula el precio promedio de productos por categorÃ­a.
El resultado parece incorrecto â€” el promedio de "electrÃ³nico" es 450.0
pero el equipo de negocio dice que deberÃ­a ser alrededor de 320.0.

```python
# El cÃ³digo:
promedios = (transacciones_rdd
    .map(lambda t: (t["categoria"], t["precio"]))
    .groupByKey()
    .mapValues(lambda precios: sum(precios) / len(list(precios)))
)

# Resultado:
# [("electrÃ³nico", 450.0), ("ropa", 85.0), ("hogar", 120.0)]
```

Al investigar, se encuentra que:
- Los datos estÃ¡n particionados en 5 archivos
- El archivo 3 tiene el 60% de los productos electrÃ³nicos mÃ¡s caros
- Los archivos 1, 2, 4, 5 tienen el 40% mÃ¡s barato

**Preguntas:**

1. Â¿El cÃ³digo tiene un bug? Â¿El resultado de 450.0 es correcto
   dado el cÃ³digo, o hay un error en la implementaciÃ³n?

2. Si el resultado es correcto dado el cÃ³digo, Â¿por quÃ© no coincide
   con la expectativa del equipo de negocio?

3. Â¿PodrÃ­a haber un problema si `list(precios)` se consume mÃ¡s de una vez?

4. Â¿El particionamiento de los datos (60% en un archivo) afecta
   el resultado del promedio? Â¿DeberÃ­a afectarlo?

5. PropÃ³n la correcciÃ³n si el resultado es incorrecto, o una explicaciÃ³n
   para el equipo de negocio si es correcto.

**Pista:** `groupByKey().mapValues(lambda precios: sum(precios) / len(list(precios)))`
es correcto matemÃ¡ticamente â€” agrupa todos los precios de cada categorÃ­a y
calcula el promedio. Si el resultado es 450.0 y el equipo espera 320.0,
la discrepancia puede ser que el equipo estÃ¡ excluyendo ciertos productos
(por ejemplo, productos con descuento o productos sin stock) que el pipeline
incluye. El particionamiento no afecta el resultado del promedio porque
`groupByKey` mueve todos los valores al mismo reducer, independientemente
de cÃ³mo estÃ©n particionados en el origen.

---

## SecciÃ³n 3.3 â€” El Shuffle: el Costo de la CoordinaciÃ³n

### Ejercicio 3.3.1 â€” Por quÃ© el shuffle es inevitable

**Tipo: Leer/analizar**

El shuffle es la operaciÃ³n mÃ¡s costosa en Map/Reduce â€” y es inevitablemente
necesaria para cualquier operaciÃ³n que requiere ver mÃºltiples elementos relacionados.

```
Sin shuffle: cada elemento se procesa independientemente
  â†’ solo operaciones de map son posibles
  â†’ no puedes calcular sumas, promedios, joins, ordenamientos

Con shuffle: elementos relacionados se mueven al mismo nodo
  â†’ puedes calcular cualquier aggregation, join, sort
  â†’ costo: serializar datos, moverlos por la red, deserializar

El shuffle tiene tres fases:

Fase 1 â€” MAP OUTPUT / SPILL:
  Cada mapper escribe sus (clave, valor) pares ordenados por clave
  en archivos locales. Si los datos no caben en memoria â†’ spill to disk.
  
Fase 2 â€” FETCH / COPY:
  Cada reducer contacta a todos los mappers para leer sus datos.
  Todos los valores de la clave "norte" vienen de todos los mappers.
  â†’ TrÃ¡fico de red proporcional a los datos shuffleados
  
Fase 3 â€” MERGE / SORT:
  El reducer ordena los datos recibidos por clave y los combina.
  Luego aplica la funciÃ³n de reduce.
```

**Preguntas:**

1. Para calcular `COUNT(*)` (contar todas las filas), Â¿es necesario un shuffle?

2. Para calcular `COUNT(*) GROUP BY region`, Â¿es necesario un shuffle?
   Â¿QuÃ© datos se mueven exactamente?

3. Para un `JOIN` entre dos tablas de 1 TB cada una,
   Â¿cuÃ¡ntos bytes se shufflean en el peor caso?

4. Â¿QuÃ© operaciones de Spark NO requieren shuffle (son narrow transformations)?
   Â¿QuÃ© operaciones SÃ lo requieren (wide transformations)?

5. Si el shuffle de 100 GB tarda 10 minutos en una red de 10 Gbps,
   Â¿cuÃ¡nto tarda en una red de 1 Gbps? Â¿Y si el shuffle es de 10 GB?

**Pista:** Para la pregunta 1: `COUNT(*)` sin GROUP BY puede ejecutarse
como un map (cada nodo cuenta sus filas) seguido de un reduce trivial
(sumar los conteos). El shuffle solo mueve los conteos parciales â€”
un nÃºmero por nodo, no los datos completos. Si tienes 100 nodos,
el shuffle mueve 100 enteros, no 1 TB de datos.

---

### Ejercicio 3.3.2 â€” Implementar shuffle simplificado

```python
from collections import defaultdict
from typing import Callable
import hashlib

def shuffle(
    pares: list[tuple],
    num_reducers: int,
    key_fn: Callable = None,
) -> list[list[tuple]]:
    """
    Distribuye pares (clave, valor) entre N reducers.
    El reducer que recibe cada par se determina por hash(clave) % num_reducers.
    
    Retorna una lista de N listas, una por reducer.
    """
    buckets = [[] for _ in range(num_reducers)]

    for clave, valor in pares:
        # Determinar quÃ© reducer recibe este par:
        hash_clave = int(hashlib.md5(str(clave).encode()).hexdigest(), 16)
        reducer_idx = hash_clave % num_reducers
        buckets[reducer_idx].append((clave, valor))

    return buckets

# Simular Map/Reduce distribuido con shuffle explÃ­cito:
def map_reduce_con_shuffle(
    datos: list,
    map_fn: Callable,
    reduce_fn: Callable,
    valor_inicial,
    num_reducers: int = 4,
) -> dict:
    # FASE MAP:
    todos_los_pares = []
    for elemento in datos:
        todos_los_pares.extend(map_fn(elemento))

    # FASE SHUFFLE:
    buckets = shuffle(todos_los_pares, num_reducers)
    print(f"Datos por reducer: {[len(b) for b in buckets]}")

    # FASE REDUCE:
    resultado = {}
    for bucket in buckets:
        agrupados = defaultdict(list)
        for clave, valor in bucket:
            agrupados[clave].append(valor)
        for clave, valores in agrupados.items():
            acumulado = valor_inicial
            for v in valores:
                acumulado = reduce_fn(acumulado, v)
            resultado[clave] = acumulado

    return resultado
```

**Restricciones:**
1. Implementar y verificar con wordcount
2. Observar la distribuciÃ³n de datos por reducer â€” Â¿es uniforme?
3. Â¿QuÃ© pasa si todos los datos tienen la misma clave?
   (simula data skew extremo)
4. Implementar una funciÃ³n de particionamiento alternativa al hash:
   range partitioning (asignar rangos de claves a reducers)
5. Â¿CuÃ¡ndo range partitioning es mejor que hash partitioning?

**Pista:** Si todos los datos tienen la misma clave, el hash partitioning
envÃ­a todos los datos al mismo reducer â€” uno trabaja 100%, el resto 0%.
Esto es exactamente el "data skew" del Cap.04. El range partitioning
divide el espacio de claves en rangos: A-F â†’ reducer 0, G-M â†’ reducer 1, etc.
Para datos uniformemente distribuidos, esto da distribuciÃ³n balanceada.
Pero si hay hot spots (muchos datos en un rango especÃ­fico), range partitioning
tambiÃ©n puede crear skew â€” no es una soluciÃ³n universal.

---

### Ejercicio 3.3.3 â€” Medir el costo real del shuffle

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.getOrCreate()

# Dataset de 1M transacciones:
n = 1_000_000
df = spark.range(n).select(
    F.col("id"),
    (F.rand() * 10000).alias("monto"),
    F.when(F.rand() < 0.25, "norte")
     .when(F.rand() < 0.50, "sur")
     .when(F.rand() < 0.75, "este")
     .otherwise("oeste").alias("region"),
)
df.cache().count()  # materializar en cachÃ©

import time

# Job sin shuffle:
inicio = time.perf_counter()
df.withColumn("monto_con_iva", F.col("monto") * 1.19).count()
sin_shuffle = time.perf_counter() - inicio

# Job con shuffle (groupBy):
inicio = time.perf_counter()
df.groupBy("region").agg(F.sum("monto")).collect()
con_shuffle = time.perf_counter() - inicio

# Job con shuffle pesado (groupBy con alta cardinalidad):
inicio = time.perf_counter()
df.groupBy("id").agg(F.sum("monto")).collect()  # 1M claves distintas
shuffle_pesado = time.perf_counter() - inicio

print(f"Sin shuffle: {sin_shuffle:.2f}s")
print(f"Shuffle (4 claves): {con_shuffle:.2f}s")
print(f"Shuffle (1M claves): {shuffle_pesado:.2f}s")
```

**Restricciones:**
1. Ejecutar y registrar los tiempos
2. En Spark UI, ver los bytes shuffleados para cada job
3. Â¿Por quÃ© el shuffle con 1M claves es mÃ¡s lento que con 4 claves,
   aunque los datos son los mismos?
4. Medir el impacto del nÃºmero de particiones del shuffle
   (`spark.sql.shuffle.partitions = 200` vs `2000` vs `20`)

**Pista:** El shuffle con 1M claves es mÃ¡s lento por dos razones:
(1) el reducer necesita mantener en memoria entradas para 1M claves distintas
vs 4 claves, aumentando el uso de memoria y potencialmente el spill a disco,
(2) el merge/sort de los datos shuffleados es mÃ¡s costoso con mÃ¡s claves distintas.
El nÃºmero de particiones del shuffle determina cuÃ¡ntos archivos se escriben
en la fase de map output y cuÃ¡ntos reducers hay â€” con mÃ¡s datos, mÃ¡s particiones
reducen el tamaÃ±o por particiÃ³n y el spill; con menos datos, menos particiones
reducen el overhead de scheduling.

---

### Ejercicio 3.3.4 â€” Leer: el shuffle que no se puede evitar vs el que sÃ­

**Tipo: Analizar**

Para cada operaciÃ³n, determinar si el shuffle es evitable o inevitable,
y proponer la alternativa si es evitable:

```python
# 1. Contar filas por regiÃ³n (4 regiones):
df.groupBy("region").count()

# 2. Ordenar todo el dataset por timestamp:
df.orderBy("timestamp")

# 3. Join entre df_ventas (1 TB) y df_productos (100 MB):
df_ventas.join(df_productos, on="producto_id")

# 4. Calcular el percentil 95 de montos:
df.approxQuantile("monto", [0.95], 0.01)

# 5. Eliminar filas duplicadas:
df.distinct()

# 6. Join entre df_ventas (1 TB) y df_clientes (1 TB)
#    donde ambas ya estÃ¡n particionadas por user_id:
df_ventas.join(df_clientes, on="user_id")

# 7. Calcular el promedio de monto:
df.agg(F.avg("monto"))

# 8. Calcular el monto mÃ¡ximo POR usuario (10M usuarios distintos):
df.groupBy("user_id").agg(F.max("monto"))
```

**Pista:** La operaciÃ³n 3 puede evitar el shuffle si `df_productos` es pequeÃ±o
(broadcast join â€” Spark envÃ­a `df_productos` completo a cada executor).
La operaciÃ³n 6 puede evitar el shuffle si ambas tablas estÃ¡n pre-particionadas
por la misma key con el mismo nÃºmero de particiones (bucket join).
La operaciÃ³n 7 puede reducirse en dos fases sin shuffle completo: cada executor
calcula su sum y count local, luego un reduce final combina.

---

### Ejercicio 3.3.5 â€” DiseÃ±ar: un pipeline sin shuffles innecesarios

**Tipo: DiseÃ±ar**

El siguiente pipeline tiene shuffles innecesarios. RediseÃ±arlo para minimizarlos:

```python
df_ventas = spark.read.parquet("s3://ventas/")           # 500 GB
df_clientes = spark.read.parquet("s3://clientes/")        # 200 GB
df_productos = spark.read.parquet("s3://productos/")      # 500 MB

# Pipeline actual:
resultado = (df_ventas
    .filter(F.col("activo") == True)                      # filtro 1
    .join(df_clientes, on="cliente_id")                   # join 1 â†’ shuffle
    .filter(F.col("cliente_premium") == True)             # filtro 2
    .join(df_productos, on="producto_id")                 # join 2 â†’ shuffle
    .filter(F.col("categoria") == "electronico")          # filtro 3
    .groupBy("region", "mes")                             # groupby â†’ shuffle
    .agg(F.sum("monto").alias("revenue"),
         F.count("*").alias("transacciones"))
    .orderBy(F.col("revenue").desc())                     # orderby â†’ shuffle
)
```

1. Identificar todos los shuffles del pipeline
2. Â¿CuÃ¡les son evitables y cÃ³mo?
3. Â¿En quÃ© orden deberÃ­an aplicarse los filtros?
4. Reescribir el pipeline optimizado con anotaciones
5. Estimar la reducciÃ³n en datos shuffleados

---

## SecciÃ³n 3.4 â€” Combiners: Optimizar Antes del Shuffle

### Ejercicio 3.4.1 â€” El combiner como pre-reduce local

```
Sin combiner:
  Mapper 1 emite: [("norte", 100), ("norte", 150), ("norte", 200)]
  â†’ shuffle: 3 pares viajan a travÃ©s de la red

Con combiner:
  Mapper 1 combina localmente: ("norte", 450)
  â†’ shuffle: 1 par viaja a travÃ©s de la red
  
  ReducciÃ³n de 3Ã— en trÃ¡fico de red â†’ job 3Ã— mÃ¡s rÃ¡pido en fase shuffle
  
  Requisito: la funciÃ³n del combiner debe ser la MISMA que la del reducer
  (o al menos producir valores que el reducer puede combinar correctamente)
```

```python
# En Hadoop MapReduce clÃ¡sico, el combiner es explÃ­cito:
# job.setCombinerClass(SumReducer.class);

# En Spark, se usa implÃ­citamente con reduceByKey (vs groupByKey):
rdd.reduceByKey(lambda a, b: a + b)
# Spark aplica el combiner automÃ¡ticamente: combina localmente antes del shuffle

# En Spark SQL, el optimizador lo hace automÃ¡ticamente con HashAggregate:
# La primera fase (partial) es el combiner
df.groupBy("region").agg(F.sum("monto"))
# Plan fÃ­sico: HashAggregate(partial) â†’ Exchange â†’ HashAggregate(final)
```

**Preguntas:**

1. Â¿Para quÃ© funciones de agregaciÃ³n el combiner es siempre correcto?
   Â¿Para cuÃ¡les no puede usarse?

2. Â¿Puede usarse un combiner para calcular la mediana? Â¿Por quÃ©?

3. Â¿Puede usarse un combiner para calcular el promedio?
   Si sÃ­, Â¿cÃ³mo debe modificarse la funciÃ³n?

4. En Spark UI, Â¿cÃ³mo distingues el "HashAggregate partial" del "HashAggregate final"?

5. Si un combiner reduce los datos de 100 GB a 1 GB antes del shuffle,
   Â¿cuÃ¡nto impacto tiene en el tiempo total del job?

**Pista:** El promedio con combiner requiere un truco: en lugar de emitir
el promedio parcial, emitir la tupla `(suma, count)`. El combiner combina
tuplas: `(suma1, count1) + (suma2, count2) = (suma1+suma2, count1+count2)`.
El reducer final calcula `suma_total / count_total`. Esto es exactamente
lo que hace Spark internamente cuando calculas `F.avg()`.

---

### Ejercicio 3.4.2 â€” Implementar un combiner generalizado

```python
from dataclasses import dataclass
from typing import Any, Callable

@dataclass
class Combiner:
    """
    AbstracciÃ³n de un combiner para Map/Reduce.
    Requiere tres funciones:
      - create_accumulator: crear el acumulador inicial
      - add_input: aÃ±adir un valor al acumulador
      - merge_accumulators: combinar dos acumuladores
    """
    create_accumulator: Callable[[], Any]
    add_input: Callable[[Any, Any], Any]
    merge_accumulators: Callable[[Any, Any], Any]
    extract_output: Callable[[Any], Any] = None

    def __post_init__(self):
        if self.extract_output is None:
            self.extract_output = lambda acc: acc

# Combiner para suma:
combiner_suma = Combiner(
    create_accumulator=lambda: 0,
    add_input=lambda acc, val: acc + val,
    merge_accumulators=lambda a, b: a + b,
)

# Combiner para promedio (requiere (sum, count)):
combiner_promedio = Combiner(
    create_accumulator=lambda: (0.0, 0),
    add_input=lambda acc, val: (acc[0] + val, acc[1] + 1),
    merge_accumulators=lambda a, b: (a[0] + b[0], a[1] + b[1]),
    extract_output=lambda acc: acc[0] / acc[1] if acc[1] > 0 else None,
)

# Combiner para top-N (mÃ¡s complejo):
def crear_combiner_top_n(n: int) -> Combiner:
    import heapq
    return Combiner(
        create_accumulator=lambda: [],
        add_input=lambda acc, val: heapq.nlargest(n, acc + [val]),
        merge_accumulators=lambda a, b: heapq.nlargest(n, a + b),
    )
```

**Restricciones:**
1. Implementar y verificar `combiner_suma` y `combiner_promedio`
2. Implementar `combiner_top_n` y verificar para `n=10`
3. Implementar `combiner_conteo_distintos` (HyperLogLog simplificado)
4. Integrar el Combiner con `map_reduce_distribuido` del ejercicio anterior

**Pista:** El combiner de top-N tiene un truco: `add_input` y `merge_accumulators`
deben mantener solo los top-N elementos en el acumulador para evitar que crezca
sin lÃ­mite. El reducer final tambiÃ©n aplica top-N al resultado combinado de todos
los mappers. En Spark, esto es el `UDAF` (User-Defined Aggregate Function)
o la funciÃ³n `approx_top_k`.

---

### Ejercicio 3.4.3 â€” Leer: cuÃ¡ndo el combiner hace daÃ±o

**Tipo: Analizar**

El combiner no siempre es beneficioso. Analizar estos casos:

```python
# Caso 1: el combiner con datos ya reducidos
# Si cada clave aparece una sola vez en cada mapper,
# el combiner no reduce nada pero aÃ±ade overhead de procesamiento
rdd_sin_repeticiones = sc.parallelize([
    ("usuario_1", datos_1), ("usuario_2", datos_2), ...
    # cada usuario_id aparece exactamente una vez
])
rdd_sin_repeticiones.reduceByKey(fn_costosa)
# El combiner local crea acumuladores para 1M usuarios â†’ overhead de memoria

# Caso 2: el combiner con funciÃ³n costosa
rdd.reduceByKey(lambda a, b: ejecutar_modelo_ml(a, b))
# El combiner ejecuta el modelo ML en la fase de map â†’ 2x el trabajo

# Caso 3: el combiner con datos no uniformes
# 99% de los datos van a la clave "hot_key"
# El combiner ayuda poco porque el reducer sigue recibiendo muchos datos
```

**Preguntas:**

1. Â¿CÃ³mo detectas en Spark UI si el combiner estÃ¡ ayudando?
   (pista: comparar "Shuffle Write" antes y despuÃ©s de un groupBy)

2. Â¿Spark aplica el combiner automÃ¡ticamente siempre, o solo en algunos casos?

3. Para el Caso 2, Â¿existe una forma de desactivar el combiner
   sin cambiar `reduceByKey` por `groupByKey`?

4. Â¿En quÃ© situaciÃ³n preferirÃ­as `groupByKey` sobre `reduceByKey`
   incluso siendo consciente del mayor shuffle?

---

### Ejercicio 3.4.4 â€” El combiner en Spark SQL: el plan de ejecuciÃ³n

```python
from pyspark.sql import functions as F

df = spark.read.parquet("transacciones.parquet")

# Aggregation simple:
df.groupBy("region").agg(F.sum("monto")).explain()

# Output del explain (simplificado):
# == Physical Plan ==
# AdaptiveSparkPlan
# +- HashAggregate(keys=[region], functions=[sum(monto)])  â† REDUCE FINAL
#    +- Exchange hashpartitioning(region, 200)              â† SHUFFLE
#       +- HashAggregate(keys=[region], functions=[partial_sum(monto)])  â† COMBINER
#          +- FileScan parquet [region, monto]
```

**Preguntas:**

1. Â¿Por quÃ© hay dos `HashAggregate` en el plan?

2. El `partial_sum` en el primer HashAggregate es el combiner.
   Â¿En quÃ© fase del Map/Reduce clÃ¡sico corresponde?

3. Si cambias `F.sum("monto")` por `F.collect_list("monto")`,
   Â¿aparece el `partial_` en el plan? Â¿Por quÃ©?

4. Â¿QuÃ© datos viajan en el `Exchange hashpartitioning(region, 200)`?
   Â¿Son los datos originales o los resultados del primer HashAggregate?

5. `AdaptiveSparkPlan` puede cambiar el nÃºmero de particiones del Exchange.
   Â¿CuÃ¡ndo lo hace y cÃ³mo lo ves en el plan final?

---

### Ejercicio 3.4.5 â€” Construir: el pipeline wordcount end-to-end con combiner

Implementar wordcount completo con todas las optimizaciones:

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.getOrCreate()

# Dataset: 10GB de texto
df = spark.read.text("s3://bucket/corpus/*.txt")

# VersiÃ³n 1: naive (sin optimizaciones)
wc_naive = (df
    .select(F.explode(F.split(F.col("value"), r"\s+")).alias("palabra"))
    .filter(F.col("palabra") != "")
    .groupBy("palabra")
    .count()
    .orderBy(F.col("count").desc())
)

# VersiÃ³n 2: optimizada
# TODO: implementar con:
# - Filtrado de stop words antes del groupBy
# - lowercase antes del groupBy
# - Limitar el resultado a top 1000 (evitar orderBy global)
# - Verificar el plan para confirmar que hay partial_count

# Medir y comparar ambas versiones
```

**Restricciones:**
1. Implementar ambas versiones
2. Comparar el plan de ejecuciÃ³n (`explain(True)`)
3. Comparar el tiempo y los bytes shuffleados
4. Â¿El `orderBy` al final agrega un shuffle? Â¿Hay alternativa?

---

## SecciÃ³n 3.5 â€” MÃ¡s AllÃ¡ de Map/Reduce: el Modelo Generalizado

### Ejercicio 3.5.1 â€” Las limitaciones de Map/Reduce clÃ¡sico

**Tipo: Leer/analizar**

El Map/Reduce clÃ¡sico de Hadoop tiene limitaciones importantes
que motivaron el desarrollo de Spark y Flink:

```
LimitaciÃ³n 1: Solo dos fases (Map y Reduce)
  Los algoritmos iterativos (PageRank, K-means, entrenamiento de ML)
  requieren mÃºltiples rondas de Map/Reduce.
  Cada ronda: leer de disco â†’ procesar â†’ escribir a disco â†’ leer de disco â†’ ...
  Con 100 iteraciones: 100 lecturas y 100 escrituras a disco.

LimitaciÃ³n 2: No hay estado entre jobs
  El estado de una ronda debe escribirse a disco para ser leÃ­do en la siguiente.
  Sin memoria compartida entre etapas.

LimitaciÃ³n 3: Solo batch (no streaming)
  Map/Reduce de Hadoop no puede procesar datos que llegan continuamente.

LimitaciÃ³n 4: API de bajo nivel
  Escribir Map/Reduce en Java para operaciones simples requiere mucho cÃ³digo.
  
PageRank en Hadoop MapReduce: ~100 lÃ­neas de Java para cada iteraciÃ³n,
  mÃ¡s la lÃ³gica de orquestaciÃ³n de mÃºltiples jobs.
PageRank en Spark: ~10 lÃ­neas de Python.
```

**Preguntas:**

1. Â¿CÃ³mo resuelve Spark la "LimitaciÃ³n 1" (algoritmos iterativos)?

2. Â¿QuÃ© innovaciÃ³n de Spark permite evitar leer/escribir a disco entre etapas?

3. Â¿CÃ³mo resuelve Flink las limitaciones 1 y 3 simultÃ¡neamente?

4. Â¿Map/Reduce sigue siendo relevante en 2024? Â¿Para quÃ© casos?

5. La "LimitaciÃ³n 4" motivÃ³ el desarrollo de Pig Latin, Hive, y finalmente
   Spark SQL. Â¿QuÃ© tienen en comÃºn estas herramientas como soluciÃ³n?

**Pista:** Spark resuelve la LimitaciÃ³n 1 con el concepto de RDD (Resilient
Distributed Dataset): los datos pueden mantenerse en memoria entre etapas
usando `.cache()` o `.persist()`. En lugar de leer desde HDFS en cada iteraciÃ³n,
el RDD permanece distribuido en la memoria de los executors. Una iteraciÃ³n de
PageRank en Spark: ~1 segundo en memoria vs ~10 minutos en Hadoop (con I/O de disco).
Esto hace que Spark sea 10-100Ã— mÃ¡s rÃ¡pido para algoritmos iterativos.

---

### Ejercicio 3.5.2 â€” El DAG generalizado: mÃ¡s de dos fases

Spark generaliza Map/Reduce a un DAG (Directed Acyclic Graph) de operaciones:

```
Map/Reduce clÃ¡sico:
  [datos] â†’ Map â†’ Shuffle â†’ Reduce â†’ [resultado]
  (siempre exactamente 2 fases)

Spark DAG:
  [datos] â†’ Op1 â†’ Op2 â†’ Shuffle â†’ Op3 â†’ Shuffle â†’ Op4 â†’ [resultado]
  (tantas fases como necesites, con shuffles solo donde son necesarios)

Ejemplo: pipeline de e-commerce
  [ventas] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                         join â†’ groupBy â†’ [resultado]
  [clientes] â†’ filter(premium=True) â†’ select(id, segm) â”€â”˜
```

```python
# El mismo pipeline en Spark â€” el DAG se construye implÃ­citamente:
resultado = (
    df_ventas
    .join(
        df_clientes.filter(F.col("premium") == True).select("id", "segmento"),
        df_ventas["cliente_id"] == df_clientes["id"]
    )
    .groupBy("segmento", F.month("fecha"))
    .agg(F.sum("monto").alias("revenue"))
)
resultado.explain()  # muestra el DAG
```

**Restricciones:**
1. Ejecutar el pipeline y ver el DAG en Spark UI (pestaÃ±a "SQL")
2. Â¿CuÃ¡ntos shuffles tiene el DAG?
3. Â¿DÃ³nde ocurrirÃ­a un shuffle adicional si aÃ±ades `.orderBy("revenue")`?
4. Dibujar el DAG manualmente antes de verlo en Spark UI
   y comparar con el resultado real

---

### Ejercicio 3.5.3 â€” Map/Reduce en streaming: el modelo de Flink

En Flink, el modelo Map/Reduce se extiende a streams continuos:

```
Map/Reduce en batch (Spark):
  [colecciÃ³n finita] â†’ transformaciones â†’ [resultado finito]
  El pipeline termina cuando se agotan los datos.

Map/Reduce en streaming (Flink):
  [stream infinito] â†’ transformaciones â†’ [stream de resultados]
  El pipeline corre indefinidamente.
  Los "reduces" se hacen sobre ventanas de tiempo.
  
Wordcount en streaming:
  stream de oraciones â†’ flatMap(split) â†’ keyBy(palabra) â†’ window(5min) â†’ sum
  Emite el conteo de cada palabra cada 5 minutos (ventana tumbling).
```

```python
# En PySpark Structured Streaming (micro-batching):
from pyspark.sql import functions as F

df_stream = spark.readStream.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "textos") \
    .load()

wordcount_stream = (df_stream
    .select(F.explode(F.split(F.col("value").cast("string"), r"\s+")).alias("palabra"))
    .filter(F.col("palabra") != "")
    .withWatermark("timestamp", "1 minute")
    .groupBy(F.window("timestamp", "5 minutes"), "palabra")
    .count()
)

query = wordcount_stream.writeStream \
    .outputMode("update") \
    .format("console") \
    .start()
```

**Preguntas:**

1. Â¿El `groupBy + count` en streaming hace un shuffle real entre micro-batches?
   Â¿O el estado se mantiene en memoria?

2. Â¿QuÃ© pasa con el estado del wordcount si el stream lleva 24 horas corriendo?
   Â¿CuÃ¡nta memoria ocupa?

3. Â¿CÃ³mo el watermark limita el crecimiento del estado?

4. En el modelo de Flink (streaming puro, no micro-batching), Â¿cuÃ¡ndo se emite
   el resultado del wordcount de la ventana de 5 minutos?

---

### Ejercicio 3.5.4 â€” Map/Reduce en SQL: todo es Map/Reduce

**Tipo: Leer/analizar**

SQL es una abstracciÃ³n sobre Map/Reduce. Cada clÃ¡usula SQL tiene un equivalente:

```
SQL:
  SELECT region, SUM(monto) as total
  FROM transacciones
  WHERE activo = TRUE
  GROUP BY region
  HAVING SUM(monto) > 1000000
  ORDER BY total DESC
  LIMIT 10

Equivalente en Map/Reduce:
  1. WHERE â†’ filter (narrow, sin shuffle)
  2. SELECT columnas + GROUP BY â†’ map a (clave, valor) pairs
  3. GROUP BY + SUM â†’ shuffle + reduce (aquÃ­ estÃ¡ el shuffle)
  4. HAVING â†’ filter post-reduce (narrow)
  5. ORDER BY â†’ shuffle de todos los datos (para sort global)
  6. LIMIT â†’ reduce final a N elementos
```

**Preguntas:**

1. Para cada clÃ¡usula SQL, indica si genera un shuffle:
   - `WHERE`
   - `GROUP BY`
   - `JOIN ... ON`
   - `ORDER BY`
   - `DISTINCT`
   - `WINDOW FUNCTION (OVER PARTITION BY ... ORDER BY ...)`
   - `LIMIT` (sin ORDER BY)

2. Una query con `ORDER BY` siempre genera un shuffle completo.
   Â¿Hay alguna forma de implementar `ORDER BY ... LIMIT 10` con menos shuffle?

3. Â¿CÃ³mo implementa BigQuery o Redshift `ORDER BY LIMIT 10` de forma eficiente?

4. Una `WINDOW FUNCTION` como `RANK() OVER (PARTITION BY region ORDER BY monto DESC)`:
   Â¿CuÃ¡ntos shuffles genera?

**Pista:** `ORDER BY ... LIMIT 10` puede implementarse eficientemente como
un "tournament sort": cada nodo encuentra su top-10 local (sin shuffle),
luego el coordinador hace un merge de todos los top-10 locales (solo NÃ—10 filas).
En lugar de ordenar TB de datos globalmente, solo se mueven NÃ—10 filas.
Spark, Flink, y todos los motores SQL modernos hacen esta optimizaciÃ³n.

---

### Ejercicio 3.5.5 â€” El modelo de Beam: unificando batch y streaming

```python
import apache_beam as beam

# En Beam, el mismo cÃ³digo funciona para batch y streaming.
# La diferencia estÃ¡ en la fuente (PCollection bounded vs unbounded).

# Wordcount en batch (archivo):
with beam.Pipeline() as p:
    resultado = (
        p
        | "Leer" >> beam.io.ReadFromText("gs://bucket/corpus/*.txt")
        | "Split" >> beam.FlatMap(str.split)
        | "Pares" >> beam.Map(lambda w: (w, 1))
        | "Contar" >> beam.CombinePerKey(sum)
        | "Escribir" >> beam.io.WriteToText("gs://bucket/wordcount")
    )

# Wordcount en streaming (Kafka):
# El mismo pipeline, diferente fuente:
with beam.Pipeline(options=streaming_options) as p:
    resultado = (
        p
        | "Leer" >> beam.io.ReadFromKafka(
            consumer_config={"bootstrap.servers": "localhost:9092"},
            topics=["textos"]
        )
        | "Split" >> beam.FlatMap(lambda msg: msg.value.decode().split())
        | "Pares" >> beam.Map(lambda w: (w, 1))
        | "Ventana" >> beam.WindowInto(beam.window.FixedWindows(300))
        | "Contar" >> beam.CombinePerKey(sum)
        | "Escribir" >> beam.io.WriteToBigQuery(...)
    )
```

**Preguntas:**

1. Â¿QuÃ© parte del pipeline de Beam corresponde al Map de Map/Reduce?
2. Â¿QuÃ© parte corresponde al Reduce?
3. Â¿DÃ³nde estÃ¡ el shuffle implÃ­cito en el pipeline de Beam?
4. Â¿Por quÃ© `CombinePerKey` funciona como un reduce con combiner automÃ¡tico?
5. Â¿QuÃ© es el "runner" de Beam y cÃ³mo decide dÃ³nde hacer el shuffle?

---

## SecciÃ³n 3.6 â€” Map/Reduce en los Frameworks Modernos

### Ejercicio 3.6.1 â€” La tabla de equivalencias

**Tipo: Construir/analizar**

Completar la tabla de equivalencias entre Map/Reduce clÃ¡sico y los frameworks modernos:

```
Concepto MR clÃ¡sico    Spark RDD      Spark SQL/DF       Beam             Flink
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
map                    .map()         .withColumn()      beam.Map()       .map()
flatMap                .flatMap()     .select(explode()) beam.FlatMap()   .flatMap()
filter                 .filter()      .filter()          beam.Filter()    .filter()
groupByKey             .groupByKey()  .groupBy()         beam.GroupByKey  .keyBy()
reduceByKey            .reduceByKey() .agg(F.sum())      beam.Combine...  .reduce()
combiner               automÃ¡tico     HashAggregate(p)   CombinePerKey    .aggregate()
shuffle                Exchange       Exchange           GBK shuffle      network exchange
output                 .saveAs*       .write.*           beam.io.Write*   .addSink()
```

Para cada celda vacÃ­a, completar con la API correspondiente del framework.

---

### Ejercicio 3.6.2 â€” Wordcount en cinco implementaciones

Implementar wordcount en las cinco formas siguientes y comparar:

```python
# 1. Map/Reduce manual en Python puro:
def wordcount_puro(textos: list[str]) -> dict:
    # TODO

# 2. Map/Reduce con multiprocessing:
def wordcount_paralelo(textos: list[str], num_workers: int = 4) -> dict:
    # TODO

# 3. Spark RDD API (el mÃ¡s parecido a Map/Reduce clÃ¡sico):
def wordcount_spark_rdd(textos_rdd) -> list:
    return (textos_rdd
        .flatMap(str.split)
        .map(lambda w: (w, 1))
        .reduceByKey(lambda a, b: a + b)
        .collect()
    )

# 4. Spark DataFrame API (SQL-like):
def wordcount_spark_df(df) -> object:
    return (df
        .select(F.explode(F.split("value", r"\s+")).alias("palabra"))
        .groupBy("palabra")
        .count()
    )

# 5. Polars:
def wordcount_polars(textos: list[str]) -> pl.DataFrame:
    return (pl.Series("textos", textos)
        .str.split(" ")
        .explode()
        .value_counts()
        .sort("counts", descending=True)
    )
```

**Restricciones:**
1. Implementar las cinco versiones
2. Verificar que producen el mismo resultado
3. Medir el tiempo para 1M, 10M, y 100M palabras
4. Identificar en cuÃ¡l implementaciÃ³n estÃ¡ mÃ¡s visible el "shuffle"

---

### Ejercicio 3.6.3 â€” PageRank: donde Map/Reduce clÃ¡sico falla y Spark gana

PageRank es el algoritmo que ilustra por quÃ© los algoritmos iterativos
son costosos en Hadoop y eficientes en Spark:

```python
# PageRank en Spark (simplificado):
def pagerank_spark(links_rdd, num_iteraciones=10):
    """
    links_rdd: RDD de (url, [url_destino_1, url_destino_2, ...])
    """
    # Inicializar ranks:
    ranks = links_rdd.map(lambda url_links: (url_links[0], 1.0))

    for _ in range(num_iteraciones):
        # Calcular contribuciones:
        contribs = links_rdd.join(ranks).flatMap(
            lambda url_links_rank: [
                (dest, url_links_rank[1][1] / len(url_links_rank[1][0]))
                for dest in url_links_rank[1][0]
            ]
        )
        # Actualizar ranks:
        ranks = contribs.reduceByKey(lambda a, b: a + b).mapValues(
            lambda rank: 0.15 + 0.85 * rank
        )

    return ranks.collect()
```

**Preguntas:**

1. Â¿CuÃ¡ntos shuffles hay en cada iteraciÃ³n de PageRank?

2. Â¿Por quÃ© es crÃ­tico hacer `.cache()` sobre `links_rdd`?

3. En Hadoop MapReduce, Â¿cuÃ¡ntas lecturas/escrituras a disco
   habrÃ­a para 10 iteraciones?

4. Â¿Por quÃ© Spark puede mantener `ranks` en memoria entre iteraciones?

5. Â¿CÃ³mo el grafo de PageRank afecta el data skew?
   (pistas: considera urls muy enlazadas vs urls poco enlazadas)

---

### Ejercicio 3.6.4 â€” Map/Reduce en Polars: sin shuffle explÃ­cito

Polars implementa el mismo modelo pero sin la complejidad del shuffle
distribuido â€” todo ocurre en una sola mÃ¡quina:

```python
import polars as pl

df = pl.read_parquet("transacciones.parquet")

# Equivalente a Map + GroupBy/Reduce:
resultado = (df
    .lazy()                                        # MAP: lazy evaluation
    .filter(pl.col("activo") == True)              # filter (narrow)
    .with_columns(                                  # map
        (pl.col("monto") * 1.19).alias("monto_iva")
    )
    .group_by("region")                            # GroupBy (shuffle en Spark,
    .agg(                                           # en memoria en Polars)
        pl.sum("monto_iva").alias("total_iva"),
        pl.count().alias("transacciones"),
    )
    .sort("total_iva", descending=True)            # sort
    .collect()                                     # REDUCE: ejecutar el plan
)
```

**Preguntas:**

1. Â¿Polars hace un "shuffle" interno para el `group_by`?
   Â¿QuÃ© estructura de datos usa en su lugar?

2. Â¿Por quÃ© Polars puede ser mÃ¡s rÃ¡pido que Spark para el mismo job
   si los datos caben en una mÃ¡quina?

3. Â¿CuÃ¡ndo Polars alcanza su lÃ­mite y necesitas Spark?

4. `.lazy()` en Polars equivale a Â¿quÃ© en Spark?

---

### Ejercicio 3.6.5 â€” Leer: el paper que cambiÃ³ todo

**Tipo: Leer/analizar**

Leer la secciÃ³n 2 y 3 del paper original de MapReduce (Dean & Ghemawat, 2004).

> ðŸ“– Profundizar: *MapReduce: Simplified Data Processing on Large Clusters*,
> Dean & Ghemawat, OSDI 2004. La SecciÃ³n 2 describe el modelo de programaciÃ³n
> (2 pÃ¡ginas). La SecciÃ³n 3 describe la implementaciÃ³n incluyendo el shuffle
> (3 pÃ¡ginas). Total: 5 pÃ¡ginas suficientes para entender el paper.

**Preguntas basadas en el paper:**

1. El paper describe que el combiner es opcional. Â¿Bajo quÃ© condiciÃ³n
   el paper dice que puede usarse un combiner?

2. El paper describe cÃ³mo Google usÃ³ Map/Reduce internamente.
   Â¿CuÃ¡l de sus casos de uso te parece mÃ¡s relevante para data engineering moderno?

3. El paper es de 2004. Â¿QuÃ© limitaciones del diseÃ±o original
   se volvieron evidentes en los aÃ±os siguientes?

4. El paper describe que las tareas lentas ("stragglers") son un problema.
   Â¿CÃ³mo lo resuelve? Â¿Los frameworks modernos usan la misma soluciÃ³n?

5. Â¿QuÃ© parte del diseÃ±o original de Map/Reduce persiste sin cambios
   en Spark, Beam, y Flink?

**Pista:** Para la pregunta 4: el paper describe el "backup task" o "speculative execution".
Cuando una tarea estÃ¡ tardando mÃ¡s que el promedio (un "straggler"), el sistema
lanza una copia idÃ©ntica en otro nodo. Cuando cualquiera de las dos termina,
la otra se cancela. Spark implementa esto con `spark.speculation=true`.
La misma soluciÃ³n persiste 20 aÃ±os despuÃ©s porque el problema es el mismo:
en un cluster grande, siempre habrÃ¡ alguna mÃ¡quina mÃ¡s lenta que las demÃ¡s.

---

## SecciÃ³n 3.7 â€” CuÃ¡ndo Map/Reduce No Es Suficiente

### Ejercicio 3.7.1 â€” Algoritmos que no encajan en Map/Reduce

**Tipo: Analizar**

No todos los algoritmos son expresables eficientemente como Map/Reduce.
Para cada algoritmo, evaluar si Map/Reduce es una buena soluciÃ³n:

```
1. Ordenamiento global de 1 TB de datos

2. BÃºsqueda de camino mÃ¡s corto (Dijkstra) en un grafo de 1B nodos

3. Entrenamiento de una red neuronal (gradient descent iterativo)

4. CÃ¡lculo de percentiles exactos sobre 1B valores

5. DetecciÃ³n de fraude en tiempo real (< 100ms por transacciÃ³n)

6. Joins entre mÃºltiples tablas con dependencias circulares

7. CompresiÃ³n de video (requiere procesamiento secuencial de frames)

8. GeneraciÃ³n de texto con un modelo de lenguaje
```

Para cada uno: Â¿es Map/Reduce suficiente? Si no, Â¿quÃ© paradigma es mÃ¡s apropiado?

---

### Ejercicio 3.7.2 â€” Graph processing: mÃ¡s allÃ¡ de Map/Reduce

El procesamiento de grafos requiere comunicaciÃ³n entre nodos que no sigue
el patrÃ³n clave/valor de Map/Reduce:

```python
# DetecciÃ³n de componentes conectados en un grafo:
# Cada nodo necesita saber el estado de sus vecinos,
# que pueden estar en cualquier particiÃ³n.

# En Spark GraphX (librerÃ­a de grafos):
from pyspark.sql import SparkSession
from graphframes import GraphFrame

# Crear el grafo:
vertices = spark.createDataFrame([
    ("u1", "Alice"), ("u2", "Bob"), ("u3", "Charlie")
], ["id", "name"])

edges = spark.createDataFrame([
    ("u1", "u2", "follows"), ("u2", "u3", "follows")
], ["src", "dst", "relationship"])

grafo = GraphFrame(vertices, edges)

# Algoritmo: PageRank (ya lo vimos)
pagerank = grafo.pageRank(resetProbability=0.15, maxIter=10)

# Algoritmo: Connected Components
componentes = grafo.connectedComponents()
```

**Preguntas:**

1. Â¿Por quÃ© el procesamiento de grafos es difÃ­cil de expresar en Map/Reduce?

2. Â¿QuÃ© es el modelo BSP (Bulk Synchronous Parallel) y cÃ³mo difiere de Map/Reduce?

3. Â¿CuÃ¡ntos shuffles hay por iteraciÃ³n de Connected Components?

4. Â¿Para quÃ© tamaÃ±o de grafo Spark GraphX es viable?
   Â¿CuÃ¡ndo necesitas una herramienta especializada (Neo4j, TigerGraph)?

> ðŸ”— Ecosistema: para grafos muy grandes (>10B de aristas), los frameworks
> especializados como GraphX de Spark, Apache Giraph, o bases de datos de
> grafos como TigerGraph son mÃ¡s apropiados que Map/Reduce general.
> No se cubren en este repositorio.

---

### Ejercicio 3.7.3 â€” Machine learning distribuido: el lÃ­mite de Map/Reduce

```python
# Gradient Descent en Map/Reduce (simplificado):
# Cada iteraciÃ³n es un Map/Reduce completo:

def entrenar_modelo_mr(datos, num_iteraciones=100):
    parametros = inicializar_parametros()

    for i in range(num_iteraciones):
        # MAP: calcular gradientes en paralelo
        gradientes_parciales = datos.map(
            lambda punto: calcular_gradiente(punto, parametros)
        )

        # REDUCE: promediar gradientes
        gradiente_promedio = gradientes_parciales.reduce(
            lambda g1, g2: sumar_gradientes(g1, g2)
        )
        gradiente_promedio /= len(datos)

        # Actualizar parÃ¡metros (en el driver):
        parametros = actualizar_parametros(parametros, gradiente_promedio)

    return parametros
```

**Preguntas:**

1. Â¿DÃ³nde estÃ¡n los parÃ¡metros del modelo durante el entrenamiento?
   (en el driver, en los workers, o en ambos)

2. Â¿QuÃ© pasa si el modelo tiene 10B parÃ¡metros (ej: un LLM)?
   Â¿Caben en la memoria del driver?

3. Â¿QuÃ© es el "parameter server" y cÃ³mo resuelve la limitaciÃ³n de los parÃ¡metros?

4. Â¿En quÃ© se diferencia este approach de cÃ³mo PyTorch distribuye el entrenamiento?

5. Â¿Map/Reduce es el paradigma correcto para entrenar LLMs?

**Pista:** Para modelos grandes, los parÃ¡metros no caben en el driver.
El "parameter server" distribuye los parÃ¡metros entre mÃºltiples nodos.
Los workers calculan gradientes y los envÃ­an al parameter server,
que actualiza los parÃ¡metros y los distribuye de vuelta.
PyTorch usa "Distributed Data Parallel" (DDP) que replica el modelo en
cada GPU y sincroniza los gradientes con All-Reduce (un shuffle especializado).
Map/Reduce general es demasiado ineficiente para esto â€” la sincronizaciÃ³n
de gradientes necesita primitivas de comunicaciÃ³n colectiva (All-Reduce, Broadcast)
que no estÃ¡n en el modelo bÃ¡sico de Map/Reduce.

---

### Ejercicio 3.7.4 â€” El lÃ­mite del modelo: cuando necesitas algo distinto

**Tipo: DiseÃ±ar**

Para cada caso de uso, determinar si Map/Reduce (o sus derivados Spark/Beam/Flink)
es la herramienta apropiada o si necesitas algo diferente:

```
Caso 1: Procesar 10 TB de logs para extraer mÃ©tricas de error por servicio.
Caso 2: Recomendar productos a usuarios con < 50ms de latencia.
Caso 3: Detectar anomalÃ­as en series temporales de sensores IoT.
Caso 4: Entrenar un modelo de clasificaciÃ³n de imÃ¡genes con 1B de parÃ¡metros.
Caso 5: Calcular el grafo de dependencias de 1M de paquetes npm.
Caso 6: Procesar transacciones bancarias con exactamente-una-vez.
Caso 7: Buscar documentos similares en una colecciÃ³n de 10M de textos.
Caso 8: Orquestar un workflow de 50 pasos con dependencias complejas.
```

Para cada caso: herramienta recomendada (puede ser Map/Reduce o no)
y justificaciÃ³n en una oraciÃ³n.

---

### Ejercicio 3.7.5 â€” El modelo mental completo: conectando todo

**Tipo: Integrar**

Este es el ejercicio de cierre de la Parte 1 del repositorio.
Tres capÃ­tulos despuÃ©s del Cap.01, el modelo mental estÃ¡ completo.

Retomando la tabla del Cap.01 (Â§1.5.4), completar ahora con mayor detalle:

```
Concepto de concurrencia â†’ Equivalente en data engineering

Goroutine leak          â†’ Consumer lag creciente
                          (los mensajes se acumulan porque el consumer
                           no puede mantener el ritmo â€” el equivalente
                           de una goroutine que produce mÃ¡s rÃ¡pido
                           de lo que el canal puede consumir)

Race condition          â†’ ???
                          (dos jobs escriben al mismo destino
                           sin coordinaciÃ³n â€” Â¿quÃ© pasa?)

Deadlock               â†’ ???
                          (dos stages de Spark esperando datos
                           del otro â€” Â¿puede ocurrir?)

Circuit breaker        â†’ Backpressure en streaming
                          (cuando el consumer no puede seguir el ritmo,
                           ralentiza al producer en lugar de fallar)

Exactly-once           â†’ Exactly-once en Kafka/Flink
                          (mucho mÃ¡s difÃ­cil de garantizar en distribuido
                           porque el fallo puede ocurrir entre el
                           procesamiento y el ack)
```

Y la pregunta de cierre:

> El wordcount de Map/Reduce, el pipeline de Spark, el stream de Flink, y
> el DAG de Beam son todos el mismo problema visto con distintos lentes.
>
> Â¿CuÃ¡l es ese problema?

---

## Resumen del capÃ­tulo

**Las cinco ideas que este capÃ­tulo deberÃ­a haber dejado claras:**

```
1. Map es paralelismo perfecto
   Cualquier transformaciÃ³n donde cada elemento es independiente
   puede ejecutarse en paralelo sin coordinaciÃ³n.
   En Spark: narrow transformations. En SQL: WHERE, SELECT.

2. Reduce requiere coordinaciÃ³n (shuffle)
   Para combinar elementos relacionados, necesitas moverlos al mismo nodo.
   El shuffle es el costo inevitable de cualquier GROUP BY, JOIN, ORDER BY.
   Minimizar el shuffle = maximizar el rendimiento.

3. El combiner es pre-reduce local
   Si la funciÃ³n de reduce es asociativa, puedes reducir antes del shuffle.
   reduceByKey() > groupByKey(). partial_sum > sum completo.
   Ahorra trÃ¡fico de red sin cambiar el resultado.

4. Todos los frameworks modernos son Map/Reduce generalizado
   Spark, Beam, Flink, Kafka Streams â€” todos implementan el mismo paradigma
   con distintos tradeoffs de latencia, estado, y garantÃ­as.
   El DAG reemplaza las dos fases fijas. El shuffle sigue siendo el centro.

5. Map/Reduce tiene lÃ­mites
   Grafos, ML distribuido, streaming de baja latencia â€” requieren
   primitivas adicionales (BSP, All-Reduce, estado persistente).
   Conocer los lÃ­mites es tan importante como conocer el modelo.
```

**La cadena de causalidad que conecta los tres primeros capÃ­tulos:**

```
Cap.01: El framework controla el paralelismo â†’ tÃº describes QUÃ‰
  â†“
Cap.02: Los datos en disco son columnar (Parquet) â†’ reduce el I/O
        Los datos en memoria son Arrow â†’ permite operaciones SIMD
        El formato determina cuÃ¡nto trabajo hay antes del cÃ³digo
  â†“
Cap.03: El trabajo se divide en Map (paralelo) y Reduce (shuffle)
        El shuffle es el cuello de botella
        Minimizar el shuffle = maximizar el rendimiento del pipeline
  â†“
Cap.04: Spark implementa este modelo con particiones, stages, y shuffles
        El Spark UI muestra exactamente dÃ³nde estÃ¡ el shuffle y cuÃ¡nto cuesta
```

Esa cadena es el mapa conceptual del repositorio.
Los siguientes capÃ­tulos son implementaciones concretas de ese mapa.
