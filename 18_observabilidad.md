# GuÃ­a de Ejercicios â€” Cap.18: Observabilidad de Pipelines de Datos

> Un pipeline que ejecuta exitosamente y produce datos incorrectos
> es mÃ¡s peligroso que uno que falla â€” porque nadie se entera.
>
> La orquestaciÃ³n (Cap.17) responde "Â¿el pipeline ejecutÃ³?".
> La observabilidad responde "Â¿el pipeline produjo datos correctos,
> a tiempo, y en el volumen esperado?".
>
> En sistemas distribuidos, observabilidad significa tres cosas:
> mÃ©tricas, logs, y traces. En data engineering, hay una cuarta:
> la calidad de los datos mismos.
>
> No puedes operar lo que no puedes ver.
> Y en data engineering, "ver" no es solo ver si el job corriÃ³ â€”
> es ver si la tabla de revenue tiene 0 filas,
> si el campo `monto` tiene valores negativos que no deberÃ­an existir,
> si los datos de ayer llegaron 3 horas tarde,
> y si el pipeline que tardaba 20 minutos ahora tarda 2 horas.

---

## El modelo mental: los cuatro pilares de la observabilidad de datos

```
Observabilidad clÃ¡sica (SRE/DevOps):          Observabilidad de datos:

  1. MÃ©tricas (Prometheus/Datadog)               1. MÃ©tricas de pipeline
     CPU, memoria, latencia, throughput              duraciÃ³n, filas procesadas,
                                                     bytes leÃ­dos/escritos

  2. Logs (ELK/CloudWatch)                       2. Logs de ejecuciÃ³n
     errores, warnings, stack traces                 errores de parsing, schemas
                                                     invÃ¡lidos, registros descartados

  3. Traces (Jaeger/Zipkin)                      3. Data lineage
     request â†’ servicio A â†’ servicio B               tabla X â†’ transformaciÃ³n â†’ tabla Y
     latencia por hop                                Â¿de dÃ³nde vienen estos datos?

  4. (no existe equivalente clÃ¡sico)             4. Data quality
                                                     freshness: Â¿los datos son recientes?
                                                     volume: Â¿hay la cantidad esperada?
                                                     schema: Â¿las columnas son correctas?
                                                     distribution: Â¿los valores son normales?

El cuarto pilar es lo que hace la observabilidad de datos DIFERENTE
de la observabilidad de software. Un microservicio que retorna 200 OK
con datos incorrectos es invisible para Prometheus.
Un pipeline que completa en verde pero escribe 0 filas
es invisible para Airflow.
```

```
Â¿QuiÃ©n necesita ver quÃ©?

  Data Engineer:
    "Â¿El pipeline ejecutÃ³? Â¿CuÃ¡nto tardÃ³? Â¿Hubo errores?"
    â†’ Airflow UI, logs, mÃ©tricas de ejecuciÃ³n

  Analytics Engineer:
    "Â¿Los datos del modelo de dbt son frescos y correctos?"
    â†’ Data quality checks, freshness monitors, schema tests

  Data Analyst / Stakeholder:
    "Â¿Puedo confiar en el dashboard de hoy?"
    â†’ Data freshness indicator, quality score, SLA status

  On-call Engineer:
    "Â¿QuÃ© se rompiÃ³, por quÃ©, y cÃ³mo lo arreglo?"
    â†’ Alertas, logs, lineage (Â¿quÃ© upstream fallÃ³?)

  Cada persona necesita una vista distinta de la misma realidad.
  La observabilidad es el sistema que une todas esas vistas.
```

---

## Tabla de contenidos

- [SecciÃ³n 18.1 â€” MÃ©tricas de pipelines: quÃ© medir y por quÃ©](#secciÃ³n-181--mÃ©tricas-de-pipelines-quÃ©-medir-y-por-quÃ©)
- [SecciÃ³n 18.2 â€” Logging estructurado para data engineering](#secciÃ³n-182--logging-estructurado-para-data-engineering)
- [SecciÃ³n 18.3 â€” Data lineage: de dÃ³nde vienen los datos](#secciÃ³n-183--data-lineage-de-dÃ³nde-vienen-los-datos)
- [SecciÃ³n 18.4 â€” Data quality: el pilar que falta](#secciÃ³n-184--data-quality-el-pilar-que-falta)
- [SecciÃ³n 18.5 â€” Dashboards operativos y alertas](#secciÃ³n-185--dashboards-operativos-y-alertas)
- [SecciÃ³n 18.6 â€” Observabilidad de Spark, Flink, y Kafka](#secciÃ³n-186--observabilidad-de-spark-flink-y-kafka)
- [SecciÃ³n 18.7 â€” El e-commerce observable: integrando los cuatro pilares](#secciÃ³n-187--el-e-commerce-observable-integrando-los-cuatro-pilares)

---

## SecciÃ³n 18.1 â€” MÃ©tricas de Pipelines: QuÃ© Medir y Por QuÃ©

### Ejercicio 18.1.1 â€” Leer: las mÃ©tricas que importan

**Tipo: Leer**

```
Las mÃ©tricas de un pipeline de datos se dividen en tres categorÃ­as:

  1. MÃ‰TRICAS DE EJECUCIÃ“N (Â¿el pipeline corriÃ³ bien?)
     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     pipeline_duration_seconds        cuÃ¡nto tardÃ³ el pipeline completo
     task_duration_seconds            cuÃ¡nto tardÃ³ cada tarea
     task_retries_total               cuÃ¡ntos reintentos hubo
     task_failures_total              cuÃ¡ntas tareas fallaron
     pipeline_success_rate            % de runs exitosos (Ãºltimos 30 dÃ­as)
     scheduler_lag_seconds            tiempo entre "scheduled" y "running"

  2. MÃ‰TRICAS DE DATOS (Â¿los datos son correctos?)
     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     rows_processed_total             filas leÃ­das/escritas
     rows_dropped_total               filas descartadas (parsing, quality)
     bytes_read_total                 volumen de datos leÃ­dos
     bytes_written_total              volumen de datos escritos
     null_rate_per_column             % de nulls por columna
     schema_changes_detected          cambios de schema detectados

  3. MÃ‰TRICAS DE NEGOCIO (Â¿el resultado tiene sentido?)
     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     daily_revenue_total              revenue calculado hoy
     unique_users_count               usuarios Ãºnicos procesados
     average_order_value              ticket promedio
     revenue_vs_yesterday_pct         cambio respecto al dÃ­a anterior

  La trampa: la mayorÃ­a de los equipos solo miden la categorÃ­a 1.
  Saben que el pipeline corriÃ³, pero no saben si los datos son correctos.
  Las categorÃ­as 2 y 3 son las que previenen el incidente del "$0 en el dashboard".
```

**Preguntas:**

1. Â¿Las mÃ©tricas de ejecuciÃ³n son responsabilidad del orquestador
   o del cÃ³digo del pipeline?

2. Â¿Las mÃ©tricas de datos deben almacenarse en Prometheus/Datadog
   o en una tabla del data warehouse?

3. Â¿`rows_dropped_total` deberÃ­a generar una alerta?
   Â¿A partir de quÃ© porcentaje?

4. Â¿`revenue_vs_yesterday_pct` es una mÃ©trica de observabilidad
   o una mÃ©trica de negocio? Â¿Hay diferencia?

5. Â¿QuÃ© mÃ©trica agregarÃ­as que no estÃ¡ en la lista?

---

### Ejercicio 18.1.2 â€” Instrumentar un pipeline con mÃ©tricas custom

```python
# Emitir mÃ©tricas desde un pipeline de datos usando structlog + StatsD.

import structlog
import time
from dataclasses import dataclass, field
from typing import Optional

@dataclass
class PipelineMetrics:
    """Acumulador de mÃ©tricas para un run del pipeline."""
    pipeline_name: str
    run_date: str
    start_time: float = field(default_factory=time.time)
    rows_read: int = 0
    rows_written: int = 0
    rows_dropped: int = 0
    bytes_read: int = 0
    bytes_written: int = 0
    errors: list = field(default_factory=list)
    step_durations: dict = field(default_factory=dict)

    def record_step(self, step_name: str, duration: float, rows: int = 0):
        self.step_durations[step_name] = duration
        self.rows_read += rows

    def record_drop(self, count: int, reason: str):
        self.rows_dropped += count
        self.errors.append({"type": "drop", "count": count, "reason": reason})

    @property
    def duration_total(self) -> float:
        return time.time() - self.start_time

    @property
    def drop_rate(self) -> float:
        total = self.rows_read + self.rows_dropped
        return self.rows_dropped / total if total > 0 else 0.0

    def to_dict(self) -> dict:
        return {
            "pipeline": self.pipeline_name,
            "run_date": self.run_date,
            "duration_seconds": round(self.duration_total, 2),
            "rows_read": self.rows_read,
            "rows_written": self.rows_written,
            "rows_dropped": self.rows_dropped,
            "drop_rate": round(self.drop_rate, 4),
            "bytes_read": self.bytes_read,
            "bytes_written": self.bytes_written,
            "steps": self.step_durations,
            "errors": self.errors,
        }


# Uso en un pipeline:
log = structlog.get_logger()

def ejecutar_pipeline(fecha: str):
    metrics = PipelineMetrics(pipeline_name="ecommerce_daily", run_date=fecha)

    # Paso 1: Extraer
    t0 = time.time()
    df = extraer_ventas(fecha)
    metrics.record_step("extraer", time.time() - t0, rows=len(df))

    # Paso 2: Validar
    t0 = time.time()
    df_valid, df_invalid = validar(df)
    metrics.record_step("validar", time.time() - t0)
    if len(df_invalid) > 0:
        metrics.record_drop(len(df_invalid), "schema_validation_failed")

    # Paso 3: Transformar
    t0 = time.time()
    resultado = transformar(df_valid)
    metrics.record_step("transformar", time.time() - t0)
    metrics.rows_written = len(resultado)

    # Emitir mÃ©tricas:
    log.info("pipeline_completed", **metrics.to_dict())

    # Enviar a StatsD/Prometheus:
    statsd.gauge("pipeline.duration", metrics.duration_total,
                 tags=[f"pipeline:{metrics.pipeline_name}"])
    statsd.gauge("pipeline.rows_written", metrics.rows_written,
                 tags=[f"pipeline:{metrics.pipeline_name}"])
    statsd.gauge("pipeline.drop_rate", metrics.drop_rate,
                 tags=[f"pipeline:{metrics.pipeline_name}"])

    return metrics
```

**Preguntas:**

1. Â¿StatsD vs Prometheus push gateway â€” cuÃ¡l para mÃ©tricas de pipeline?

2. Â¿Las mÃ©tricas deben emitirse durante el pipeline o al final?

3. Â¿`drop_rate` de 0.1% es aceptable? Â¿Y 5%? Â¿Depende del pipeline?

4. Â¿Guardar las mÃ©tricas en una tabla de BigQuery ademÃ¡s de Prometheus
   tiene valor? Â¿Para quÃ©?

5. Â¿CÃ³mo instrumentas un pipeline de Spark distribuido
   donde las mÃ©tricas estÃ¡n en los executors?

---

### Ejercicio 18.1.3 â€” MÃ©tricas de Airflow: quÃ© expone el scheduler

```python
# Airflow expone mÃ©tricas vÃ­a StatsD de forma nativa.
# Configurar en airflow.cfg:
# [metrics]
# statsd_on = True
# statsd_host = statsd-server
# statsd_port = 8125
# statsd_prefix = airflow

# MÃ©tricas clave que Airflow emite automÃ¡ticamente:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# airflow.dag.{dag_id}.duration              duraciÃ³n del DagRun
# airflow.dag_processing.total_parse_time    tiempo de parseo de DAGs
# airflow.scheduler.tasks.running            tareas en ejecuciÃ³n
# airflow.scheduler.tasks.starving           tareas esperando slot
# airflow.pool.open_slots.{pool}             slots disponibles en el pool
# airflow.executor.queued_tasks              tareas en cola
# airflow.executor.running_tasks             tareas ejecutando
# airflow.ti.finish.{dag_id}.{task_id}.{state}  estado final de la tarea

# Dashboard de Grafana para Airflow:
# Panel 1: Pipeline Duration (time series)
#   query: airflow.dag.ecommerce_daily.duration
#   alert: si > 2Ã— el promedio de los Ãºltimos 7 dÃ­as

# Panel 2: Tasks Starving (gauge)
#   query: airflow.scheduler.tasks.starving
#   alert: si > 10 tareas esperando por mÃ¡s de 5 minutos

# Panel 3: DAG Parse Time (time series)
#   query: airflow.dag_processing.total_parse_time
#   alert: si > 30 segundos (indica DAGs pesados)

# Panel 4: Pool Utilization (gauge)
#   query: airflow.pool.open_slots.default / total_slots
#   alert: si utilizaciÃ³n > 90% por mÃ¡s de 10 minutos
```

**Preguntas:**

1. Â¿`tasks.starving` es la mÃ©trica mÃ¡s importante para detectar
   cuellos de botella en Airflow?

2. Â¿El parse time de DAGs afecta directamente la latencia de scheduling?

3. Â¿Las mÃ©tricas de Airflow incluyen el tiempo de ejecuciÃ³n
   de las tareas mismas, o solo el overhead del scheduler?

4. Â¿Managed Airflow (MWAA, Composer) expone las mismas mÃ©tricas StatsD?

5. Â¿Dagster y Prefect tienen mÃ©tricas equivalentes?

---

### Ejercicio 18.1.4 â€” Detectar anomalÃ­as en mÃ©tricas de pipeline

```python
import numpy as np
from datetime import datetime, timedelta

# Detectar si la mÃ©trica de hoy es anÃ³mala respecto al histÃ³rico.
# No hardcodear umbrales â€” usar estadÃ­sticas del propio pipeline.

def detectar_anomalia(
    valor_actual: float,
    historico: list[float],
    metodo: str = "zscore",
    umbral: float = 3.0,
) -> dict:
    """
    Detectar si el valor actual es anÃ³malo respecto al histÃ³rico.

    MÃ©todos:
    - zscore: |valor - media| / std > umbral
    - iqr: valor < Q1 - 1.5*IQR o valor > Q3 + 1.5*IQR
    - pct_change: |cambio porcentual vs ayer| > umbral (en %)
    """
    historico = np.array(historico)

    if metodo == "zscore":
        media = np.mean(historico)
        std = np.std(historico)
        if std == 0:
            return {"anomalo": valor_actual != media, "metodo": "zscore",
                    "detalle": "std=0, comparaciÃ³n exacta"}
        zscore = abs(valor_actual - media) / std
        return {"anomalo": zscore > umbral, "metodo": "zscore",
                "zscore": round(zscore, 2), "media": round(media, 2)}

    elif metodo == "iqr":
        q1, q3 = np.percentile(historico, [25, 75])
        iqr = q3 - q1
        lower = q1 - 1.5 * iqr
        upper = q3 + 1.5 * iqr
        return {"anomalo": valor_actual < lower or valor_actual > upper,
                "metodo": "iqr", "rango": [round(lower, 2), round(upper, 2)]}

    elif metodo == "pct_change":
        ayer = historico[-1]
        if ayer == 0:
            return {"anomalo": valor_actual != 0, "metodo": "pct_change"}
        cambio = abs(valor_actual - ayer) / ayer * 100
        return {"anomalo": cambio > umbral, "metodo": "pct_change",
                "cambio_pct": round(cambio, 2)}


# Ejemplo: verificar revenue diario
historico_30d = [12500, 13200, 11800, 14500, 12900, 13100, ...]  # Ãºltimos 30 dÃ­as
resultado = detectar_anomalia(valor_actual=150, historico=historico_30d)
# {"anomalo": True, "zscore": 8.7, "media": 12850}
# Revenue de $150 cuando el promedio es $12,850 â†’ claramente anÃ³malo
```

**Preguntas:**

1. Â¿Z-score es robusto para datos con estacionalidad
   (ej: revenue de fin de semana vs entre semana)?

2. Â¿IQR es mejor que z-score para detectar anomalÃ­as en datos con outliers?

3. Â¿CuÃ¡ntos dÃ­as de histÃ³rico son suficientes? Â¿7? Â¿30? Â¿90?

4. Â¿La detecciÃ³n de anomalÃ­as debe ejecutarse dentro del pipeline
   o en un sistema separado?

5. Â¿Monte Carlo Simulation (Cap.18 de otro dominio) aplica
   para predecir valores esperados de mÃ©tricas?

**Pista:** Para datos con estacionalidad (fin de semana vs laborable),
usa ventanas por dÃ­a de la semana: compara el lunes de hoy con los Ãºltimos
4 lunes, no con los Ãºltimos 30 dÃ­as. Para holidays, necesitas un calendario
de excepciones. NingÃºn mÃ©todo estadÃ­stico simple maneja esto bien â€”
en la prÃ¡ctica, la mayorÃ­a de los equipos usan z-score con ventanas
por dÃ­a de semana + un diccionario de excepciones conocidas.

---

### Ejercicio 18.1.5 â€” Implementar: tabla de mÃ©tricas del pipeline

**Tipo: Implementar**

```python
# DiseÃ±ar una tabla que almacene las mÃ©tricas de cada run del pipeline.
# Esta tabla es consultable por SQL â†’ dashboards, anÃ¡lisis de tendencias.

# Schema:
# pipeline_metrics (
#   run_id            STRING    -- UUID del run
#   pipeline_name     STRING    -- "ecommerce_daily"
#   run_date          DATE      -- fecha de datos procesados
#   started_at        TIMESTAMP -- cuÃ¡ndo iniciÃ³
#   completed_at      TIMESTAMP -- cuÃ¡ndo terminÃ³ (NULL si fallÃ³)
#   status            STRING    -- "success", "failed", "running"
#   duration_seconds  FLOAT     -- duraciÃ³n total
#   rows_read         INT64     -- filas leÃ­das
#   rows_written      INT64     -- filas escritas
#   rows_dropped      INT64     -- filas descartadas
#   drop_rate         FLOAT     -- rows_dropped / (rows_read + rows_dropped)
#   bytes_read        INT64     -- bytes leÃ­dos
#   bytes_written     INT64     -- bytes escritos
#   error_message     STRING    -- mensaje de error (NULL si Ã©xito)
#   step_durations    JSON      -- {"extraer": 45.2, "transformar": 120.5}
#   metadata          JSON      -- cualquier contexto adicional
# )

# Queries Ãºtiles sobre esta tabla:
# 1. Tendencia de duraciÃ³n del pipeline (Ãºltimos 30 dÃ­as)
# 2. Top 5 pipelines mÃ¡s lentos esta semana
# 3. Pipelines con drop_rate > 1%
# 4. Tasa de Ã©xito por pipeline (Ãºltimos 7 dÃ­as)
# 5. Alertar si duration > 2Ã— promedio_historico
```

**Restricciones:**
1. Implementar la tabla y la lÃ³gica de inserciÃ³n
2. Implementar las 5 queries listadas
3. Crear un check que detecte anomalÃ­as en duraciÃ³n y drop_rate
4. Â¿BigQuery, PostgreSQL, o la metadata DB de Airflow? Justificar.

---

## SecciÃ³n 18.2 â€” Logging Estructurado para Data Engineering

### Ejercicio 18.2.1 â€” Leer: por quÃ© print() no es logging

**Tipo: Leer**

```
print() vs logging estructurado:

  print():
    print(f"Procesando ventas del 2024-03-14, encontrÃ© 15432 filas")
    # Output: Procesando ventas del 2024-03-14, encontrÃ© 15432 filas

    Problemas:
    - No tiene timestamp (Â¿cuÃ¡ndo ocurriÃ³?)
    - No tiene nivel (Â¿es info, warning, error?)
    - No es parseable (Â¿cÃ³mo buscas "filas > 10000" en 1M de logs?)
    - No tiene contexto (Â¿quÃ© pipeline? Â¿quÃ© run? Â¿quÃ© tarea?)

  structlog (JSON):
    log.info("ventas_procesadas", fecha="2024-03-14", filas=15432,
             pipeline="ecommerce_daily", run_id="abc-123")
    # Output:
    # {"event": "ventas_procesadas", "fecha": "2024-03-14",
    #  "filas": 15432, "pipeline": "ecommerce_daily",
    #  "run_id": "abc-123", "timestamp": "2024-03-15T03:12:45Z",
    #  "level": "info"}

    Ventajas:
    - Timestamp automÃ¡tico
    - Nivel explÃ­cito (info/warning/error)
    - Parseable (JSON â†’ buscar con jq, CloudWatch Insights, BigQuery)
    - Contexto automÃ¡tico (pipeline, run_id, tarea)
    - CorrelaciÃ³n (buscar TODOS los logs del run abc-123)

En data engineering, logs estructurados permiten:
  - "Â¿CuÃ¡ntas filas procesÃ³ cada run de la Ãºltima semana?"
    â†’ SELECT filas FROM logs WHERE event = 'ventas_procesadas'
  - "Â¿QuÃ© runs tuvieron errores de parsing?"
    â†’ SELECT * FROM logs WHERE level = 'error' AND event LIKE '%parsing%'
  - "Â¿El pipeline de hoy procesÃ³ menos filas que ayer?"
    â†’ Comparar filas entre dos runs
```

**Preguntas:**

1. Â¿structlog vs logging estÃ¡ndar de Python â€” cuÃ¡l para data engineering?

2. Â¿Los logs de un pipeline de Spark van a stdout de los executors?
   Â¿CÃ³mo los centralizas?

3. Â¿JSON logs son legibles para humanos?
   Â¿CÃ³mo balanceas legibilidad y parseabilidad?

4. Â¿CuÃ¡nto log es demasiado? Â¿Un log por fila procesada es viable?

5. Â¿Los logs de Airflow y los logs del pipeline se mezclan?
   Â¿CÃ³mo los separas?

---

### Ejercicio 18.2.2 â€” Configurar structlog para pipelines de datos

```python
import structlog
import logging
import sys

def configurar_logging(pipeline_name: str, run_id: str):
    """Configurar structlog con contexto automÃ¡tico del pipeline."""

    structlog.configure(
        processors=[
            structlog.contextvars.merge_contextvars,
            structlog.processors.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.JSONRenderer(),
        ],
        wrapper_class=structlog.make_filtering_bound_logger(logging.INFO),
        context_class=dict,
        logger_factory=structlog.PrintLoggerFactory(),
    )

    # Bind contexto global (aparece en TODOS los logs):
    structlog.contextvars.clear_contextvars()
    structlog.contextvars.bind_contextvars(
        pipeline=pipeline_name,
        run_id=run_id,
    )


# Uso:
configurar_logging("ecommerce_daily", "run-2024-03-14-abc123")
log = structlog.get_logger()

log.info("pipeline_started", fecha="2024-03-14")
# {"event": "pipeline_started", "fecha": "2024-03-14",
#  "pipeline": "ecommerce_daily", "run_id": "run-2024-03-14-abc123",
#  "timestamp": "2024-03-15T03:00:01Z", "level": "info"}

log.info("step_completed", step="extraer", rows=15432, duration_s=45.2)
# {"event": "step_completed", "step": "extraer", "rows": 15432,
#  "duration_s": 45.2, "pipeline": "ecommerce_daily", ...}

try:
    resultado = transformar(df)
except Exception as e:
    log.error("step_failed", step="transformar", error=str(e), exc_info=True)
    raise
```

**Preguntas:**

1. Â¿`contextvars` de structlog funciona correctamente con multiprocessing?
   Â¿Y con threads?

2. Â¿`JSONRenderer` es para producciÃ³n y `ConsoleRenderer` para desarrollo?

3. Â¿Los logs deben ir a stdout, a un archivo, o a ambos?

4. Â¿CÃ³mo evitas que un log con `exc_info=True` exponga datos sensibles
   en los stack traces?

5. Â¿structlog puede integrarse con el logging de Airflow?

---

### Ejercicio 18.2.3 â€” Log levels: cuÃ¡ndo usar cada uno

```python
import structlog
log = structlog.get_logger()

# DEBUG: informaciÃ³n detallada para debugging (NO en producciÃ³n por defecto)
log.debug("query_ejecutada", sql="SELECT COUNT(*) FROM ventas WHERE ...",
          parametros={"fecha": "2024-03-14"})

# INFO: eventos normales del pipeline (el "heartbeat" del sistema)
log.info("step_completed", step="extraer", rows=15432, duration_s=45.2)
log.info("quality_check_passed", check="min_rows", expected=100, actual=15432)

# WARNING: algo inesperado pero no fatal (el pipeline continÃºa)
log.warning("rows_dropped", count=23, reason="null_user_id",
            drop_rate=0.0015)
log.warning("slow_query", query="groupby_region", duration_s=180,
            expected_s=60)
log.warning("schema_change_detected", columna_nueva="discount_code",
            tipo="string")

# ERROR: algo fallÃ³ (la tarea puede reintentar o fallar)
log.error("step_failed", step="transformar",
          error="OutOfMemoryError: Java heap space",
          rows_input=15432, heap_max_mb=4096)

# CRITICAL: el pipeline no puede continuar, intervenciÃ³n humana necesaria
log.critical("data_corruption_detected",
             tabla="metricas_diarias",
             detalle="revenue negativo en 500 registros",
             accion="pipeline detenido, se requiere investigaciÃ³n")
```

**Preguntas:**

1. Â¿WARNING para `rows_dropped` o solo si supera un umbral?

2. Â¿Un `schema_change_detected` es WARNING o ERROR?
   Â¿Depende del contexto?

3. Â¿CuÃ¡ntos logs INFO por minuto son aceptables
   antes de que la factura de CloudWatch sea un problema?

4. Â¿ERROR debe siempre acompaÃ±arse de un stack trace?

5. Â¿CÃ³mo defines la polÃ­tica de log levels para un equipo de 5 personas?

---

### Ejercicio 18.2.4 â€” Centralizar logs: ELK, CloudWatch, y alternativas

```python
# PatrÃ³n: enviar logs a un sistema centralizado para bÃºsqueda y alertas.

# OpciÃ³n 1: CloudWatch (AWS)
# Los logs de Airflow en MWAA van automÃ¡ticamente a CloudWatch.
# Los logs del pipeline necesitan un handler:
import watchtower
import logging

cloudwatch_handler = watchtower.CloudWatchLogHandler(
    log_group="/data-engineering/pipelines",
    stream_name="ecommerce-daily-{date}",
)
logging.getLogger().addHandler(cloudwatch_handler)

# Buscar en CloudWatch Insights:
# fields @timestamp, @message
# | filter pipeline = "ecommerce_daily"
# | filter level = "error"
# | sort @timestamp desc
# | limit 50

# OpciÃ³n 2: ELK Stack (Elasticsearch + Logstash + Kibana)
# Los logs JSON de structlog se envÃ­an a Logstash via Filebeat.
# Kibana permite dashboards y alertas sobre los logs.

# OpciÃ³n 3: Datadog / Grafana Loki
# Datadog: integraciÃ³n directa con structlog via ddtrace.
# Loki: alternativa ligera a Elasticsearch, integrada con Grafana.

# OpciÃ³n 4: BigQuery como log sink
# Para anÃ¡lisis SQL sobre logs histÃ³ricos:
# Los logs JSON se escriben a un topic de Pub/Sub â†’ BigQuery.
# Ventaja: consultas SQL sobre meses de logs.
# Desventaja: latencia de minutos (no real-time).
```

**Preguntas:**

1. Â¿CloudWatch Insights vs Elasticsearch â€” cuÃ¡l para buscar logs
   de pipelines de datos?

2. Â¿Grafana Loki es suficiente para un equipo de 10 data engineers?

3. Â¿Logs en BigQuery tiene sentido para auditorÃ­a y compliance?

4. Â¿El costo de almacenar logs es significativo?
   Â¿CuÃ¡l es la polÃ­tica de retenciÃ³n razonable?

5. Â¿Los logs de Spark executors deben ir al mismo sistema
   que los logs de Airflow?

---

### Ejercicio 18.2.5 â€” Implementar: correlacionar logs entre componentes

**Tipo: Implementar**

```python
# Problema: un pipeline involucra Airflow, Spark, y BigQuery.
# Cada componente genera sus propios logs.
# Â¿CÃ³mo correlacionas los logs de una ejecuciÃ³n especÃ­fica?

# SoluciÃ³n: propagar un trace_id a travÃ©s de todos los componentes.

# Airflow task:
def extraer_ventas(**context):
    run_id = context["run_id"]
    dag_id = context["dag"].dag_id
    task_id = context["task_instance"].task_id

    # Generar trace_id Ãºnico para esta ejecuciÃ³n:
    trace_id = f"{dag_id}-{run_id}-{task_id}"

    # Pasar a Spark:
    spark = SparkSession.builder \
        .config("spark.app.name", f"extraer-{trace_id}") \
        .getOrCreate()
    spark.sparkContext.setLocalProperty("trace_id", trace_id)

    # Pasar a BigQuery:
    job_config = bigquery.QueryJobConfig(
        labels={"trace_id": trace_id}
    )
```

**Restricciones:**
1. Implementar propagaciÃ³n de trace_id entre Airflow â†’ Spark â†’ BigQuery
2. Implementar una query que busque todos los logs de un trace_id
3. Â¿OpenTelemetry es una alternativa? Â¿EstÃ¡ maduro para data engineering?
4. Â¿El trace_id debe incluirse en los datos mismos (como una columna)?

---

## SecciÃ³n 18.3 â€” Data Lineage: de DÃ³nde Vienen los Datos

### Ejercicio 18.3.1 â€” Leer: quÃ© es lineage y por quÃ© importa

**Tipo: Leer**

```
Data lineage responde dos preguntas:

  1. Upstream (Â¿de dÃ³nde vienen?):
     "La tabla metricas_diarias â€” Â¿de quÃ© fuentes se alimenta?"
     â†’ PostgreSQL.ventas + Kafka.eventos + S3.clientes

  2. Downstream (Â¿a quiÃ©n afecta?):
     "Si cambio la tabla ventas_raw â€” Â¿quÃ© dashboards se rompen?"
     â†’ metricas_diarias â†’ dashboard_revenue â†’ reporte_ejecutivo

Niveles de lineage:

  Nivel 1: Table-level lineage
    ventas_raw â†’ metricas_diarias â†’ dashboard
    "Esta tabla viene de esta otra tabla"
    â†’ La mayorÃ­a de las herramientas hacen esto

  Nivel 2: Column-level lineage
    ventas_raw.monto â†’ metricas_diarias.revenue (via SUM)
    ventas_raw.region â†’ metricas_diarias.region (passthrough)
    "Esta columna viene de esta otra columna, con esta transformaciÃ³n"
    â†’ MÃ¡s difÃ­cil, requiere parsear SQL o analizar el cÃ³digo

  Nivel 3: Row-level lineage
    La fila 42 de metricas_diarias viene de las filas 100-150 de ventas_raw
    "Este registro especÃ­fico viene de estos registros especÃ­ficos"
    â†’ Muy costoso, solo para auditorÃ­a regulatoria (ej: finanzas)

Â¿CuÃ¡ndo el lineage salva el dÃ­a?

  Escenario: el dashboard de revenue muestra nÃºmeros incorrectos desde el martes.
  Sin lineage: "Â¿QuÃ© cambiÃ³? Â¿Fue la extracciÃ³n? Â¿La transformaciÃ³n?
                Â¿Un cambio de schema en PostgreSQL? Â¿Un deploy de dbt?"
  Con lineage: "metricas_diarias depende de ventas_raw,
                que depende del extract de PostgreSQL.
                El martes se modificÃ³ la columna 'descuento' en PostgreSQL.
                El extract no falla pero el cÃ¡lculo de revenue no incluye descuentos."
```

**Preguntas:**

1. Â¿Lineage de tablas es suficiente para la mayorÃ­a de los casos?
   Â¿CuÃ¡ndo necesitas lineage de columnas?

2. Â¿dbt genera lineage automÃ¡ticamente?

3. Â¿El lineage de Dagster (asset dependencies) es equivalente
   al lineage de una herramienta dedicada como OpenLineage?

4. Â¿El lineage captura transformaciones en Spark
   (no solo SQL)?

5. Â¿Lineage y data catalog son lo mismo?

---

### Ejercicio 18.3.2 â€” OpenLineage: el estÃ¡ndar abierto de lineage

```python
# OpenLineage es el estÃ¡ndar abierto para emitir y consumir eventos de lineage.
# Integraciones: Airflow, Spark, dbt, Flink, Great Expectations.

# Un evento OpenLineage tiene esta estructura:
import json

evento_lineage = {
    "eventType": "COMPLETE",
    "eventTime": "2024-03-15T03:45:00Z",
    "run": {
        "runId": "run-2024-03-14-abc123",
    },
    "job": {
        "namespace": "ecommerce",
        "name": "calcular_metricas_diarias",
    },
    "inputs": [
        {
            "namespace": "bigquery",
            "name": "proyecto.raw.ventas",
            "facets": {
                "schema": {
                    "fields": [
                        {"name": "user_id", "type": "STRING"},
                        {"name": "monto", "type": "FLOAT64"},
                        {"name": "region", "type": "STRING"},
                    ]
                },
                "dataQualityMetrics": {
                    "rowCount": 15432,
                    "bytes": 2_500_000,
                }
            }
        }
    ],
    "outputs": [
        {
            "namespace": "bigquery",
            "name": "proyecto.analytics.metricas_diarias",
            "facets": {
                "schema": {
                    "fields": [
                        {"name": "region", "type": "STRING"},
                        {"name": "revenue", "type": "FLOAT64"},
                        {"name": "transacciones", "type": "INT64"},
                    ]
                },
                "dataQualityMetrics": {
                    "rowCount": 5,
                }
            }
        }
    ]
}

# Airflow emite estos eventos automÃ¡ticamente si instalas:
# pip install openlineage-airflow

# Marquez (https://marquezproject.ai) es el servidor open-source
# que recolecta y visualiza eventos OpenLineage.
```

**Preguntas:**

1. Â¿OpenLineage captura el lineage automÃ¡ticamente o requiere instrumentaciÃ³n?

2. Â¿Marquez vs Datahub vs Amundsen â€” cuÃ¡l para lineage open-source?

3. Â¿OpenLineage funciona con Dagster y Prefect?

4. Â¿Los "facets" de OpenLineage pueden incluir data quality metrics?

5. Â¿Column-level lineage estÃ¡ soportado en OpenLineage?

---

### Ejercicio 18.3.3 â€” Lineage en la prÃ¡ctica: rastrear un error hasta la fuente

**Tipo: Diagnosticar**

```
Escenario: el analista reporta que el revenue de la regiÃ³n "centro"
cayÃ³ 80% el dÃ­a 14 de marzo.

Con lineage, el proceso de investigaciÃ³n es:

  1. Buscar en Marquez: "metricas_diarias, fecha=2024-03-14, region=centro"
     â†’ Input: ventas_raw (15432 filas), clientes_dim (50000 filas)

  2. Verificar ventas_raw para regiÃ³n centro:
     â†’ Solo 200 filas para "centro" (normalmente son 3000)
     â†’ El problema estÃ¡ en la extracciÃ³n, no en la transformaciÃ³n

  3. Buscar upstream de ventas_raw:
     â†’ Fuente: PostgreSQL, tabla "orders", schema "production"
     â†’ El extract del 14 de marzo usÃ³ un filtro incorrecto

  4. Verificar el extract:
     â†’ El filtro era WHERE region = 'centro'
     â†’ Pero el 13 de marzo, el equipo de backend renombrÃ³
       la regiÃ³n "centro" a "central" en PostgreSQL
     â†’ El extract obtuvo solo los 200 registros legacy

  5. Fix: actualizar el filtro a WHERE region IN ('centro', 'central')
     â†’ Re-ejecutar el backfill del 14 de marzo
     â†’ Verificar que las mÃ©tricas se corrigen

Sin lineage, el paso 1-3 habrÃ­an tardado horas en vez de minutos.
```

**Preguntas:**

1. Â¿Este tipo de investigaciÃ³n es posible solo con Airflow logs?

2. Â¿Notificaciones proactivas ("el schema de PostgreSQL cambiÃ³")
   habrÃ­an prevenido el incidente?

3. Â¿Column-level lineage habrÃ­a acelerado la investigaciÃ³n?

4. Â¿CÃ³mo automatizas la detecciÃ³n de "region renombrada en upstream"?

5. Â¿El lineage debe ser parte del pipeline o un sistema paralelo?

---

### Ejercicio 18.3.4 â€” Implementar: lineage casero con metadata tables

**Tipo: Implementar**

```python
# Si no tienes OpenLineage/Marquez, puedes implementar lineage bÃ¡sico
# con una tabla de metadata:

# pipeline_lineage (
#   run_id            STRING
#   pipeline_name     STRING
#   run_date          DATE
#   input_table       STRING    -- "raw.ventas"
#   input_row_count   INT64
#   input_schema_hash STRING    -- hash del schema para detectar cambios
#   output_table      STRING    -- "analytics.metricas_diarias"
#   output_row_count  INT64
#   transformation    STRING    -- "groupby_region_sum_monto"
#   created_at        TIMESTAMP
# )

# Registrar lineage en cada paso del pipeline:
def registrar_lineage(run_id, pipeline, input_table, output_table,
                      input_rows, output_rows, transformation):
    bq_client.query(f"""
        INSERT INTO metadata.pipeline_lineage
        VALUES ('{run_id}', '{pipeline}', CURRENT_DATE(),
                '{input_table}', {input_rows}, ...,
                '{output_table}', {output_rows},
                '{transformation}', CURRENT_TIMESTAMP())
    """)

# Query: "Â¿de dÃ³nde viene la tabla metricas_diarias?"
# SELECT * FROM metadata.pipeline_lineage
# WHERE output_table = 'analytics.metricas_diarias'
# ORDER BY created_at DESC
```

**Restricciones:**
1. Implementar la tabla y la funciÃ³n de registro
2. Implementar una query recursiva que trace el lineage completo
   (output â†’ input â†’ input del input â†’ ...)
3. Implementar detecciÃ³n de schema changes (comparar hash del schema)
4. Â¿CuÃ¡ndo este enfoque casero es suficiente y cuÃ¡ndo necesitas OpenLineage?

---

### Ejercicio 18.3.5 â€” Data catalog: descubribilidad de datos

**Tipo: Analizar**

```
Data catalog vs data lineage:

  Lineage: "Â¿de dÃ³nde viene este dato?"
  Catalog: "Â¿quÃ© datos tenemos y quÃ© significan?"

  Un data catalog es un inventario de todos los datasets,
  con metadata: descripciÃ³n, owner, schema, tags, freshness.

Herramientas:
  Open-source: Datahub (LinkedIn), Amundsen (Lyft), Apache Atlas
  Managed: Google Data Catalog, AWS Glue Data Catalog, Alation

  Datahub incluye:
  - Catalog (inventario)
  - Lineage (dependencias)
  - Quality (tests)
  - Governance (ownership, PII tags)

Ejemplo de metadata en un catalog:
  tabla: analytics.metricas_diarias
  descripciÃ³n: "Revenue y transacciones por regiÃ³n, calculado diariamente"
  owner: data-engineering@empresa.com
  freshness: Ãºltima actualizaciÃ³n hace 2 horas
  schema: region (STRING), revenue (FLOAT64), transacciones (INT64)
  tags: [pii:no, tier:gold, domain:ecommerce]
  lineage: depende de raw.ventas, dim.clientes
  quality: 5/5 tests pasando
```

**Preguntas:**

1. Â¿Un data catalog es necesario para un equipo de 5 personas?
   Â¿A partir de cuÃ¡ntas tablas vale la pena?

2. Â¿dbt docs genera un catalog bÃ¡sico?

3. Â¿Datahub vs Amundsen â€” cuÃ¡l recomiendas en 2024?

4. Â¿El catalog debe ser mantenido manualmente o generado automÃ¡ticamente?

5. Â¿Tags de PII en el catalog ayudan con compliance (GDPR, etc.)?

---

## SecciÃ³n 18.4 â€” Data Quality: el Pilar que Falta

### Ejercicio 18.4.1 â€” Leer: las cinco dimensiones de la calidad de datos

**Tipo: Leer**

```
Las cinco dimensiones (el "framework" de data quality):

  1. FRESHNESS (frescura)
     "Â¿Los datos son recientes?"
     Esperado: la tabla de mÃ©tricas se actualiza antes de las 6:00 UTC.
     Check: MAX(updated_at) > NOW() - INTERVAL 6 HOURS

  2. VOLUME (volumen)
     "Â¿Hay la cantidad esperada de datos?"
     Esperado: entre 10,000 y 100,000 filas por dÃ­a.
     Check: COUNT(*) BETWEEN 10000 AND 100000

  3. SCHEMA (estructura)
     "Â¿Las columnas y tipos son correctos?"
     Esperado: columna 'monto' es FLOAT64, no STRING.
     Check: column_type('monto') == 'FLOAT64'

  4. DISTRIBUTION (distribuciÃ³n)
     "Â¿Los valores tienen sentido?"
     Esperado: monto > 0, region IN ('norte','sur','este','oeste').
     Check: MIN(monto) >= 0 AND COUNT(DISTINCT region) <= 10

  5. UNIQUENESS (unicidad)
     "Â¿Hay duplicados?"
     Esperado: cada (user_id, timestamp) es Ãºnico.
     Check: COUNT(*) == COUNT(DISTINCT user_id, timestamp)

Cada dimensiÃ³n se puede monitorear de forma independiente,
y cada una puede tener su propio umbral de alerta.

Herramientas:
  - Great Expectations: framework Python, declarativo, amplio
  - dbt tests: SQL-based, integrado con dbt
  - Soda: YAML-based, cloud y open-source
  - Elementary: monitoring de dbt nativo
  - Monte Carlo / Bigeye: SaaS, zero-config anomaly detection
```

**Preguntas:**

1. Â¿Las cinco dimensiones son exhaustivas?
   Â¿QuÃ© dimensiÃ³n falta?

2. Â¿Freshness es la dimensiÃ³n mÃ¡s fÃ¡cil de monitorear?

3. Â¿Distribution checks pueden detectar data drift
   (cambios graduales que no son errores)?

4. Â¿Uniqueness checks deben ejecutarse ANTES o DESPUÃ‰S de cargar datos?

5. Â¿Data quality es responsabilidad del data engineer
   o del analytics engineer?

---

### Ejercicio 18.4.2 â€” Great Expectations: data quality como cÃ³digo

```python
import great_expectations as gx

# Great Expectations define "expectativas" sobre los datos.
# Una expectativa = una afirmaciÃ³n que debe ser verdadera.

# Crear un contexto:
context = gx.get_context()

# Definir un datasource (conexiÃ³n a los datos):
datasource = context.sources.add_pandas("mi_datasource")
asset = datasource.add_dataframe_asset("ventas")

# Definir expectativas (suite):
suite = context.add_or_update_expectation_suite("ventas_quality")

# Expectativa 1: la tabla no estÃ¡ vacÃ­a
suite.add_expectation(
    gx.expectations.ExpectTableRowCountToBeBetween(min_value=100, max_value=1_000_000)
)

# Expectativa 2: no hay nulls en user_id
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToNotBeNull(column="user_id")
)

# Expectativa 3: monto es positivo
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToBeBetween(
        column="monto", min_value=0.01, max_value=1_000_000
    )
)

# Expectativa 4: region es un valor conocido
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToBeInSet(
        column="region", value_set=["norte", "sur", "este", "oeste", "centro"]
    )
)

# Expectativa 5: no hay duplicados en (user_id, timestamp)
suite.add_expectation(
    gx.expectations.ExpectCompoundColumnsToBeUnique(column_list=["user_id", "timestamp"])
)

# Ejecutar validaciÃ³n:
batch = asset.get_batch(dataframe=df_ventas)
results = batch.validate(suite)

if not results.success:
    fallos = [r for r in results.results if not r.success]
    for f in fallos:
        print(f"FALLO: {f.expectation_config.expectation_type} â€” {f.result}")
    raise ValueError(f"{len(fallos)} quality checks fallaron")
```

**Preguntas:**

1. Â¿Great Expectations ejecuta las validaciones en memoria o en la DB?

2. Â¿Las expectativas deben definirse en cÃ³digo o en YAML?

3. Â¿CuÃ¡ntas expectativas por tabla es razonable? Â¿5? Â¿50? Â¿500?

4. Â¿Great Expectations genera reportes HTML? Â¿Son Ãºtiles?

5. Â¿CÃ³mo integras Great Expectations en un DAG de Airflow?

---

### Ejercicio 18.4.3 â€” dbt tests: quality integrado en las transformaciones

```sql
-- dbt tests: validaciones declaradas en YAML junto al modelo.

-- models/analytics/metricas_diarias.sql
SELECT
    fecha,
    region,
    SUM(monto) AS revenue,
    COUNT(DISTINCT user_id) AS usuarios_unicos,
    COUNT(*) AS transacciones
FROM {{ ref('stg_ventas') }}
GROUP BY 1, 2

-- models/analytics/schema.yml
version: 2
models:
  - name: metricas_diarias
    description: "MÃ©tricas diarias de revenue por regiÃ³n"
    columns:
      - name: fecha
        tests:
          - not_null
          - unique  -- combinado con region para PK
      - name: region
        tests:
          - not_null
          - accepted_values:
              values: ['norte', 'sur', 'este', 'oeste', 'centro']
      - name: revenue
        tests:
          - not_null
          - dbt_utils.expression_is_true:
              expression: ">= 0"
      - name: usuarios_unicos
        tests:
          - not_null
          - dbt_utils.expression_is_true:
              expression: "> 0"

    # Tests a nivel de tabla:
    tests:
      - dbt_utils.unique_combination_of_columns:
          combination_of_columns: [fecha, region]
      - dbt_utils.expression_is_true:
          expression: "transacciones >= usuarios_unicos"
          # No puede haber mÃ¡s usuarios Ãºnicos que transacciones

-- Ejecutar tests: dbt test --select metricas_diarias
-- Los tests fallidos bloquean el pipeline (en el DAG de Airflow/Dagster)
```

**Preguntas:**

1. Â¿dbt tests ejecutan despuÃ©s de que el modelo se materializa
   o pueden ejecutar como pre-check?

2. Â¿`dbt_utils.expression_is_true` puede expresar cualquier validaciÃ³n SQL?

3. Â¿Los tests de dbt reemplazan a Great Expectations?
   Â¿O son complementarios?

4. Â¿Elementary (herramienta de monitoring para dbt) aÃ±ade
   anomaly detection automÃ¡tica sobre los tests de dbt?

5. Â¿CuÃ¡ntos tests de dbt tiene un proyecto tÃ­pico de 50 modelos?

---

### Ejercicio 18.4.4 â€” Freshness monitoring: los datos mÃ¡s peligrosos son los stale

```python
# Freshness: verificar que los datos se actualizan a tiempo.
# Datos stale = datos del dÃ­a anterior presentados como si fueran de hoy.

# PatrÃ³n 1: query directa
def verificar_freshness(tabla: str, columna_tiempo: str,
                        max_age_hours: int = 6) -> bool:
    query = f"""
        SELECT
            MAX({columna_tiempo}) AS ultimo_dato,
            TIMESTAMP_DIFF(CURRENT_TIMESTAMP(), MAX({columna_tiempo}), HOUR) AS age_hours
        FROM `{tabla}`
    """
    result = bq_client.query(query).result().to_dataframe()
    age = result["age_hours"].iloc[0]

    if age > max_age_hours:
        log.warning("datos_stale", tabla=tabla, age_hours=age,
                    max_allowed=max_age_hours)
        return False
    return True

# PatrÃ³n 2: dbt source freshness
# En sources.yml:
# sources:
#   - name: raw
#     tables:
#       - name: ventas
#         loaded_at_field: updated_at
#         freshness:
#           warn_after: {count: 6, period: hour}
#           error_after: {count: 12, period: hour}
#
# Ejecutar: dbt source freshness

# PatrÃ³n 3: check periÃ³dico con Airflow Sensor
from airflow.sensors.sql import SqlSensor

freshness_check = SqlSensor(
    task_id="verificar_freshness_ventas",
    conn_id="bigquery",
    sql="""
        SELECT COUNT(*) FROM `raw.ventas`
        WHERE DATE(updated_at) = CURRENT_DATE()
        HAVING COUNT(*) > 0
    """,
    timeout=3600,
    poke_interval=300,
)
```

**Preguntas:**

1. Â¿Freshness check antes o despuÃ©s de la transformaciÃ³n?

2. Â¿Datos de 6 horas de antigÃ¼edad son "frescos" para un dashboard de revenue?
   Â¿Y para detecciÃ³n de fraude?

3. Â¿`dbt source freshness` es la forma mÃ¡s simple de monitorear freshness?

4. Â¿CÃ³mo manejas freshness para tablas que se actualizan irregularmente
   (ej: catÃ¡logo de productos)?

5. Â¿Un dashboard debe mostrar "datos actualizados hace X horas"
   como indicador de confianza?

---

### Ejercicio 18.4.5 â€” Implementar: quality framework integrado en el pipeline

**Tipo: Implementar**

```python
# DiseÃ±ar un framework de quality checks que:
# 1. Ejecute ANTES de cargar datos (pre-check)
# 2. Ejecute DESPUÃ‰S de cargar datos (post-check)
# 3. Bloquee el pipeline si checks crÃ­ticos fallan
# 4. Alerte pero no bloquee si checks no-crÃ­ticos fallan
# 5. Registre resultados en una tabla de histÃ³rico

# Schema para la tabla de histÃ³rico:
# quality_results (
#   check_id        STRING    -- "ventas_min_rows"
#   run_id          STRING
#   run_date        DATE
#   table_name      STRING    -- "raw.ventas"
#   check_type      STRING    -- "volume", "freshness", "schema", etc.
#   severity        STRING    -- "critical", "warning", "info"
#   passed          BOOLEAN
#   expected_value  STRING    -- ">=100"
#   actual_value    STRING    -- "15432"
#   message         STRING    -- detalle del resultado
#   checked_at      TIMESTAMP
# )

# Con este histÃ³rico puedes:
# - Ver la tasa de Ã©xito de cada check en el tiempo
# - Detectar checks que fallan intermitentemente
# - Generar reportes de quality para stakeholders
```

**Restricciones:**
1. Implementar al menos 10 checks cubriendo las 5 dimensiones
2. Implementar la lÃ³gica de bloqueo (critical) vs alerta (warning)
3. Implementar la tabla de histÃ³rico y queries de anÃ¡lisis
4. Integrar como tarea de Airflow entre extract y transform

---

## SecciÃ³n 18.5 â€” Dashboards Operativos y Alertas

### Ejercicio 18.5.1 â€” El dashboard operativo: quÃ© debe mostrar

**Tipo: DiseÃ±ar**

```
Un dashboard operativo de pipelines NO es un dashboard de negocio.
Es la vista que el on-call engineer mira a las 3am
cuando recibe una alerta de PagerDuty.

Panel 1: Estado actual de pipelines (traffic light)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Pipeline                â”‚ Estado   â”‚ Ãšltima   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ ecommerce_daily         â”‚ ğŸŸ¢ OK    â”‚ 03:45    â”‚
  â”‚ fraud_detection         â”‚ ğŸŸ¡ SLOW  â”‚ 03:30    â”‚
  â”‚ inventory_sync          â”‚ ğŸ”´ FAIL  â”‚ 02:15    â”‚
  â”‚ ml_inference            â”‚ ğŸŸ¢ OK    â”‚ 04:00    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Panel 2: DuraciÃ³n del pipeline (time series, Ãºltimos 30 dÃ­as)
  [grÃ¡fico: lÃ­nea con banda de Â±2Ïƒ, punto rojo si fuera de banda]

Panel 3: Filas procesadas (time series)
  [grÃ¡fico: barras por dÃ­a, rojo si < mÃ­nimo esperado]

Panel 4: Quality checks (stacked bar)
  [grÃ¡fico: checks passed vs failed por dÃ­a]

Panel 5: Freshness (gauge por tabla)
  ventas_raw:      actualizado hace 2h  [ğŸŸ¢]
  metricas_diarias: actualizado hace 3h  [ğŸŸ¢]
  clientes_dim:    actualizado hace 25h [ğŸ”´]

Panel 6: Alertas activas
  [lista de alertas no resueltas con timestamp y severidad]
```

**Preguntas:**

1. Â¿Grafana o Looker para el dashboard operativo?

2. Â¿El dashboard operativo debe ser separado del dashboard de negocio?

3. Â¿CuÃ¡ntos paneles son demasiados? Â¿QuÃ© eliminarÃ­as?

4. Â¿El dashboard debe tener un botÃ³n de "re-ejecutar pipeline"?

5. Â¿Los stakeholders no-tÃ©cnicos necesitan acceso
   al dashboard operativo?

---

### Ejercicio 18.5.2 â€” Alertas que no se ignoran

```python
# El problema #1 de las alertas: alert fatigue.
# Si alertas por todo, la gente ignora las alertas.

# Reglas para alertas que se respetan:

# 1. Cada alerta debe ser actionable
# MAL:  "El pipeline tardÃ³ mÃ¡s de lo normal" â†’ Â¿y quÃ© hago?
# BIEN: "El pipeline tardÃ³ 3Ã— mÃ¡s de lo normal.
#        Paso mÃ¡s lento: 'transformar' (120min vs 40min normal).
#        Posible causa: data skew en la regiÃ³n 'norte'.
#        AcciÃ³n: verificar distribuciÃ³n de datos."

# 2. Cada alerta debe tener un owner claro
# MAL:  alerta a #general â†’ nadie responde
# BIEN: alerta a #data-oncall con @mention del owner del pipeline

# 3. Severidades deben significar algo
# P1 (PagerDuty, wake someone up):
#    - Dashboard de revenue muestra $0
#    - Pipeline crÃ­tico no completÃ³ antes del SLA
# P2 (Slack, responder en 1 hora):
#    - Quality check warning
#    - Pipeline 2Ã— mÃ¡s lento de lo normal
# P3 (Slack, responder en 1 dÃ­a):
#    - Schema change detectado
#    - Drop rate > 0.5% pero < 2%

# Implementar una alerta P1:
def alerta_revenue_cero(context):
    fecha = context["data_interval_start"].strftime("%Y-%m-%d")
    revenue = bq_client.query(f"""
        SELECT COALESCE(SUM(revenue), 0) AS total
        FROM analytics.metricas_diarias
        WHERE fecha = '{fecha}'
    """).result().to_dataframe()["total"].iloc[0]

    if revenue == 0:
        enviar_pagerduty(
            severity="critical",
            summary=f"Revenue = $0 para {fecha}",
            dedup_key=f"revenue-zero-{fecha}",
            details={
                "pipeline": "ecommerce_daily",
                "fecha": fecha,
                "accion": "Verificar extracciÃ³n de ventas y quality checks",
                "runbook": "https://wiki.empresa.com/runbooks/revenue-zero",
            }
        )
```

**Preguntas:**

1. Â¿`dedup_key` en PagerDuty previene alertas duplicadas?
   Â¿CÃ³mo funciona?

2. Â¿Un runbook link en la alerta reduce el MTTR (mean time to resolve)?

3. Â¿CuÃ¡ntas alertas P1 por semana son aceptables?
   Â¿Y cuÃ¡ntas P2?

4. Â¿Las alertas deben auto-resolverse cuando el problema se corrige?

5. Â¿OpsGenie vs PagerDuty â€” cuÃ¡l para data engineering?

---

### Ejercicio 18.5.3 â€” Anomaly detection automÃ¡tica vs reglas manuales

```python
# Dos enfoques para detectar problemas:

# Enfoque 1: Reglas manuales (explÃ­citas)
REGLAS = {
    "revenue_min": lambda revenue: revenue > 1000,
    "revenue_max": lambda revenue: revenue < 1_000_000,
    "filas_min": lambda rows: rows > 100,
    "drop_rate_max": lambda rate: rate < 0.05,
    "duracion_max": lambda dur: dur < 7200,
}

# Ventajas: predecibles, fÃ¡ciles de entender, sin falsos positivos
# Desventajas: no detectan anomalÃ­as nuevas, requieren mantenimiento,
#              los umbrales se desactualizan

# Enfoque 2: Anomaly detection automÃ¡tica (estadÃ­stica)
def detectar_anomalias_automaticas(tabla, columna, dias_historico=30):
    """Detectar automÃ¡ticamente valores fuera de lo normal."""
    historico = bq_client.query(f"""
        SELECT {columna}, fecha
        FROM {tabla}
        WHERE fecha BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL {dias_historico} DAY)
              AND CURRENT_DATE()
        ORDER BY fecha
    """).result().to_dataframe()

    media = historico[columna].mean()
    std = historico[columna].std()
    ultimo = historico[columna].iloc[-1]

    zscore = abs(ultimo - media) / std if std > 0 else 0
    return {
        "anomalo": zscore > 3,
        "valor": ultimo,
        "media_historica": media,
        "zscore": zscore,
    }

# Ventajas: detecta anomalÃ­as que no esperabas, se adapta automÃ¡ticamente
# Desventajas: falsos positivos, requiere histÃ³rico, difÃ­cil de debuggear

# Enfoque 3: Combinar ambos (recomendado)
# - Reglas manuales para checks crÃ­ticos conocidos (revenue > 0)
# - Anomaly detection para detectar lo inesperado (drift gradual)
```

**Preguntas:**

1. Â¿Monte Carlo (SaaS) usa anomaly detection automÃ¡tica?
   Â¿Funciona bien en la prÃ¡ctica?

2. Â¿CuÃ¡ntos falsos positivos son aceptables para anomaly detection?

3. Â¿La estacionalidad (fin de semana, holidays) rompe la detecciÃ³n
   automÃ¡tica?

4. Â¿ML-based anomaly detection (isolation forest, etc.)
   es overkill para mÃ©tricas de pipeline?

5. Â¿El enfoque combinado es mÃ¡s trabajo que solo reglas manuales?
   Â¿Vale la pena?

---

### Ejercicio 18.5.4 â€” Runbooks: quÃ© hacer cuando la alerta suena

```markdown
# Runbook: Revenue = $0 en el dashboard
# Owner: data-engineering@empresa.com
# Severidad: P1
# SLA de resoluciÃ³n: 1 hora

## SÃ­ntoma
Dashboard de revenue muestra $0 para el dÃ­a de ayer.

## DiagnÃ³stico (seguir en orden)

### Paso 1: Â¿El pipeline ejecutÃ³?
- Abrir Airflow UI â†’ DAG `ecommerce_daily` â†’ Ãºltimo run
- Si el run estÃ¡ en "failed" â†’ ir al Paso 2
- Si el run estÃ¡ en "success" â†’ ir al Paso 3

### Paso 2: Pipeline fallÃ³
- Abrir logs de la tarea fallida
- Errores comunes:
  - `ConnectionRefusedError` â†’ PostgreSQL caÃ­do â†’ contactar DBA
  - `OutOfMemoryError` â†’ escalar el Spark cluster o reducir partitions
  - `SchemaValidationError` â†’ schema cambiÃ³ upstream â†’ verificar con equipo backend
- Re-ejecutar: `airflow dags trigger ecommerce_daily -e 2024-03-14`

### Paso 3: Pipeline completÃ³ pero datos incorrectos
- Verificar tabla `raw.ventas`:
  ```sql
  SELECT COUNT(*), MIN(fecha), MAX(fecha) FROM raw.ventas
  WHERE fecha = '2024-03-14'
  ```
- Si COUNT = 0 â†’ la extracciÃ³n no trajo datos â†’ verificar filtro de fecha
- Si COUNT > 0 pero revenue = 0 â†’ verificar campo `monto`:
  ```sql
  SELECT AVG(monto), MIN(monto), MAX(monto) FROM raw.ventas
  WHERE fecha = '2024-03-14'
  ```

### Paso 4: Escalar si no se resuelve en 30 minutos
- Notificar en #data-incidents con contexto
- Contactar al tech lead de data engineering
```

**Preguntas:**

1. Â¿CuÃ¡ntos runbooks necesita un sistema con 20 pipelines?

2. Â¿Los runbooks deben estar en Confluence, GitHub, o en la propia alerta?

3. Â¿Un runbook puede automatizarse parcialmente?
   (ej: el paso de diagnÃ³stico como un script)

4. Â¿QuiÃ©n escribe los runbooks: el que creÃ³ el pipeline
   o el on-call?

5. Â¿Los runbooks deben actualizarse despuÃ©s de cada incidente?

---

### Ejercicio 18.5.5 â€” Implementar: sistema de alertas con contexto

**Tipo: Implementar**

```python
# Implementar un sistema de alertas que:
# 1. Detecte problemas (reglas + anomalÃ­as)
# 2. Enriquezca la alerta con contexto (logs, lineage, queries de diagnÃ³stico)
# 3. EnvÃ­e la alerta al canal correcto (Slack, PagerDuty, email)
# 4. Incluya un link al runbook correspondiente
# 5. Se auto-resuelva cuando el problema se corrige

# Bonus: implementar un "incident timeline" que registre:
# - CuÃ¡ndo se detectÃ³ el problema
# - CuÃ¡ndo se alertÃ³
# - CuÃ¡ndo se empezÃ³ a investigar
# - CuÃ¡ndo se resolviÃ³
# â†’ Calcular MTTD (mean time to detect) y MTTR (mean time to resolve)
```

**Restricciones:**
1. Implementar al menos 5 reglas de detecciÃ³n
2. Implementar enriquecimiento automÃ¡tico con queries SQL
3. Implementar envÃ­o a Slack con bloques formateados
4. Implementar la tabla de incident timeline

---

## SecciÃ³n 18.6 â€” Observabilidad de Spark, Flink, y Kafka

### Ejercicio 18.6.1 â€” Spark UI y mÃ©tricas: diagnosticar un job lento

```python
# Spark expone mÃ©tricas vÃ­a:
# 1. Spark UI (web): jobs, stages, tasks, storage, executors
# 2. Spark Metrics (Prometheus/Graphite): mÃ©tricas custom
# 3. SparkListener (programÃ¡tico): eventos en tiempo real

# MÃ©tricas clave de Spark para diagnosticar:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# executor.cpuTime             CPU usado por executor
# executor.runTime             tiempo total de ejecuciÃ³n
# executor.memoryUsed          memoria usada
# executor.diskBytesSpilled    datos spilled a disco (MAL)
# stage.shuffleReadBytes       bytes leÃ­dos en shuffle
# stage.shuffleWriteBytes      bytes escritos en shuffle
# task.garbageCollectionTime   tiempo en GC por task

# Si diskBytesSpilled > 0: los datos no caben en memoria
# â†’ Aumentar spark.executor.memory o reducir el partition count

# Si shuffleReadBytes es muy alto: el shuffle domina
# â†’ Verificar si hay data skew (una particiÃ³n >> otras)
# â†’ Considerar broadcast join si una tabla es pequeÃ±a

# Si garbageCollectionTime > 10% del runtime: GC problem
# â†’ Aumentar memoria o reducir el tamaÃ±o de objetos

# Emitir mÃ©tricas custom desde PySpark:
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext

# Accumulator como mÃ©trica custom:
registros_invalidos = sc.accumulator(0)

def procesar_fila(fila):
    if fila.monto < 0:
        registros_invalidos.add(1)
        return None
    return fila

# Al final del job:
print(f"Registros invÃ¡lidos: {registros_invalidos.value}")
statsd.gauge("spark.custom.registros_invalidos", registros_invalidos.value)
```

**Preguntas:**

1. Â¿Spark UI estÃ¡ disponible despuÃ©s de que el job termina?
   Â¿CÃ³mo preservas la informaciÃ³n?

2. Â¿`diskBytesSpilled > 0` siempre es un problema?
   Â¿O hay casos donde es aceptable?

3. Â¿Los accumulators de Spark son equivalentes a mÃ©tricas de Prometheus?

4. Â¿Spark History Server es suficiente como observabilidad de Spark?

5. Â¿Databricks tiene observabilidad de Spark superior a open-source?

---

### Ejercicio 18.6.2 â€” Flink metrics: observar un job de streaming

```java
// Flink expone mÃ©tricas vÃ­a su sistema de mÃ©tricas integrado.
// Se integra con Prometheus, Datadog, Graphite, etc.

// MÃ©tricas clave de Flink:
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// numRecordsIn / numRecordsOut    throughput del operador
// numRecordsInPerSecond           throughput por segundo
// currentInputWatermark           watermark actual
// numLateRecordsDropped           eventos late descartados
// checkpointDuration              duraciÃ³n del checkpoint
// checkpointSize                  tamaÃ±o del state checkpointed
// isBackPressured                 Â¿el operador estÃ¡ en backpressure?
// busyTimeMsPerSecond             % de tiempo que el operador estÃ¡ busy

// Dashboard de Grafana para Flink:
// Panel 1: Throughput (records/sec) per operator
// Panel 2: Latencia end-to-end (event time - processing time)
// Panel 3: Checkpoint duration y size (detectar state growth)
// Panel 4: Backpressure map (quÃ© operador es el cuello de botella)
// Panel 5: Watermark lag (quÃ© tan "atrÃ¡s" estÃ¡ el watermark)

// Alerta crÃ­tica: checkpoint falla
// Si Flink no puede completar un checkpoint, no puede recuperarse de fallos.
// Si checkpointDuration > timeout â†’ el job no tiene exactly-once.

// Alerta warning: backpressure sostenido
// Backpressure por > 5 minutos â†’ el pipeline no puede mantenerse al dÃ­a
// â†’ escalar parallelism o optimizar el operador lento
```

**Preguntas:**

1. Â¿Backpressure en Flink es equivalente a consumer lag en Kafka?

2. Â¿Checkpoint size creciendo linealmente indica un memory leak en state?

3. Â¿`numLateRecordsDropped` deberÃ­a ser 0 siempre?
   Â¿CuÃ¡nto es aceptable?

4. Â¿El Flink Dashboard (UI web) es suficiente o necesitas Grafana?

5. Â¿CÃ³mo observas un job de Flink en Kubernetes
   que se reinicia automÃ¡ticamente?

---

### Ejercicio 18.6.3 â€” Kafka monitoring: consumer lag y mÃ¡s allÃ¡

```python
# Kafka expone mÃ©tricas vÃ­a JMX (Java Management Extensions).
# Las mÃ¡s importantes para data engineering:

# CONSUMER LAG: la mÃ©trica #1 de Kafka
# lag = offset mÃ¡s reciente del topic - offset del consumer group
# lag = "cuÃ¡ntos mensajes no se han procesado"

# Si lag crece â†’ el consumer no puede mantenerse al dÃ­a
# Si lag = 0 â†’ el consumer estÃ¡ al dÃ­a
# Si lag oscila â†’ el consumer tiene throughput variable

# Herramientas para monitorear Kafka:
# - Burrow (LinkedIn): consumer lag monitoring dedicado
# - Kafka-exporter: exportar mÃ©tricas a Prometheus
# - Confluent Control Center: UI comercial completa
# - AKHQ / Kafdrop: UIs open-source ligeras

# MÃ©tricas clave:
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# consumer_lag                    mensajes sin procesar por partition
# consumer_lag_seconds            lag en tiempo (no solo offsets)
# bytes_in_per_sec                throughput de entrada al cluster
# bytes_out_per_sec               throughput de salida
# under_replicated_partitions     particiones sin rÃ©plicas suficientes
# request_latency_avg             latencia promedio de requests
# active_controller_count         debe ser exactamente 1

# Alerta P1: under_replicated_partitions > 0
# â†’ El cluster estÃ¡ perdiendo redundancia. Datos en riesgo.

# Alerta P2: consumer_lag_seconds > 300 (5 minutos)
# â†’ El consumer estÃ¡ 5 minutos atrÃ¡s. El dashboard real-time no es real-time.
```

**Preguntas:**

1. Â¿Consumer lag en offsets vs consumer lag en segundos â€” cuÃ¡l es mÃ¡s Ãºtil?

2. Â¿Un consumer lag de 1000 mensajes es mucho o poco?
   Â¿Depende del throughput?

3. Â¿`under_replicated_partitions > 0` siempre merece P1?

4. Â¿Burrow es mejor que el consumer lag nativo de Kafka?

5. Â¿CÃ³mo monitoras un cluster de Kafka con 100 topics
   y 50 consumer groups sin ahogarte en mÃ©tricas?

---

### Ejercicio 18.6.4 â€” End-to-end latency: medir desde el evento hasta el dashboard

```python
# La mÃ©trica mÃ¡s importante que nadie mide:
# Â¿CuÃ¡nto tiempo pasa desde que ocurre un evento
# hasta que aparece en el dashboard?

# Pipeline: evento â†’ Kafka â†’ Flink â†’ BigQuery â†’ Looker

# Medir cada hop:
# 1. event_time â†’ kafka_timestamp:     latencia de producciÃ³n
# 2. kafka_timestamp â†’ flink_processed: latencia de consumo
# 3. flink_processed â†’ bigquery_loaded: latencia de escritura
# 4. bigquery_loaded â†’ dashboard_shown: latencia de refresh

# Implementar con timestamps embebidos:
import time
import json

# Productor (al generar el evento):
evento = {
    "user_id": "alice",
    "monto": 150.0,
    "event_time": time.time(),  # timestamp del evento real
    "produced_at": time.time(), # timestamp de producciÃ³n a Kafka
}
producer.send("ventas", json.dumps(evento))

# Consumer (al procesar en Flink/Spark):
def procesar(evento):
    consumed_at = time.time()
    latencia_kafka = consumed_at - evento["produced_at"]
    latencia_total = consumed_at - evento["event_time"]
    statsd.timing("pipeline.latency.kafka", latencia_kafka * 1000)
    statsd.timing("pipeline.latency.total", latencia_total * 1000)

# Dashboard: mostrar p50, p95, p99 de latencia end-to-end
```

**Preguntas:**

1. Â¿End-to-end latency de 30 segundos es aceptable
   para un dashboard de revenue?

2. Â¿El refresh rate del dashboard (ej: 1 minuto)
   domina la latencia end-to-end?

3. Â¿CÃ³mo mides latencia end-to-end si no controlas todos los hops?

4. Â¿La latencia de producciÃ³n (event â†’ Kafka) es responsabilidad
   del data engineer o del equipo de backend?

5. Â¿p99 de latencia es mÃ¡s Ãºtil que p50 para detectar problemas?

---

### Ejercicio 18.6.5 â€” Implementar: dashboard de observabilidad del stack completo

**Tipo: Implementar**

```python
# DiseÃ±ar el dashboard de Grafana que muestre:
# 1. Airflow: estado de DAGs, duraciÃ³n, queue time, SLA compliance
# 2. Spark: job duration, shuffle, spill, GC time
# 3. Flink: throughput, latency, checkpoint, backpressure
# 4. Kafka: consumer lag, throughput, under-replicated partitions
# 5. Data quality: checks passed/failed, freshness, anomalies
# 6. End-to-end: latencia del evento al dashboard

# Para cada secciÃ³n, definir:
# - Queries de Prometheus/CloudWatch
# - Umbrales de alerta (P1, P2, P3)
# - Panel type (time series, gauge, table, heatmap)
```

**Restricciones:**
1. DiseÃ±ar al menos 15 paneles distribuidos en las 6 secciones
2. Definir las alertas para cada panel con severidad y canal
3. Incluir un "health score" global (0-100) calculado de las mÃ©tricas
4. Â¿CÃ³mo evitas que el dashboard sea demasiado ruidoso?

---

## SecciÃ³n 18.7 â€” El E-commerce Observable: Integrando los Cuatro Pilares

### Ejercicio 18.7.1 â€” Leer: la arquitectura de observabilidad completa

**Tipo: Leer**

```
El sistema de e-commerce con observabilidad integrada:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                    PIPELINES                                â”‚
  â”‚  Airflow DAGs â”‚ Spark Jobs â”‚ Flink Streaming â”‚ dbt Models  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚           â”‚            â”‚             â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                 INSTRUMENTACIÃ“N                             â”‚
  â”‚  structlog    â”‚ Spark Metrics â”‚ Flink Metrics â”‚ OpenLineage â”‚
  â”‚  StatsD       â”‚ Accumulators  â”‚ JMX           â”‚ GX/dbt testsâ”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚           â”‚            â”‚             â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚              ALMACENAMIENTO DE SEÃ‘ALES                      â”‚
  â”‚  Prometheus  â”‚ CloudWatch â”‚ Marquez â”‚ BigQuery (quality)    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚           â”‚            â”‚             â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚              VISUALIZACIÃ“N Y ALERTAS                        â”‚
  â”‚  Grafana dashboards â”‚ PagerDuty alerts â”‚ Slack notificationsâ”‚
  â”‚  Datahub catalog    â”‚ Runbooks wiki    â”‚ Incident timeline  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Los cuatro pilares en acciÃ³n para un incidente:

  1. MÃ‰TRICAS detectan: "pipeline_duration 3Ã— mÃ¡s lento que lo normal"
  2. ALERTAS notifican: PagerDuty â†’ on-call engineer
  3. LOGS diagnostican: "step 'transformar' tardÃ³ 120min por data skew"
  4. LINEAGE localiza: "la tabla raw.ventas tiene distribuciÃ³n anÃ³mala
                        porque PostgreSQL cambiÃ³ el encoding de regiÃ³n"
  5. QUALITY confirma: "el quality check 'region_in_set' fallÃ³ para
                        30% de los registros"

  Sin observabilidad: "algo estÃ¡ lento, no sÃ© quÃ© ni por quÃ©"
  Con observabilidad: "sÃ© exactamente quÃ© pasÃ³, por quÃ©, y cÃ³mo arreglarlo"
```

**Preguntas:**

1. Â¿La instrumentaciÃ³n debe ser responsabilidad de cada data engineer
   o de un equipo de platform engineering?

2. Â¿El costo de la observabilidad (Datadog, Grafana Cloud, etc.)
   es significativo respecto al costo del pipeline?

3. Â¿CuÃ¡nto tiempo invierte un equipo de 5 personas
   en mantener la observabilidad?

4. Â¿La observabilidad de datos estÃ¡ madura en 2024
   o todavÃ­a es "estado del arte"?

5. Â¿Un startup necesita los cuatro pilares desde el dÃ­a 1?

---

### Ejercicio 18.7.2 â€” Incident management: del alerta al postmortem

```
Proceso de gestiÃ³n de incidentes para pipelines de datos:

  1. DETECCIÃ“N (automÃ¡tica)
     Alerta: "revenue = $0 para 2024-03-14"
     Fuente: quality check en el pipeline post-transform
     Severidad: P1 (PagerDuty)

  2. TRIAGE (on-call, < 5 minutos)
     On-call acknowledges en PagerDuty.
     Abre #incident-2024-0314 en Slack.
     Sigue el runbook "revenue-zero".

  3. DIAGNÃ“STICO (< 30 minutos)
     Runbook paso 1: Â¿pipeline ejecutÃ³? â†’ SÃ­, success.
     Runbook paso 2: Â¿ventas_raw tiene datos? â†’ SÃ­, 15000 filas.
     Runbook paso 3: Â¿monto tiene valores vÃ¡lidos? â†’ NO.
       â†’ AVG(monto) = 0. Todos los montos son 0.
     Runbook paso 4: Â¿la fuente (PostgreSQL) tiene montos correctos? â†’ SÃ­.
     â†’ El extract lee la columna incorrecta (un deploy cambiÃ³ el nombre).

  4. RESOLUCIÃ“N (< 1 hora)
     Fix: actualizar el mapping de columnas en el extract.
     Re-ejecutar backfill del 14 de marzo.
     Verificar que revenue > 0.
     Cerrar incidente en Slack.

  5. POSTMORTEM (< 48 horas)
     Root cause: deploy de backend renombrÃ³ columna sin notificar a data eng.
     Acciones:
     - AÃ±adir schema validation en el extract (detectar cambios de columna).
     - AÃ±adir contrato entre backend y data eng para cambios de schema.
     - AÃ±adir alerta de anomalÃ­a de distribuciÃ³n (AVG(monto) = 0).
     Timeline: detecciÃ³n 03:45, resoluciÃ³n 04:30, MTTR = 45 minutos.
```

**Preguntas:**

1. Â¿45 minutos de MTTR es bueno o malo para un pipeline P1?

2. Â¿El postmortem debe ser blameless (sin culpar a nadie)?

3. Â¿CuÃ¡ntos action items del postmortem se implementan realmente?

4. Â¿Los postmortems deben ser pÃºblicos para todo el equipo de engineering?

5. Â¿Un schema contract entre backend y data engineering
   es implementable en la prÃ¡ctica?

---

### Ejercicio 18.7.3 â€” Implementar: observabilidad del sistema de e-commerce

**Tipo: Implementar**

```python
# Implementar observabilidad completa para el sistema de e-commerce:
#
# 1. Instrumentar el pipeline batch (Airflow + Spark):
#    - MÃ©tricas custom (duraciÃ³n, filas, bytes)
#    - Logging estructurado con trace_id
#    - Quality checks (5 dimensiones)
#
# 2. Instrumentar el pipeline streaming (Flink):
#    - Consumer lag monitoring
#    - End-to-end latency measurement
#    - Checkpoint monitoring
#
# 3. Configurar alertas:
#    - P1: revenue = 0, pipeline fail despuÃ©s de retries
#    - P2: pipeline lento, quality check warning
#    - P3: schema change, freshness borderline
#
# 4. Crear dashboard de Grafana:
#    - Health score global
#    - MÃ©tricas por pipeline
#    - Quality trends
#    - Incident timeline

# Bonus: implementar "data SLOs" (Service Level Objectives para datos):
# - freshness SLO: 99% de los dÃ­as, los datos estÃ¡n disponibles antes de 6am
# - quality SLO: 99.9% de los registros pasan todos los quality checks
# - completeness SLO: 95% de los registros esperados estÃ¡n presentes
```

**Restricciones:**
1. Implementar la instrumentaciÃ³n para batch y streaming
2. Configurar al menos 10 alertas con severidad y canal
3. DiseÃ±ar el dashboard con al menos 12 paneles
4. Calcular data SLOs sobre el histÃ³rico simulado

---

### Ejercicio 18.7.4 â€” El costo de no observar: calcular el ROI

**Tipo: Analizar**

```
Calcular el ROI de la observabilidad:

  Costo de la observabilidad:
    - Herramientas: Datadog ~$200/mes, Grafana Cloud ~$50/mes
    - Tiempo de setup: 2 semanas de un data engineer
    - Mantenimiento: 2 horas/semana
    - Total aÃ±o 1: ~$20,000

  Costo de NO tener observabilidad (estimado):
    - Incidente tipo "$0 en dashboard": 4 horas de 3 ingenieros = 12h
      Ã— $150/hora Ã— 2 incidentes/mes = $3,600/mes = $43,200/aÃ±o
    - Datos incorrectos no detectados: decisiones de negocio errÃ³neas
      Ã— impacto estimado = difÃ­cil de cuantificar pero potencialmente enorme
    - Tiempo de debugging sin herramientas: 2Ã— mÃ¡s que con herramientas
      Ã— frecuencia de problemas = $20,000+/aÃ±o en productividad

  ROI estimado: ($63,000 - $20,000) / $20,000 = 215%
  (sin contar decisiones de negocio errÃ³neas por datos incorrectos)
```

**Preguntas:**

1. Â¿El cÃ¡lculo de ROI es convincente para un CTO?

2. Â¿Observabilidad open-source (Prometheus + Grafana + Marquez)
   reduce significativamente el costo?

3. Â¿CuÃ¡l es el costo de Datadog para un equipo de 10 data engineers
   con 50 pipelines?

4. Â¿El costo mÃ¡s grande es la herramienta o el tiempo del equipo?

5. Â¿Empezar con observabilidad mÃ­nima y crecer
   es mejor que implementar todo de una vez?

---

### Ejercicio 18.7.5 â€” El ecosystem fit: observabilidad en el stack de 2024

**Tipo: Analizar**

```
Estado del ecosistema en 2024:

  MÃ©tricas y dashboards:
    Prometheus + Grafana (open-source, estÃ¡ndar)
    Datadog (SaaS, todo-en-uno, caro)
    CloudWatch (AWS, integrado, limitado)

  Logging:
    ELK Stack (open-source, complejo)
    Grafana Loki (open-source, simple)
    CloudWatch Logs (AWS, integrado)
    Datadog Logs (SaaS, caro pero completo)

  Lineage:
    OpenLineage + Marquez (open-source, estÃ¡ndar emergente)
    Datahub (open-source, completo)
    Atlan / Alation (SaaS, enterprise)

  Data quality:
    Great Expectations (open-source, Python)
    dbt tests (open-source, SQL)
    Soda (open-source + cloud)
    Monte Carlo / Bigeye (SaaS, anomaly detection)
    Elementary (open-source, dbt-native)

  RecomendaciÃ³n para un equipo nuevo:
    MÃ­nimo viable: structlog + Grafana + dbt tests
    Medio: + Prometheus + Great Expectations + OpenLineage
    Completo: + Datahub + PagerDuty + Monte Carlo
```

**Preguntas:**

1. Â¿Un equipo puede empezar solo con dbt tests y Grafana?

2. Â¿Monte Carlo (SaaS) justifica el costo respecto a Great Expectations (free)?

3. Â¿Datahub reemplaza a OpenLineage + Marquez?

4. Â¿La observabilidad de datos se consolidarÃ¡ en una sola herramienta?

5. Â¿Para el sistema de e-commerce del libro, Â¿cuÃ¡l es la recomendaciÃ³n final?

---

## Resumen del capÃ­tulo

**Observabilidad de datos: los cuatro pilares**

```
Pilar 1: MÃ©tricas
  Â¿CuÃ¡nto tardÃ³? Â¿CuÃ¡ntas filas? Â¿CuÃ¡ntos errores?
  â†’ Prometheus, StatsD, custom metrics en BigQuery
  â†’ Detectar anomalÃ­as automÃ¡ticamente (zscore, IQR)

Pilar 2: Logs
  Â¿QuÃ© pasÃ³ exactamente? Â¿En quÃ© orden? Â¿CuÃ¡l fue el error?
  â†’ structlog (JSON), CloudWatch/Loki/ELK
  â†’ CorrelaciÃ³n con trace_id entre componentes

Pilar 3: Lineage
  Â¿De dÃ³nde vienen los datos? Â¿A quiÃ©n afecta un cambio?
  â†’ OpenLineage, Marquez, Datahub
  â†’ Table-level para operaciÃ³n, column-level para auditorÃ­a

Pilar 4: Data Quality
  Â¿Los datos son correctos, frescos, completos?
  â†’ Great Expectations, dbt tests, Soda
  â†’ Las 5 dimensiones: freshness, volume, schema, distribution, uniqueness
```

**El principio de la observabilidad de datos:**

```
La observabilidad no previene los problemas.
Los hace visibles antes de que el negocio se entere.

  Sin observabilidad:
    Problema ocurre â†’ nadie se entera â†’ stakeholder reporta
    â†’ investigaciÃ³n manual â†’ fix â†’ "Â¿cuÃ¡nto tiempo estuvo mal?"

  Con observabilidad:
    Problema ocurre â†’ alerta automÃ¡tica â†’ diagnÃ³stico guiado
    â†’ fix â†’ postmortem â†’ prevenciÃ³n

  La diferencia no es tÃ©cnica â€” es de confianza.
  Un equipo de datos con buena observabilidad
  puede decir "nuestros datos son correctos con 99.9% de confianza".
  Un equipo sin observabilidad dice "creemos que estÃ¡ bien".
```

**ConexiÃ³n con el Cap.19 (Testing):**

> La observabilidad detecta problemas en producciÃ³n.
> El testing previene que esos problemas lleguen a producciÃ³n.
> La observabilidad dice "el revenue de ayer es $0 â€” algo estÃ¡ mal".
> Los tests dicen "este cambio habrÃ­a producido $0 â€” no lo deployemos".
> El Cap.19 explora cÃ³mo testear pipelines de datos
> antes de que lleguen al mundo real.
