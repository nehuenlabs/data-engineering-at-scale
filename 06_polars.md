# Gu√≠a de Ejercicios ‚Äî Cap.06: Polars ‚Äî Paralelismo sin JVM

> Polars hace una apuesta distinta a Spark: en lugar de distribuir el
> procesamiento entre m√∫ltiples m√°quinas, maximiza el uso de una sola.
>
> En la pr√°ctica, la mayor√≠a de los datasets de una empresa mediana
> caben en una m√°quina bien equipada. Para esos casos, Polars es
> frecuentemente 10-50√ó m√°s r√°pido que Spark ‚Äî sin cluster, sin JVM,
> sin overhead de shuffle.
>
> Pero el argumento no es "Polars reemplaza a Spark". Es entender cu√°ndo
> cada herramienta es la respuesta correcta, y por qu√©.

---

## Por qu√© Polars es r√°pido: las tres razones

```
1. Rust + sin GIL
   Polars est√° escrito en Rust. No hay JVM que arrancar, no hay GC que pause,
   no hay GIL que serialice threads. Los cores trabajan en paralelo real.

2. Apache Arrow en memoria
   Polars usa Arrow como su representaci√≥n interna. Todas las operaciones
   trabajan sobre buffers contiguos de memoria ‚Äî SIMD natural, cache-friendly.
   Sin conversiones entre formatos al combinar con otras herramientas Arrow.

3. Lazy evaluation con optimizaci√≥n del plan
   Como Spark, Polars no ejecuta hasta que le pides el resultado (.collect()).
   El optimizador puede reordenar operaciones, eliminar columnas no usadas,
   y aplicar predicate pushdown ‚Äî antes de tocar los datos.
```

```python
# La diferencia se siente en la primera l√≠nea:
import polars as pl

# Spark: arranca JVM, SparkSession, ejecutors...  (~5-10 segundos de overhead)
# Polars: sin overhead inicial ‚Äî la primera operaci√≥n tarda lo que tarda

df = pl.read_parquet("transacciones.parquet")  # listo en ~ms
resultado = df.filter(pl.col("monto") > 1000).group_by("region").agg(
    pl.sum("monto").alias("total")
)
# No ha ejecutado nada a√∫n ‚Äî es un plan lazy

resultado.collect()  # ahora ejecuta, en paralelo, sobre todos los cores
```

---

## Polars vs Pandas: el cambio de paradigma

Antes de los ejercicios, la comparaci√≥n que m√°s confunde a quienes vienen de Pandas:

```
Pandas:                              Polars:
  df["nueva"] = df["a"] * 2           df.with_columns(
                                         (pl.col("a") * 2).alias("nueva")
  df[df["a"] > 1]                      )
                                       df.filter(pl.col("a") > 1)

  df.groupby("x")["y"].sum()           df.group_by("x").agg(pl.sum("y"))

  df.apply(fn, axis=1)  ‚Üê LENTO        df.select(pl.struct(cols).map_elements(fn))
                                        o mejor: expresiones nativas sin apply
```

La diferencia conceptual: en Pandas, operas sobre Series (columnas) y puedes
referenciar el DataFrame desde afuera. En Polars, las operaciones son expresiones
(`pl.col(...)`) que se eval√∫an dentro del contexto del DataFrame ‚Äî sin estado externo.

---

## Tabla de contenidos

- [Secci√≥n 6.1 ‚Äî El modelo de ejecuci√≥n de Polars](#secci√≥n-61--el-modelo-de-ejecuci√≥n-de-polars)
- [Secci√≥n 6.2 ‚Äî Expresiones: el lenguaje de Polars](#secci√≥n-62--expresiones-el-lenguaje-de-polars)
- [Secci√≥n-6.3 ‚Äî Lazy API: planificar antes de ejecutar](#secci√≥n-63--lazy-api-planificar-antes-de-ejecutar)
- [Secci√≥n 6.4 ‚Äî Joins y GroupBy: sin shuffle expl√≠cito](#secci√≥n-64--joins-y-groupby-sin-shuffle-expl√≠cito)
- [Secci√≥n 6.5 ‚Äî Polars streaming: datos m√°s grandes que la RAM](#secci√≥n-65--polars-streaming-datos-m√°s-grandes-que-la-ram)
- [Secci√≥n 6.6 ‚Äî Integraci√≥n con el ecosistema Arrow](#secci√≥n-66--integraci√≥n-con-el-ecosistema-arrow)
- [Secci√≥n 6.7 ‚Äî Cu√°ndo Polars y cu√°ndo Spark](#secci√≥n-67--cu√°ndo-polars-y-cu√°ndo-spark)

---

## Secci√≥n 6.1 ‚Äî El Modelo de Ejecuci√≥n de Polars

### Ejercicio 6.1.1 ‚Äî Leer: paralelismo sin coordinaci√≥n de red

**Tipo: Leer/comparar**

Polars y Spark ejecutan el mismo GroupBy de forma muy diferente:

```
Spark GroupBy (distribuido):
  1. Cada executor procesa sus particiones ‚Üí calcula parciales
  2. SHUFFLE: mover datos por la red para que cada clave quede en un nodo
  3. Cada executor recibe sus claves y calcula el resultado final
  Costo dominante: el shuffle (I/O de red)

Polars GroupBy (una m√°quina, m√∫ltiples cores):
  1. Fase de particionamiento: asigna filas a buckets en memoria
     (hash(clave) % num_buckets) ‚Äî sin red, solo RAM
  2. Cada thread procesa su bucket en paralelo
  3. Merge final de los buckets en el thread principal
  Costo dominante: acceso a memoria (L3 cache, RAM)
```

```python
import polars as pl
import time

# Dataset: 50M filas, 4 columnas
df = pl.DataFrame({
    "user_id": pl.arange(0, 50_000_000, eager=True),
    "monto": pl.Series("monto", [1.0] * 50_000_000),
    "region": pl.Series("region",
        ["norte", "sur", "este", "oeste"] * 12_500_000),
    "mes": pl.Series("mes", list(range(1, 13)) * 4_166_666 + [1, 2]),
})

# GroupBy en Polars (paralelo, en memoria):
inicio = time.perf_counter()
resultado = df.group_by("region").agg(pl.sum("monto"))
print(f"Polars (lazy, no ejecutado): {time.perf_counter()-inicio:.4f}s")

inicio = time.perf_counter()
resultado = df.group_by("region").agg(pl.sum("monto"))
# collect() no es necesario aqu√≠ porque group_by es eager por defecto en Polars
print(f"Polars (eager group_by): {time.perf_counter()-inicio:.3f}s")
```

**Preguntas:**

1. ¬øPor qu√© el GroupBy de Polars no necesita shuffle de red?
   ¬øQu√© mecanismo usa para coordinar entre threads?

2. ¬øCu√°ntos threads usa Polars por defecto? ¬øC√≥mo lo controlas?

3. Para un GroupBy con 4 claves √∫nicas (como `region`), ¬øcu√°ntos buckets
   crea Polars internamente? ¬øTiene sentido usar todos los cores para 4 claves?

4. ¬øEl GroupBy de Polars escala linealmente con el n√∫mero de cores?
   ¬øQu√© lo limita?

5. Si la m√°quina tiene 8 cores y el dataset tiene 50M filas,
   ¬øcu√°ntas filas procesa cada core en la fase de particionamiento?

**Pista:** Polars usa el n√∫mero de cores f√≠sicos del sistema por defecto
(`os.cpu_count()`). Para un GroupBy con 4 claves, puede haber m√°s buckets
que claves (para reducir contenci√≥n en memoria), pero el merge final
colapsa a exactamente 4 grupos. El l√≠mite de escalado: para operaciones
muy r√°pidas (pocos datos), el overhead de crear y sincronizar threads supera
el beneficio ‚Äî Polars tiene heur√≠sticas para usar un solo thread en datasets peque√±os.

---

### Ejercicio 6.1.2 ‚Äî Medir: Polars vs Pandas vs Spark

```python
import polars as pl
import pandas as pd
import time

# Dataset de referencia: 10M filas
n = 10_000_000
data = {
    "id": list(range(n)),
    "monto": [float(i % 10000) for i in range(n)],
    "region": ["norte", "sur", "este", "oeste"][i % 4]
               if True else None for i in range(n),
    "mes": [i % 12 + 1 for i in range(n)],
}

df_pandas = pd.DataFrame(data)
df_polars = pl.DataFrame(data)

def medir(nombre, fn):
    inicio = time.perf_counter()
    resultado = fn()
    duracion = time.perf_counter() - inicio
    print(f"{nombre}: {duracion:.3f}s")
    return resultado

# Query 1: filtro + suma de columna
medir("Pandas  - filtro+suma",
    lambda: df_pandas[df_pandas["region"] == "norte"]["monto"].sum())
medir("Polars  - filtro+suma",
    lambda: df_polars.filter(pl.col("region") == "norte")["monto"].sum())

# Query 2: groupby + m√∫ltiples agregaciones
medir("Pandas  - groupby",
    lambda: df_pandas.groupby("region").agg({"monto": ["sum", "mean", "count"]}))
medir("Polars  - groupby",
    lambda: df_polars.group_by("region").agg([
        pl.sum("monto"), pl.mean("monto"), pl.count()
    ]))

# Query 3: join (requiere dos DataFrames)
df_dim_pandas = pd.DataFrame({"region": ["norte","sur","este","oeste"],
                               "factor": [1.1, 1.2, 1.3, 1.4]})
df_dim_polars = pl.DataFrame({"region": ["norte","sur","este","oeste"],
                               "factor": [1.1, 1.2, 1.3, 1.4]})
medir("Pandas  - join",
    lambda: df_pandas.merge(df_dim_pandas, on="region"))
medir("Polars  - join",
    lambda: df_polars.join(df_dim_polars, on="region"))

# Query 4: string operations
medir("Pandas  - str ops",
    lambda: df_pandas["region"].str.upper())
medir("Polars  - str ops",
    lambda: df_polars["region"].str.to_uppercase())
```

**Restricciones:**
1. Ejecutar y completar la tabla de tiempos
2. Calcular el speedup de Polars sobre Pandas para cada query
3. ¬øPara cu√°l query la diferencia es menor? ¬øPor qu√©?
4. ¬øQu√© pasa con el uso de memoria en cada caso?
5. A√±adir Spark (local mode) a la comparaci√≥n y completar la tabla

**Pista:** La diferencia de Polars sobre Pandas es m√°s grande para operaciones
de groupby y join (estructuras de datos complejas donde el layout columnar
de Arrow brilla) y menor para operaciones simples de filtro o suma sobre
una sola columna (donde Pandas + NumPy tambi√©n es eficiente).
Para strings, Polars es especialmente m√°s r√°pido porque usa su propio
motor de strings construido sobre Arrow, mientras Pandas usa Python objects.

---

### Ejercicio 6.1.3 ‚Äî El modelo de memoria: por qu√© Polars no tiene GC pauses

```python
# Polars est√° escrito en Rust ‚Äî sin Garbage Collector.
# La memoria se libera determin√≠sticamente cuando los objetos salen de scope.

import polars as pl
import psutil
import os

def uso_memoria_mb():
    proceso = psutil.Process(os.getpid())
    return proceso.memory_info().rss / 1_000_000

print(f"Antes: {uso_memoria_mb():.0f} MB")

# Crear un DataFrame grande:
df_grande = pl.DataFrame({
    "datos": list(range(5_000_000)),
    "mas_datos": [float(i) for i in range(5_000_000)],
})

print(f"Con DataFrame (5M filas): {uso_memoria_mb():.0f} MB")

# Cuando df_grande sale de scope (o se reasigna), la memoria se libera
# inmediatamente en Rust ‚Äî sin esperar al GC de Python
del df_grande

print(f"Despu√©s de del: {uso_memoria_mb():.0f} MB")
# En Python, 'del' reduce el refcount. Si llega a 0, Rust libera la memoria.
# (Python s√≠ tiene GC, pero el objeto de Rust se libera con el refcount)

# Comparar con Pandas (NumPy arrays tambi√©n se liberan con refcount,
# pero los objetos Python internos de Pandas pueden ser m√°s lentos de liberar):
import pandas as pd
import gc

df_pandas_grande = pd.DataFrame({
    "datos": range(5_000_000),
    "mas_datos": [float(i) for i in range(5_000_000)],
})
print(f"Con Pandas DataFrame: {uso_memoria_mb():.0f} MB")
del df_pandas_grande
gc.collect()  # forzar GC de Python
print(f"Despu√©s de del+gc: {uso_memoria_mb():.0f} MB")
```

**Preguntas:**

1. ¬øPor qu√© los GC pauses de la JVM (en Spark) son un problema para
   los tiempos de respuesta (latencia P99)?

2. ¬øPolars tiene GC pauses? ¬øPor qu√© no?

3. ¬øCu√°ndo puede Python (el wrapper de Polars) introducir pausas de GC
   aunque Rust no las tenga?

4. Para un job de Spark con GC time = 30%, ¬øcu√°nto se acelerar√≠a si se
   eliminasen las pauses? ¬øEso es realista?

5. ¬øEl modelo de memoria de Rust (ownership) garantiza ausencia de memory leaks?
   ¬øQu√© tipos de memory leaks son posibles de todas formas?

**Pista:** Polars no tiene GC pauses del lado de Rust porque el ownership
system de Rust garantiza que la memoria se libera cuando el propietario
sale de scope ‚Äî sin necesidad de un GC. Sin embargo, el wrapper de Python
s√≠ tiene el GC de Python para los objetos Python (el `pl.DataFrame` en Python
es un objeto Python con una referencia al objeto Rust subyacente). Las pausas
de GC de Python son mucho m√°s cortas que las de JVM y menos frecuentes,
porque Python no tiene que trazar referencias en un heap grande de objetos.

---

### Ejercicio 6.1.4 ‚Äî Leer: cu√°ndo Polars usa un solo thread

**Tipo: Diagnosticar**

Un data engineer reporta que Polars "no usa todos los cores":

```python
import polars as pl
import psutil

# Observar el uso de CPU mientras ejecuta:
print(f"Cores disponibles: {psutil.cpu_count()}")  # 16 cores

# Este query usa todos los cores:
df.group_by("region").agg(pl.sum("monto"))

# Este query usa un solo core:
df.select(pl.col("region").unique())

# Este query es complicado:
df.sort("timestamp")
```

**Preguntas:**

1. ¬øPor qu√© `unique()` puede usar solo un thread?

2. ¬øPor qu√© `sort()` tiene comportamiento variable en paralelismo?

3. Polars tiene una configuraci√≥n `POLARS_MAX_THREADS`. Si la estableces en 1,
   ¬øtodos los queries se vuelven single-threaded?

4. ¬øHay casos donde single-thread en Polars es m√°s r√°pido que multi-thread?

5. ¬øC√≥mo verificas en c√≥digo cu√°ntos threads usa Polars para una operaci√≥n dada?

**Pista:** Polars paralleliza internamente seg√∫n la operaci√≥n:
- `group_by`: altamente paralelizable (hash partitioning paralelo)
- `sort`: paralelo en la fase de particionamiento, merge-sort final puede
  tener una fase secuencial para garantizar orden global
- `unique()`: puede ser paralelo (hash-based) o secuencial dependiendo
  de si el resultado debe estar ordenado
- Operaciones de IO (read_parquet): paralelo entre archivos, potencialmente
  paralelo dentro de un archivo si tiene m√∫ltiples row groups

---

### Ejercicio 6.1.5 ‚Äî Implementar: medir el speedup real de paralelismo en Polars

```python
import polars as pl
import os
import time

def benchmark_paralelismo(n_filas: int = 10_000_000):
    """
    Mide el speedup de Polars seg√∫n el n√∫mero de threads.
    """
    df = pl.DataFrame({
        "x": pl.arange(0, n_filas, eager=True),
        "y": pl.Series([float(i % 1000) for i in range(n_filas)]),
        "cat": pl.Series(["a", "b", "c", "d"] * (n_filas // 4)),
    })

    resultados = {}
    for n_threads in [1, 2, 4, 8, 16]:
        os.environ["POLARS_MAX_THREADS"] = str(n_threads)
        # Nota: cambiar POLARS_MAX_THREADS en runtime no siempre funciona.
        # Para un benchmark real, relanzar el proceso con la variable establecida.

        inicio = time.perf_counter()
        _ = df.group_by("cat").agg([
            pl.sum("y"),
            pl.mean("y"),
            pl.std("y"),
            pl.min("x"),
            pl.max("x"),
        ])
        resultados[n_threads] = time.perf_counter() - inicio

    # Calcular speedup:
    baseline = resultados[1]
    for threads, tiempo in resultados.items():
        speedup = baseline / tiempo
        eficiencia = speedup / threads * 100
        print(f"{threads:2d} threads: {tiempo:.3f}s  speedup={speedup:.1f}√ó  eficiencia={eficiencia:.0f}%")

    return resultados
```

**Restricciones:**
1. Ejecutar el benchmark (requiere relanzar el proceso por cada configuraci√≥n de threads)
2. Calcular la eficiencia de paralelismo (speedup / threads √ó 100%)
3. ¬øLa eficiencia es constante o decrece con m√°s threads? ¬øPor qu√©?
4. ¬øQu√© operaci√≥n escala mejor? ¬øCu√°l peor?

---

## Secci√≥n 6.2 ‚Äî Expresiones: el Lenguaje de Polars

Las expresiones (`pl.col(...)`, `pl.lit(...)`, `pl.when(...)`) son el
coraz√≥n de Polars. Son objetos lazy ‚Äî describen una computaci√≥n
sin ejecutarla ‚Äî y se componen para construir planes complejos.

### Ejercicio 6.2.1 ‚Äî Expresiones b√°sicas: col, lit, when, alias

```python
import polars as pl

df = pl.DataFrame({
    "id": [1, 2, 3, 4, 5],
    "monto": [100.0, 250.0, 50.0, 1500.0, 75.0],
    "region": ["norte", "sur", "norte", "este", "sur"],
    "activo": [True, True, False, True, False],
})

# col: referenciar una columna
expr_monto = pl.col("monto")

# lit: un valor literal
expr_iva = pl.lit(1.19)

# Operaciones aritm√©ticas sobre expresiones:
expr_con_iva = pl.col("monto") * pl.lit(1.19)

# when/then/otherwise: condicional vectorizado
expr_categoria = (
    pl.when(pl.col("monto") > 1000)
    .then(pl.lit("premium"))
    .when(pl.col("monto") > 200)
    .then(pl.lit("standard"))
    .otherwise(pl.lit("basico"))
).alias("categoria")

# Usar expresiones en select/with_columns:
df_transformado = df.select([
    pl.col("id"),
    expr_con_iva.alias("monto_con_iva"),
    expr_categoria,
    pl.col("region").str.to_uppercase().alias("region_upper"),
    (~pl.col("activo")).alias("inactivo"),  # NOT booleano
])

print(df_transformado)
```

**Restricciones:**
1. A√±adir una expresi√≥n que calcula `descuento_aplicado`:
   si `activo=True` ‚Üí `monto * 0.9`, si `activo=False` ‚Üí `monto`
2. A√±adir una expresi√≥n que extrae el primer car√°cter de `region`
3. ¬øCu√°ntas veces se lee la columna `monto` en el plan si usas
   `pl.col("monto")` tres veces en el mismo `select()`?
4. ¬øQu√© error produce `pl.col("columna_que_no_existe")` y cu√°ndo se detecta?

**Pista:** Polars optimiza el plan y no lee la columna `monto` tres veces ‚Äî
lee el buffer de Arrow una vez y aplica las tres expresiones sobre √©l.
Esta es una de las ventajas del modelo de expresiones lazy: el optimizador
puede eliminar lecturas duplicadas. El error por columna inexistente se
lanza en tiempo de ejecuci√≥n (al llamar `.collect()` en lazy o al ejecutar
la operaci√≥n en eager) ‚Äî no en tiempo de compilaci√≥n.

---

### Ejercicio 6.2.2 ‚Äî Expresiones avanzadas: over, cumsum, rolling

```python
import polars as pl

df = pl.DataFrame({
    "fecha": pl.date_range(
        start=pl.date(2024, 1, 1),
        end=pl.date(2024, 1, 31),
        interval="1d",
        eager=True,
    ),
    "region": ["norte", "sur"] * 15 + ["norte"],
    "monto": [float(i * 10) for i in range(1, 32)],
})

# over(): equivale a OVER PARTITION BY en SQL (window function)
# Calcula el monto m√°ximo POR regi√≥n (sin groupby que reduce filas)
df_con_max = df.with_columns(
    pl.col("monto").max().over("region").alias("max_por_region"),
    pl.col("monto").rank("dense").over("region").alias("rank_en_region"),
    pl.col("monto").sum().over("region").alias("suma_region"),
)

# cumsum: suma acumulada
df_con_cumsum = df.with_columns(
    pl.col("monto").cum_sum().alias("monto_acumulado"),
    pl.col("monto").cum_sum().over("region").alias("monto_acum_por_region"),
)

# rolling: media m√≥vil
df_con_rolling = df.with_columns(
    pl.col("monto").rolling_mean(window_size=7).alias("media_movil_7d"),
    pl.col("monto")
      .rolling_mean(window_size=7)
      .over("region")
      .alias("media_movil_7d_por_region"),
)
```

**Preguntas:**

1. ¬øCu√°l es la diferencia entre `group_by("region").agg(pl.sum("monto"))`
   y `with_columns(pl.col("monto").sum().over("region"))`?
   ¬øCu√°ntas filas tiene el resultado de cada uno?

2. `.over()` es una window function. ¬øHace un "shuffle" interno en Polars?
   ¬øC√≥mo gestiona los grupos?

3. ¬ø`rolling_mean` puede tener valores nulos al principio? ¬øPor qu√©?
   ¬øC√≥mo manejarlos?

4. Implementar "ratio de monto sobre el total de su regi√≥n" en una sola
   expresi√≥n usando `over()`.

5. ¬øLas window functions de Polars son equivalentes a las de Spark SQL?
   ¬øHay diferencias de sem√°ntica?

**Pista:** `group_by().agg()` reduce el DataFrame: si hay 4 regiones,
el resultado tiene 4 filas. `.over()` no reduce: el resultado mantiene
todas las filas originales, pero a√±ade una columna calculada por grupo.
Es el equivalente de SQL `SUM(monto) OVER (PARTITION BY region)` ‚Äî el DataFrame
tiene las mismas filas pero con el total de la regi√≥n repetido en cada fila.

---

### Ejercicio 6.2.3 ‚Äî Expresiones sobre listas y structs

```python
import polars as pl

# Polars soporta columnas de tipo List y Struct (datos anidados):
df = pl.DataFrame({
    "user_id": [1, 2, 3],
    "eventos": [
        ["click", "view", "purchase"],
        ["view", "click"],
        ["purchase", "purchase", "view", "click"],
    ],
    "metadata": [
        {"canal": "web", "dispositivo": "laptop"},
        {"canal": "mobile", "dispositivo": "iphone"},
        {"canal": "web", "dispositivo": "desktop"},
    ],
})

# Operaciones sobre listas:
df_con_lista = df.with_columns([
    pl.col("eventos").list.len().alias("num_eventos"),
    pl.col("eventos").list.contains("purchase").alias("tiene_compra"),
    pl.col("eventos").list.first().alias("primer_evento"),
    pl.col("eventos").list.eval(
        pl.element().filter(pl.element() == "purchase").len()
    ).list.first().alias("num_compras"),
])

# Unnesting: explotar la lista en filas separadas
df_exploded = df.explode("eventos")

# Operaciones sobre structs:
df_con_struct = df.with_columns([
    pl.col("metadata").struct.field("canal").alias("canal"),
    pl.col("metadata").struct.field("dispositivo").alias("dispositivo"),
])
```

**Restricciones:**
1. Calcular la secuencia de eventos m√°s frecuente (par de eventos consecutivos)
2. Filtrar usuarios que tienen al menos 2 "purchase" en su lista de eventos
3. Implementar "conversi√≥n de click a purchase": usuarios con "click" Y "purchase"
   como porcentaje del total con "click"
4. Comparar la velocidad de `.explode()` + `group_by` vs `.list.eval()` para
   el mismo c√°lculo

**Pista:** Para calcular pares consecutivos:
```python
df.with_columns(
    pl.col("eventos").list.eval(
        pl.concat_list([
            pl.element().slice(0, pl.element().len() - 1),
            pl.element().slice(1)
        ]).list.first()  # no exactamente ‚Äî hay que usar zip o similar
    )
)
```
La soluci√≥n m√°s limpia: `.explode()` + `.shift()` dentro de un `group_by`.
El tradeoff: `.list.eval()` mantiene el dato anidado; `.explode()` lo aplana
pero permite usar todas las expresiones de columna est√°ndar.

---

### Ejercicio 6.2.4 ‚Äî El reemplazo de apply/map en Polars

```python
import polars as pl

# En Pandas, apply() es el "martillo de todo":
# df["nueva"] = df.apply(lambda row: fn(row["a"], row["b"]), axis=1)
# ‚Üí lento porque itera fila a fila con Python

# En Polars, la alternativa casi siempre es una expresi√≥n nativa:

df = pl.DataFrame({
    "precio": [100.0, 250.0, 50.0, 1500.0],
    "cantidad": [2, 1, 5, 1],
    "descuento": [0.1, 0.0, 0.2, 0.05],
})

# MAL: map_elements (equivalente a apply, lento ‚Äî fila por fila en Python)
resultado_lento = df.with_columns(
    pl.struct(["precio", "cantidad", "descuento"]).map_elements(
        lambda row: row["precio"] * row["cantidad"] * (1 - row["descuento"]),
        return_dtype=pl.Float64,
    ).alias("total_con_descuento")
)

# BIEN: expresi√≥n nativa (vectorizado, SIMD)
resultado_rapido = df.with_columns(
    (pl.col("precio") * pl.col("cantidad") * (1 - pl.col("descuento")))
    .alias("total_con_descuento")
)

# Casos donde map_elements es inevitable:
# - L√≥gica compleja que no se puede expresar con las funciones nativas
# - Llamadas a APIs externas (HTTP, base de datos)
# - Algoritmos con estado entre filas (no vectorizables)
```

**Restricciones:**
1. Medir la diferencia de velocidad entre `map_elements` y la expresi√≥n nativa
   para 1M filas
2. Identificar 5 operaciones que parecen requerir `map_elements` pero tienen
   equivalente nativo
3. Implementar una funci√≥n que aplica un modelo de ML (sklearn) sobre
   columnas de Polars de forma eficiente

**Pista:** Para el modelo de sklearn, la soluci√≥n eficiente no es `map_elements`
por cada fila ‚Äî es convertir las columnas relevantes a numpy con `.to_numpy()`,
aplicar el modelo vectorizado (`modelo.predict(X)`), y asignar el resultado
de vuelta como columna. Esto es O(n) en numpy/sklearn sin iterar en Python.
El truco: `pl.Series(modelo.predict(df.select(features).to_numpy()))`.

---

### Ejercicio 6.2.5 ‚Äî Leer: debuggear expresiones complejas

**Tipo: Diagnosticar**

Una expresi√≥n produce un resultado inesperado:

```python
import polars as pl

df = pl.DataFrame({
    "monto": [100.0, None, 300.0, None, 500.0],
    "region": ["norte", "norte", "sur", "sur", "norte"],
})

# El ingeniero quiere: suma de monto por regi√≥n, ignorando nulls
resultado = df.group_by("region").agg(
    pl.col("monto").fill_null(0).sum().alias("total")
)

# Resultado esperado: norte=600, sur=300
# Resultado obtenido: norte=600, sur=300  ‚Üê correcto aqu√≠, pero...

# Versi√≥n 2 con l√≥gica diferente:
resultado_2 = df.group_by("region").agg(
    pl.col("monto").sum().fill_null(0).alias("total")
)
# ¬øEs igual? ¬øCu√°ndo no ser√≠a igual?

# Versi√≥n 3:
resultado_3 = df.with_columns(
    pl.col("monto").fill_null(0)
).group_by("region").agg(
    pl.col("monto").sum().alias("total")
)
# ¬øEs igual a la versi√≥n 1?
```

**Preguntas:**

1. ¬ø`fill_null(0).sum()` y `sum().fill_null(0)` producen siempre el mismo resultado?
   Da un contraejemplo donde difieran.

2. ¬øC√≥mo maneja Polars los nulls en `sum()` por defecto?

3. ¬øLa Versi√≥n 3 es equivalente a la Versi√≥n 1? ¬øQu√© diferencia hay?

4. Si quieres la media ignorando nulls vs incluyendo nulls como 0,
   ¬øc√≥mo afecta el resultado?

5. ¬øExiste una forma de ver el plan de ejecuci√≥n de una expresi√≥n de Polars
   (equivalente al `explain()` de Spark)?

**Pista:** `sum()` en Polars ignora nulls por defecto ‚Äî `[1, null, 3].sum() = 4`.
Entonces `fill_null(0).sum()` = `[1, 0, 3].sum() = 4` ‚Äî mismo resultado.
La diferencia aparece con `mean()`: `[1, null, 3].mean() = 2` (ignora null)
vs `[1, 0, 3].mean() = 1.33` (incluye el 0). El plan lazy se puede ver
con `df.lazy().group_by(...).agg(...).explain()`.

---

## Secci√≥n 6.3 ‚Äî Lazy API: Planificar Antes de Ejecutar

### Ejercicio 6.3.1 ‚Äî Eager vs Lazy: cu√°ndo usar cada una

```python
import polars as pl

# API Eager (default en Polars para operaciones de DataFrame):
# Ejecuta inmediatamente. √ötil para exploraci√≥n interactiva.
df = pl.read_parquet("datos.parquet")
resultado = df.filter(pl.col("monto") > 100)  # ejecuta ahora
print(resultado)  # ya tiene el resultado

# API Lazy: construye un plan, ejecuta al final
lf = pl.scan_parquet("datos.parquet")  # scan, no read ‚Äî no carga en memoria
plan = lf.filter(pl.col("monto") > 100).select(["id", "monto"])
# Nada ha ejecutado a√∫n

plan.explain()  # ver el plan optimizado
resultado = plan.collect()  # ejecutar y materializar

# La diferencia cr√≠tica: scan_parquet con lazy API hace predicate pushdown
# autom√°ticamente ‚Äî solo lee las columnas y filas necesarias del Parquet
```

```
Sin lazy (eager):
  read_parquet ‚Üí carga TODO en memoria ‚Üí filter ‚Üí select
  I/O: leer 10 GB completos
  Memoria pico: 10 GB

Con lazy (scan + collect):
  scan_parquet ‚Üí (plan) ‚Üí filter pushdown a Parquet ‚Üí select de columnas
  I/O: leer solo las columnas y row groups que pasan el filtro ‚Üí ~500 MB
  Memoria pico: 500 MB + resultado
```

**Preguntas:**

1. ¬ø`pl.read_parquet()` carga todo el archivo en memoria? ¬øY `pl.scan_parquet()`?

2. ¬øPor qu√© el predicate pushdown funciona con `scan_parquet` pero no con `read_parquet`?

3. ¬øCu√°ndo la API eager es preferible a la lazy?

4. ¬øQu√© pasa si llamas a `.collect()` dos veces sobre el mismo LazyFrame?

5. ¬øPuedes mezclar DataFrames eager con LazyFrames en el mismo pipeline?

**Pista:** `pl.scan_parquet()` no lee nada ‚Äî registra la fuente de datos.
El plan de ejecuci√≥n puede entonces "bajar" el filtro hasta la capa de
lectura del Parquet y usar las estad√≠sticas de row groups para saltar
los que no contienen datos relevantes. `pl.read_parquet()` ya ley√≥ todo
antes de que el filtro tenga oportunidad de reducir el I/O.
Llamar `.collect()` dos veces re-ejecuta el plan ‚Äî si quieres reutilizar
el resultado, almacenarlo en un DataFrame: `df = lf.collect()`.

---

### Ejercicio 6.3.2 ‚Äî El optimizador de Polars en acci√≥n

```python
import polars as pl

# Plan sin optimizar vs optimizado:
lf = pl.scan_parquet("transacciones.parquet")

plan_sin_opt = (lf
    .select(["id", "monto", "region", "timestamp",
             "user_id", "producto_id"])  # select de 6 columnas
    .filter(pl.col("region") == "norte")  # filtro
    .select(["id", "monto"])             # luego seleccionamos solo 2
)

# El optimizador de Polars:
# 1. Projection pushdown: elimina columnas innecesarias antes del filtro
#    (solo necesita "region", "id", "monto" para el plan completo)
# 2. Predicate pushdown: baja el filtro de region al nivel del scan

plan_sin_opt.explain(optimized=False)  # plan sin optimizar
plan_sin_opt.explain(optimized=True)   # plan optimizado

# ¬øSon iguales los dos plans?
```

**Restricciones:**
1. Comparar los dos planes y documentar qu√© optimizaciones aplic√≥ Polars
2. Crear un pipeline donde la optimizaci√≥n tiene un impacto medible en tiempo
3. ¬øHay casos donde el plan optimizado es peor que el sin optimizar?

---

### Ejercicio 6.3.3 ‚Äî Streaming lazy: datos m√°s grandes que el plan

```python
import polars as pl

# Polars Streaming: ejecutar el plan en chunks para datasets que no caben en RAM
# IMPORTANTE: no es streaming de eventos (como Kafka) ‚Äî es procesamiento
# de archivos grandes en batches para reducir uso de memoria pico

# Activar el modo streaming:
resultado = (
    pl.scan_parquet("datos_grandes/*.parquet")  # m√∫ltiples archivos
    .filter(pl.col("monto") > 100)
    .group_by("region")
    .agg(pl.sum("monto"))
    .collect(streaming=True)  # ‚Üê activar streaming mode
)

# Sin streaming=True: Polars carga todo en memoria antes de agrupar
# Con streaming=True: procesa en chunks, usa mucho menos memoria pico

# Verificar que el plan soporta streaming:
lf = pl.scan_parquet("datos_grandes/*.parquet") \
    .filter(pl.col("monto") > 100) \
    .group_by("region") \
    .agg(pl.sum("monto"))

lf.explain(streaming=True)
# Si el plan muestra "[STREAMING]", la operaci√≥n es compatible
# Si alguna operaci√≥n no es compatible, Polars vuelve al modo no-streaming
```

**Preguntas:**

1. ¬øTodas las operaciones de Polars soportan el modo streaming?
   ¬øCu√°les no?

2. ¬øEl resultado de `collect(streaming=True)` es id√©ntico a `collect()`?
   ¬øO puede variar por el orden?

3. ¬øCu√°ndo el modo streaming de Polars es preferible a Spark?
   ¬øCu√°ndo Spark sigue siendo mejor?

4. ¬øEl modo streaming de Polars puede usar m√∫ltiples cores?

**Pista:** Las operaciones que no soportan streaming (a√∫n en 2024):
`sort()` global (requiere todos los datos para ordenar), `join()` de tipo
right/full outer con ambas tablas muy grandes, y algunas operaciones de ventana
complejas. Polars detecta autom√°ticamente qu√© partes del plan pueden ejecutarse
en streaming y cu√°les no, y aplica streaming solo donde es posible.
El resultado es id√©ntico siempre que no haya dependencia de orden.

---

### Ejercicio 6.3.4 ‚Äî Leer: el pipeline que no se optimiz√≥ correctamente

**Tipo: Diagnosticar**

Un pipeline de Polars tarda 45 segundos para 2 GB de datos.
El ingeniero asegura que "us√≥ la API lazy":

```python
import polars as pl

df = pl.read_parquet("datos.parquet")  # ‚Üê carga 2 GB en memoria

resultado = (df
    .lazy()                                    # ‚Üê convierte a lazy despu√©s de cargar
    .filter(pl.col("region") == "norte")
    .select(["id", "monto", "region"])
    .collect()
)
```

**Preguntas:**

1. ¬øQu√© error cometi√≥ el ingeniero en el uso de la API lazy?

2. ¬øEl predicate pushdown se aplica en este pipeline? ¬øPor qu√©?

3. ¬øCu√°ntos GB de datos lee el pipeline del disco?

4. Reescribir el pipeline correctamente y estimar el speedup.

5. ¬øHay alg√∫n caso donde `df.lazy()` (convertir un DataFrame eager a lazy)
   sea √∫til?

**Pista:** El error: `pl.read_parquet()` carga TODO el archivo en memoria
antes de llamar a `.lazy()`. La conversi√≥n eager‚Üílazy no hace "un-load"
de la memoria ‚Äî el DataFrame ya est√° materializado. Para que el predicate
pushdown y el projection pushdown funcionen, hay que empezar con
`pl.scan_parquet()` que no carga nada. La regla: si el objetivo es eficiencia,
siempre comenzar con `scan_*` y terminar con `.collect()`.

---

### Ejercicio 6.3.5 ‚Äî Implementar: un pipeline lazy completo para el sistema de e-commerce

```python
import polars as pl
from pathlib import Path

def calcular_metricas_diarias(
    ruta_eventos: str,
    ruta_clientes: str,
    ruta_productos: str,
    fecha: str,  # "2024-01-15"
) -> dict[str, pl.DataFrame]:
    """
    Calcula m√©tricas diarias usando Polars con lazy API y streaming.
    
    Dise√±ado para datasets de hasta ~100 GB en una sola m√°quina.
    """
    # Fuentes lazy (nada se carga a√∫n):
    eventos = pl.scan_parquet(f"{ruta_eventos}/fecha={fecha}/*.parquet")
    clientes = pl.scan_parquet(f"{ruta_clientes}/*.parquet")
    productos = pl.scan_parquet(f"{ruta_productos}/*.parquet")

    # Filtrar solo los eventos relevantes (predicate pushdown):
    eventos_filtrados = eventos.filter(
        pl.col("tipo").is_in(["click", "compra", "vista"])
    )

    # TODO: implementar los siguientes c√°lculos como LazyFrames
    # (no llamar a .collect() hasta el final):
    
    # 1. Revenue por regi√≥n (solo compras)
    revenue_por_region = ...

    # 2. Tasa de conversi√≥n por producto (click ‚Üí compra)
    conversion = ...

    # 3. Top 10 productos por revenue
    top_productos = ...

    # Ejecutar todo en paralelo (Polars puede optimizar m√∫ltiples LazyFrames):
    resultados = pl.collect_all([revenue_por_region, conversion, top_productos])

    return {
        "revenue_por_region": resultados[0],
        "conversion": resultados[1],
        "top_productos": resultados[2],
    }
```

**Restricciones:**
1. Implementar los tres c√°lculos como LazyFrames
2. Usar `pl.collect_all()` para ejecutarlos en paralelo
3. Verificar con `explain()` que el predicate pushdown est√° activo
4. Medir el tiempo vs la versi√≥n que hace `read_parquet` al inicio

---

## Secci√≥n 6.4 ‚Äî Joins y GroupBy: sin Shuffle Expl√≠cito

### Ejercicio 6.4.1 ‚Äî Tipos de join en Polars

```python
import polars as pl

df_izq = pl.DataFrame({
    "id": [1, 2, 3, 4],
    "valor": [10, 20, 30, 40],
})

df_der = pl.DataFrame({
    "id": [2, 3, 5],
    "extra": ["a", "b", "c"],
})

# Inner join (solo los que est√°n en ambos):
inner = df_izq.join(df_der, on="id", how="inner")
# id: [2, 3]

# Left join (todos los de izquierda):
left = df_izq.join(df_der, on="id", how="left")
# id: [1, 2, 3, 4] ‚Äî extra es null para 1 y 4

# Full outer join:
outer = df_izq.join(df_der, on="id", how="full")
# id: [1, 2, 3, 4, 5]

# Semi join (filtrar izquierda por existencia en derecha):
semi = df_izq.join(df_der, on="id", how="semi")
# id: [2, 3] ‚Äî sin columnas de df_der

# Anti join (filtrar izquierda por ausencia en derecha):
anti = df_izq.join(df_der, on="id", how="anti")
# id: [1, 4]

# Cross join (producto cartesiano):
cross = df_izq.join(df_der, how="cross")
# 4 √ó 3 = 12 filas
```

**Preguntas:**

1. ¬øQu√© algoritmo usa Polars para el join internamente?
   ¬øHash join, sort-merge join, o depende del tama√±o?

2. ¬øPor qu√© el semi join y el anti join son √∫tiles en data engineering?
   Da un ejemplo real para cada uno.

3. ¬øEl join de Polars puede spill a disco si los datos son muy grandes
   para la memoria?

4. ¬øPolars hace broadcast autom√°ticamente para la tabla peque√±a en un join?

5. ¬øHay diferencia de rendimiento entre `join(how="left")` y
   `join(how="right")` si los DataFrames son del mismo tama√±o?

**Pista:** Polars usa hash join para la mayor√≠a de los casos: construye
una hash table del DataFrame m√°s peque√±o (la "build side") y hace probe
sobre el m√°s grande (la "probe side"). Cuando ambos DataFrames son grandes,
el hash table puede ser costoso en memoria ‚Äî Polars puede hacer spill
a disco en modo streaming pero no en modo eager por defecto.
El broadcast autom√°tico: Polars analiza el tama√±o relativo de los DataFrames
y construye la hash table sobre el m√°s peque√±o, similar al broadcast join de Spark.

---

### Ejercicio 6.4.2 ‚Äî GroupBy avanzado: aggregations m√∫ltiples eficientes

```python
import polars as pl

df = pl.DataFrame({
    "user_id": [1, 1, 2, 2, 3],
    "producto_id": [10, 20, 10, 30, 20],
    "monto": [100.0, 200.0, 150.0, 50.0, 300.0],
    "tipo": ["compra", "compra", "devolucion", "compra", "compra"],
    "fecha": pl.date_range(
        start=pl.date(2024, 1, 1), periods=5, interval="1d", eager=True
    ),
})

# M√∫ltiples agregaciones en un solo group_by (una sola pasada sobre los datos):
resultado = df.group_by("user_id").agg([
    pl.count().alias("num_transacciones"),
    pl.sum("monto").alias("revenue_total"),
    pl.mean("monto").alias("ticket_promedio"),
    pl.std("monto").alias("desviacion_monto"),
    pl.col("producto_id").n_unique().alias("productos_distintos"),
    pl.col("tipo").filter(pl.col("tipo") == "compra").count().alias("compras"),
    pl.col("tipo").filter(pl.col("tipo") == "devolucion").count().alias("devoluciones"),
    pl.col("fecha").min().alias("primera_compra"),
    pl.col("fecha").max().alias("ultima_compra"),
    pl.col("monto").top_k(3).alias("top3_montos"),  # lista de top 3
])

# group_by din√°mico: agrupar por m√∫ltiples columnas
resultado_multi = df.group_by(["user_id", "tipo"]).agg(
    pl.sum("monto").alias("total")
)
```

**Restricciones:**
1. Ejecutar y verificar el resultado
2. ¬øCu√°ntas pasadas sobre los datos hace Polars para calcular todas las agregaciones?
3. A√±adir una agregaci√≥n "personalizada": la ratio de devoluciones sobre compras
4. Comparar el tiempo de estas 9 agregaciones en Polars vs Pandas vs Spark SQL

**Pista:** Polars calcula todas las agregaciones en **una sola pasada** sobre
los datos ‚Äî es una de sus fortalezas principales. En Pandas, algunas combinaciones
de agregaciones requieren m√∫ltiples pasadas (por ejemplo, si combinas `.agg()` con
operaciones de filtro por grupo). En Spark SQL, el optimizer tambi√©n colapsa
m√∫ltiples agregaciones en una sola pasada (HashAggregate con m√∫ltiples funciones).

---

### Ejercicio 6.4.3 ‚Äî Join con condiciones complejas (non-equi join)

```python
import polars as pl

# Polars 0.20+ soporta joins con condiciones no-equi:
df_transacciones = pl.DataFrame({
    "id": [1, 2, 3, 4, 5],
    "monto": [100.0, 500.0, 150.0, 2000.0, 50.0],
    "fecha": pl.date_range(
        start=pl.date(2024, 1, 1), periods=5, interval="1d", eager=True
    ),
})

df_rangos_descuento = pl.DataFrame({
    "monto_min": [0.0, 100.0, 500.0, 1000.0],
    "monto_max": [100.0, 500.0, 1000.0, float("inf")],
    "descuento": [0.0, 0.05, 0.10, 0.20],
})

# Join por rango (non-equi):
resultado = df_transacciones.join_where(
    df_rangos_descuento,
    pl.col("monto") >= pl.col("monto_min"),
    pl.col("monto") < pl.col("monto_max"),
)
```

**Preguntas:**

1. ¬øQu√© algoritmo usa Polars para el join non-equi?
   ¬øPor qu√© es m√°s caro que el equi-join?

2. ¬øEste tipo de join es posible en Pandas directamente?
   ¬øC√≥mo lo resolver√≠as en Pandas?

3. ¬øCu√°ndo es preferible un join non-equi sobre un `when/then` en una columna?

4. ¬øEl join non-equi soporta el modo streaming de Polars?

---

### Ejercicio 6.4.4 ‚Äî Optimizaci√≥n de joins: order matters

```python
import polars as pl

# Dataset: 10M transacciones, 100K usuarios, 10K productos
df_trans = pl.scan_parquet("transacciones.parquet")  # 10M filas
df_users = pl.scan_parquet("usuarios.parquet")        # 100K filas
df_prods = pl.scan_parquet("productos.parquet")       # 10K filas

# Versi√≥n A: join en orden sub√≥ptimo
plan_a = (df_trans
    .join(df_users, on="user_id")      # 10M √ó 100K ‚Üí resultado: 10M
    .join(df_prods, on="producto_id")  # 10M √ó 10K ‚Üí resultado: 10M
    .filter(pl.col("activo") == True)  # filtro al final
    .filter(pl.col("categoria") == "electronico")
)

# Versi√≥n B: filtros primero, luego joins
plan_b = (df_trans
    .filter(pl.col("monto") > 100)    # elimina ~70% de transacciones
    .join(
        df_prods.filter(pl.col("categoria") == "electronico"),
        on="producto_id"
    )
    .join(
        df_users.filter(pl.col("activo") == True),
        on="user_id"
    )
)
```

**Restricciones:**
1. Ejecutar ambos planes y medir el tiempo
2. Ver los planes con `.explain()` ‚Äî ¬øPolars optimiza la Versi√≥n A autom√°ticamente?
3. ¬øEl optimizador de Polars reordena los joins autom√°ticamente?
4. Si el optimizador no lo hace, ¬øcu√°nto speedup da ordenar manualmente?

---

### Ejercicio 6.4.5 ‚Äî Leer: diagnosticar un GroupBy lento en Polars

**Tipo: Diagnosticar**

Un pipeline de Polars tarda 8 minutos para 500M filas. El profiling muestra:

```
group_by("session_id"):          7m 45s  ‚Üê 97% del tiempo
  hash table construction:       3m 20s
  data partitioning:             2m 15s
  aggregation:                   2m 10s

filter + select:                 15s
write_parquet:                   0m 0s (nada escrito todav√≠a)
```

El schema:
```
session_id: String  ‚Üê UUIDs √∫nicos por sesi√≥n (500M valores distintos!)
evento: String
timestamp: Datetime
monto: Float64
```

**Preguntas:**

1. ¬øPor qu√© el GroupBy tarda tanto si es Polars y no Spark?

2. `session_id` tiene 500M valores distintos. ¬øQu√© implica eso para
   la hash table de Polars?

3. ¬øCu√°nta memoria ocupa la hash table para 500M UUIDs?
   (asumiendo UUID de 36 chars + overhead de hash table)

4. ¬øEs este un buen caso de uso para Polars? ¬øO deber√≠a usarse Spark?

5. ¬øC√≥mo redise√±ar√≠as el pipeline para evitar el GroupBy con
   alta cardinalidad?

**Pista:** 500M claves distintas en un GroupBy: la hash table necesita
espacio para 500M entradas. Cada entrada en una hash table de Polars
ocupa aproximadamente 32-64 bytes (clave + valor + overhead). Con 500M entradas:
500M √ó 48 bytes ‚âà 24 GB de RAM solo para la hash table. Si la m√°quina tiene
32 GB, la hash table ocupa el 75% de la RAM disponible, causando presi√≥n de memoria
y potencial swap. Este es el l√≠mite de Polars: GroupBy con cardinalidad extrema
sobre datasets muy grandes supera lo que una m√°quina puede manejar eficientemente.

---

## Secci√≥n 6.5 ‚Äî Polars Streaming: Datos m√°s Grandes que la RAM

### Ejercicio 6.5.1 ‚Äî Cu√°ndo usar el modo streaming

```python
import polars as pl

# El modo streaming de Polars NO es lo mismo que Kafka/Flink streaming.
# Es un modo de ejecuci√≥n que procesa los datos en chunks para reducir
# el uso de memoria pico.

# Caso 1: archivo de 200 GB en una m√°quina con 32 GB RAM
# Sin streaming: OOM (intenta cargar 200 GB en RAM)
# Con streaming: procesa en chunks de ~1 GB, usa ~2 GB de RAM pico

resultado = (
    pl.scan_parquet("archivo_enorme.parquet")
    .filter(pl.col("a√±o") == 2024)
    .group_by("region")
    .agg(pl.sum("monto"))
    .collect(streaming=True)
)

# Caso 2: m√∫ltiples archivos que juntos superan la RAM
resultado = (
    pl.scan_parquet("datos/*.parquet")  # puede ser 500 archivos √ó 500 MB
    .filter(pl.col("activo") == True)
    .select(["id", "region", "monto"])
    .collect(streaming=True)
)
```

**Preguntas:**

1. ¬øEl modo streaming de Polars puede procesar datos de una fuente
   que crece continuamente (como Kafka)? ¬øPor qu√© no?

2. ¬øQu√© operaciones de Polars NO soportan el modo streaming?
   ¬øPor qu√© `sort()` global no es compatible?

3. Si el chunk size del streaming es 1 GB y tienes 200 GB de datos,
   ¬øcu√°ntos chunks procesa Polars?

4. ¬øEl modo streaming es siempre m√°s lento que el modo normal?
   ¬øCu√°ndo puede ser m√°s r√°pido?

5. ¬øC√≥mo determina Polars el tama√±o del chunk en streaming?
   ¬øEs configurable?

**Pista:** El modo streaming es m√°s r√°pido que el normal cuando:
(1) el dataset no cabe en RAM ‚Äî sin streaming, el proceso fallar√≠a o
usar√≠a swap (que es mucho m√°s lento); (2) el pipeline elimina muchos datos
temprano (filtros selectivos) ‚Äî el chunk procesado es peque√±o aunque el
archivo de origen sea grande. El tama√±o del chunk es configurable pero
Polars tiene heur√≠sticas basadas en la RAM disponible del sistema.

---

### Ejercicio 6.5.2 ‚Äî Comparar: Polars streaming vs Spark para datasets medianos

```python
# Benchmark para datos de 50 GB (caben en Spark pero son grandes para Polars eager)
# Hardware: m√°quina con 32 GB RAM, 16 cores

# Opci√≥n A: Polars eager (falla o usa swap)
# resultado = pl.read_parquet("50gb/*.parquet")  # OOM probable

# Opci√≥n B: Polars streaming (funciona pero sin broadcast de dimensiones)
resultado_polars = (
    pl.scan_parquet("50gb/*.parquet")
    .filter(pl.col("region") == "norte")
    .group_by("mes")
    .agg(pl.sum("monto"))
    .collect(streaming=True)
)

# Opci√≥n C: Spark local (distribuye en los 16 cores con spill a disco)
from pyspark.sql import SparkSession, functions as F
spark = SparkSession.builder \
    .config("spark.driver.memory", "16g") \
    .config("spark.executor.memory", "12g") \
    .getOrCreate()

resultado_spark = (spark.read.parquet("50gb/*.parquet")
    .filter(F.col("region") == "norte")
    .groupBy("mes")
    .agg(F.sum("monto"))
    .collect()
)
```

**Restricciones:**
1. Dise√±ar el benchmark para 50 GB de datos con el pipeline descrito
2. Medir tiempo, uso de RAM pico, y I/O para cada opci√≥n
3. ¬øCu√°l es m√°s r√°pido? ¬øDepende del hardware?
4. ¬øHay una cuarta opci√≥n (DuckDB) que vale comparar?

> üîó Ecosistema: DuckDB es otro motor anal√≠tico de una sola m√°quina que
> compite directamente con Polars para datasets que no caben en RAM.
> DuckDB usa columnar on-disk storage y puede procesar terabytes en una
> sola m√°quina. No se cubre en este repo, pero en 2024 es relevante compararlo.

---

### Ejercicio 6.5.3 ‚Äî Implementar: pipeline streaming con escritura incremental

```python
import polars as pl
from pathlib import Path

def procesar_archivos_grandes(
    ruta_entrada: str,
    ruta_salida: str,
    chunk_size: int = 1_000_000,  # filas por chunk
) -> dict:
    """
    Procesa archivos m√°s grandes que la RAM usando streaming de Polars.
    Escribe el resultado incrementalmente para evitar acumular en memoria.
    """
    stats = {"archivos_procesados": 0, "filas_procesadas": 0, "errores": []}

    # Usar sink en lugar de collect para escritura incremental:
    (pl.scan_parquet(f"{ruta_entrada}/*.parquet")
        .filter(pl.col("activo") == True)
        .with_columns(
            (pl.col("monto") * 1.19).alias("monto_con_iva"),
        )
        .group_by("region", "mes")
        .agg([
            pl.sum("monto_con_iva").alias("revenue"),
            pl.count().alias("transacciones"),
        ])
        .sink_parquet(  # escribe incrementalmente sin acumular en RAM
            f"{ruta_salida}/resultado.parquet",
            maintain_order=False,  # m√°s eficiente sin orden garantizado
        )
    )

    return stats
```

**Restricciones:**
1. Implementar usando `sink_parquet` (que ejecuta en streaming autom√°ticamente)
2. ¬øCu√°l es la diferencia entre `.collect(streaming=True)` y `.sink_parquet()`?
3. ¬ø`sink_parquet` puede escribir particionado por columna?
4. Implementar manejo de errores: si un archivo est√° corrupto, registrar el error
   y continuar con los dem√°s

---

### Ejercicio 6.5.4 ‚Äî El l√≠mite del streaming: cu√°ndo necesitas Spark

**Tipo: Analizar**

Para cada workload, determinar si Polars streaming es suficiente
o si se necesita Spark:

```
Workload 1:
  Dataset: 500 GB de transacciones en Parquet
  Operaci√≥n: filtro + GroupBy por regi√≥n (4 valores √∫nicos) + suma
  Hardware: m√°quina con 64 GB RAM, 32 cores

Workload 2:
  Dataset: 2 TB de eventos, particionado por d√≠a
  Operaci√≥n: join entre eventos (2 TB) y clientes (50 GB)
  Hardware: m√°quina con 256 GB RAM, 64 cores

Workload 3:
  Dataset: 100 GB en S3, actualiz√°ndose continuamente (1 GB/hora)
  Operaci√≥n: calcular KPIs cada 15 minutos con los √∫ltimos 24 horas
  Hardware: cualquier configuraci√≥n de cloud

Workload 4:
  Dataset: 10 TB distribuidos en 500 archivos en S3
  Operaci√≥n: sort global por timestamp (ordenar TODO el dataset)
  Hardware: cualquier configuraci√≥n de cloud
```

Para cada uno: ¬øPolars streaming, Polars normal, Spark, u otro?

**Pista:** Workload 2: el join entre 2 TB y 50 GB es el caso dif√≠cil.
Polars streaming puede manejar la lectura del dataset de 2 TB en chunks,
pero el join requiere tener la tabla de 50 GB ("build side") en memoria
completa para construir la hash table. Con 256 GB de RAM, 50 GB caben
c√≥modamente ‚Äî Polars streaming puede funcionar aqu√≠.
Workload 4: el sort global de 10 TB requiere comparar todos los elementos
entre s√≠. Polars streaming puede hacerlo en chunks, pero el merge-sort final
necesita acceso a todo el dataset. Para 10 TB esto es impracticable en
una sola m√°quina ‚Äî Spark distribuye el sort entre m√∫ltiples nodos.

---

### Ejercicio 6.5.5 ‚Äî Leer: la historia de un dataset de 80 GB en una m√°quina de 32 GB

**Tipo: Diagnosticar**

Un equipo tiene un dataset de 80 GB y una m√°quina con 32 GB de RAM.
El pipeline actual usa Pandas y falla con OOM. El equipo est√° evaluando tres opciones:

```
Opci√≥n 1: Comprar m√°s RAM (256 GB)
  Costo: ~$500/mes en cloud
  Tiempo de implementaci√≥n: inmediato

Opci√≥n 2: Migrar a Polars con streaming
  Costo: $0 adicional (misma m√°quina)
  Tiempo de implementaci√≥n: 1-2 d√≠as de desarrollo

Opci√≥n 3: Migrar a Spark (cluster de 5 nodos √ó 16 GB)
  Costo: ~$200/mes en cloud
  Tiempo de implementaci√≥n: 1-2 semanas (setup + reescritura)
```

El pipeline: filtrar, hacer GroupBy con 100 claves √∫nicas, agregar 3 m√©tricas.
Se ejecuta una vez al d√≠a, no hay SLA estricto de latencia.

**Preguntas:**

1. ¬øCu√°l de las tres opciones recomendar√≠as y por qu√©?

2. ¬øEl dataset crecer√° con el tiempo? ¬øC√≥mo cambia la recomendaci√≥n
   si el a√±o que viene son 500 GB?

3. ¬øQu√© informaci√≥n adicional necesitar√≠as para hacer una recomendaci√≥n
   m√°s precisa?

4. Si el equipo ya tiene experiencia con Spark (tiene un cluster existente),
   ¬øcambia la recomendaci√≥n?

5. ¬øExiste una "cuarta opci√≥n" que el equipo no consider√≥?

---

## Secci√≥n 6.6 ‚Äî Integraci√≥n con el Ecosistema Arrow

### Ejercicio 6.6.1 ‚Äî De Polars a Pandas y viceversa

```python
import polars as pl
import pandas as pd
import numpy as np

df_polars = pl.DataFrame({
    "id": [1, 2, 3, 4, 5],
    "monto": [100.0, None, 300.0, 400.0, None],
    "region": ["norte", "sur", "norte", "este", "sur"],
    "activo": [True, False, True, True, False],
})

# Polars ‚Üí Pandas (zero-copy para tipos compatibles):
df_pandas = df_polars.to_pandas()
# monto: Float64 en Polars ‚Üí float64 en Pandas (zero-copy si no hay nulls)
# Con nulls: necesita pd.NA o NaN ‚Üí puede requerir copia

# Pandas ‚Üí Polars:
df_polars_2 = pl.from_pandas(df_pandas)

# Verificar zero-copy:
arr_polars = df_polars["monto"].to_numpy()
arr_pandas = df_pandas["monto"].values
print(f"Comparte buffer: {np.shares_memory(arr_polars, arr_pandas)}")
# True si zero-copy, False si se copi√≥

# V√≠a Arrow (m√°s expl√≠cito):
tabla_arrow = df_polars.to_arrow()
df_pandas_via_arrow = tabla_arrow.to_pandas()
```

**Preguntas:**

1. ¬øLa columna `monto` con nulls requiere copia al convertir a Pandas?
   ¬øPor qu√© (qu√© tipo de dato usa Pandas para floats con nulls)?

2. ¬øLa columna `activo` (bool) requiere copia? ¬øQu√© tipo usa Pandas?

3. ¬øCu√°ndo preferir√≠as `df.to_pandas()` vs `df.to_arrow().to_pandas()`?

4. Si tienes un DataFrame de Pandas con 1M filas y lo conviertes a Polars,
   ¬øcu√°nta memoria adicional se usa? ¬øDepende del tipo de las columnas?

---

### Ejercicio 6.6.2 ‚Äî Integraci√≥n con Spark via Arrow

```python
# Transferir datos entre Polars y Spark usando Arrow como formato intermedio:
import polars as pl
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

# Polars ‚Üí Spark:
df_polars = pl.read_parquet("datos_locales.parquet")
tabla_arrow = df_polars.to_arrow()
df_spark = spark.createDataFrame(tabla_arrow.to_pandas())
# Con Arrow habilitado, la conversi√≥n es mucho m√°s eficiente

# Spark ‚Üí Polars:
df_spark_resultado = spark.read.parquet("s3://resultado/")
df_polars_resultado = pl.from_pandas(
    df_spark_resultado.toPandas()
)
# toPandas() con Arrow ‚Üí Arrow buffer en memoria ‚Üí Polars sin copia adicional

# Mejor alternativa: escribir a Parquet y leer
df_spark_resultado.write.parquet("/tmp/resultado_local.parquet")
df_polars_resultado = pl.read_parquet("/tmp/resultado_local.parquet")
```

**Preguntas:**

1. ¬øCu√°ntas copias de los datos hay en la cadena
   `Polars ‚Üí Arrow ‚Üí Pandas ‚Üí Spark`?

2. ¬øPor qu√© escribir a Parquet y leer es frecuentemente m√°s eficiente
   que la conversi√≥n en memoria para datasets grandes?

3. ¬øExiste una forma de transferir datos entre Polars y Spark sin
   pasar por Pandas?

4. ¬øCu√°ndo tendr√≠a sentido usar Polars para preprocesar datos y luego
   enviarlos a Spark para procesamiento distribuido?

---

### Ejercicio 6.6.3 ‚Äî Polars con DeltaLake

```python
import polars as pl

# Polars puede leer tablas Delta Lake directamente (via delta-rs):
df = pl.read_delta("s3://mi-lakehouse/ventas/")

# Leer una versi√≥n espec√≠fica (time travel):
df_historico = pl.read_delta(
    "s3://mi-lakehouse/ventas/",
    version=10,  # versi√≥n 10 de la tabla
)

# Escribir a Delta Lake:
df.write_delta(
    "s3://mi-lakehouse/metricas/",
    mode="append",
)

# Upsert (merge) con Delta Lake desde Polars:
# (a√∫n en desarrollo en 2024 ‚Äî verificar la versi√≥n actual de delta-rs)
from deltalake import DeltaTable, write_deltalake

write_deltalake(
    "s3://mi-lakehouse/ventas/",
    df.to_arrow(),
    mode="merge",
    predicate="s.id = t.id",
)
```

> ‚öôÔ∏è Versi√≥n: la integraci√≥n de Polars con Delta Lake via `delta-rs` est√°
> activamente en desarrollo. Las APIs de escritura y merge cambian entre
> versiones menores de `polars` y `deltalake`. Verificar la documentaci√≥n
> de `delta-rs` para la versi√≥n que uses en producci√≥n.

**Preguntas:**

1. ¬øQu√© es `delta-rs` y c√≥mo se relaciona con Delta Lake de Databricks?

2. ¬øPolars puede leer tablas Delta Lake particionadas eficientemente?
   ¬øHace predicate pushdown sobre las particiones?

3. ¬øCu√°les son las limitaciones de Polars + Delta Lake vs Spark + Delta Lake?

---

### Ejercicio 6.6.4 ‚Äî Compartir datos entre microservicios con Arrow Flight

```python
# Ver Cap.02 ¬ß2.3.3 para la implementaci√≥n de Arrow Flight.
# Aqu√≠: integrar Polars con Arrow Flight.

import polars as pl
import pyarrow.flight as flight

class ServidorPolars(flight.FlightServerBase):
    """Servidor que expone DataFrames de Polars via Arrow Flight."""
    
    def __init__(self, datasets: dict[str, pl.DataFrame], *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.datasets = datasets
    
    def do_get(self, context, ticket):
        nombre = ticket.ticket.decode()
        df = self.datasets[nombre]
        # Polars ‚Üí Arrow sin copia:
        tabla = df.to_arrow()
        return flight.RecordBatchStream(tabla)

# El cliente recibe datos como Arrow y los convierte a Polars:
def obtener_dataset(nombre: str, host: str, port: int) -> pl.DataFrame:
    cliente = flight.connect(f"grpc://{host}:{port}")
    tabla_arrow = cliente.do_get(
        flight.Ticket(nombre.encode())
    ).read_all()
    return pl.from_arrow(tabla_arrow)  # zero-copy desde Arrow a Polars
```

**Restricciones:**
1. Implementar el servidor y cliente completos
2. Medir la velocidad de transferencia vs HTTP+JSON para 1M filas
3. ¬øCu√°ndo este patr√≥n es m√°s √∫til que simplemente escribir a S3?

---

### Ejercicio 6.6.5 ‚Äî Polars en Python con extensiones nativas

```python
# Polars permite escribir extensiones en Rust que se llaman desde Python.
# Para casos donde ninguna funci√≥n nativa de Polars es suficiente.

# Ejemplo: una transformaci√≥n personalizada de alto rendimiento
# (normalmente no necesitas esto ‚Äî las expresiones nativas cubren el 99%)

# Usando la API de plugins de Polars (Polars 0.20+):
# 1. Escribir la funci√≥n en Rust (polars_plugin crate)
# 2. Compilar como librer√≠a din√°mica
# 3. Registrar en Polars
# 4. Llamar desde Python como si fuera una funci√≥n nativa

# Para el 99% de los casos: map_elements con una funci√≥n Python
# es suficiente aunque sea m√°s lento

# Caso v√°lido para extensi√≥n Rust:
# Algoritmo de criptograf√≠a custom sobre columnas de strings
# Parsing de formato binario propietario
# Algoritmo iterativo que no puede vectorizarse

# Para este ejercicio: explorar el ecosistema de extensiones de Polars
# y documentar 3 extensiones √∫tiles disponibles en 2024
```

**Restricciones:**
1. Investigar el ecosistema de plugins de Polars
2. Identificar 3 casos donde una extensi√≥n Rust mejora significativamente
   el rendimiento sobre `map_elements` Python
3. ¬øCu√°ndo vale el esfuerzo de escribir una extensi√≥n en Rust?

> üîó Ecosistema: el repositorio `pola-rs/polars-plugins` contiene ejemplos
> de c√≥mo escribir extensiones. Tambi√©n `polars-ds` y `polars-ols` son
> extensiones de terceros para data science y regresi√≥n, respectivamente.

---

## Secci√≥n 6.7 ‚Äî Cu√°ndo Polars y Cu√°ndo Spark

### Ejercicio 6.7.1 ‚Äî El √°rbol de decisi√≥n actualizado

**Tipo: Construir**

Completar el √°rbol de decisi√≥n para elegir entre Polars, Spark y otros:

```
¬øLos datos caben en una sola m√°quina (< RAM disponible)?
  S√≠ ‚Üí ¬øNecesitas operaciones de ML/estad√≠sticas avanzadas?
       S√≠ ‚Üí Polars + scikit-learn/scipy (Polars para preprocesar, sklearn para ML)
       No ‚Üí Polars eager (r√°pido, simple)
       
  ¬øLos datos caben en una sola m√°quina con streaming (< 10√ó RAM)?
  S√≠ ‚Üí ???
  No ‚Üí ???

¬øNecesitas datos en tiempo real (latencia < 1 minuto)?
  S√≠ ‚Üí ???

¬øEl equipo ya tiene un cluster de Spark funcionando?
  S√≠ ‚Üí ???

¬øLos datos est√°n en Delta Lake / Iceberg con historial y ACID?
  S√≠ ‚Üí ???
```

**Restricciones:**
1. Completar el √°rbol con todas las ramas
2. A√±adir DuckDB como opci√≥n en las ramas relevantes
3. ¬øHay casos donde la respuesta correcta es "los dos en secuencia"?

---

### Ejercicio 6.7.2 ‚Äî Casos donde Polars supera a Spark

```python
import polars as pl
from pyspark.sql import SparkSession, functions as F
import time

spark = SparkSession.builder.getOrCreate()

# Caso 1: Dataset peque√±o (<10 GB), job frecuente
# Polars: sin overhead de SparkSession, sin serializaci√≥n
for i in range(100):  # 100 ejecuciones al d√≠a
    inicio = time.perf_counter()
    df = pl.read_parquet(f"datos_{i}.parquet")  # 500 MB
    resultado = df.group_by("region").agg(pl.sum("monto"))
    tiempo_polars = time.perf_counter() - inicio

    inicio = time.perf_counter()
    df_s = spark.read.parquet(f"datos_{i}.parquet")
    resultado_s = df_s.groupBy("region").agg(F.sum("monto")).collect()
    tiempo_spark = time.perf_counter() - inicio
    
    print(f"Polars: {tiempo_polars:.2f}s | Spark: {tiempo_spark:.2f}s")
```

**Restricciones:**
1. Medir ambas opciones para 10, 100, y 1000 MB de datos
2. Calcular el breakeven: ¬øa qu√© tama√±o de datos Spark empieza a ser m√°s r√°pido?
3. Calcular el costo total para 100 ejecuciones al d√≠a de cada opci√≥n
4. ¬øEl overhead de Spark baja si reutilizas la SparkSession? ¬øCu√°nto?

---

### Ejercicio 6.7.3 ‚Äî Casos donde Spark es mejor que Polars

Para cada caso, explicar por qu√© Spark supera a Polars y no al rev√©s:

```
Caso 1: 10 TB de datos de click stream distribuidos en 1,000 archivos en S3
        Job: calcular revenue por campa√±a por d√≠a

Caso 2: Join entre tabla de 1 TB y tabla de 800 GB (ambas grandes)
        Hardware disponible: m√°quina de 64 GB RAM

Caso 3: Pipeline que necesita tolerancia a fallos: si un nodo falla
        a mitad del job, debe retomar desde donde qued√≥ sin reempezar

Caso 4: Pipeline ejecutado en EMR o Databricks con autoescalado:
        algunos d√≠as son 1 TB, otros 50 TB

Caso 5: 20 data scientists ejecutan queries ad-hoc simult√°neamente
        sobre el mismo dataset de 5 TB (multi-tenancy)
```

---

### Ejercicio 6.7.4 ‚Äî Polars en producci√≥n: patrones reales

**Tipo: Dise√±ar**

Un equipo de analytics tiene este stack:
- Pipeline batch diario: procesa 50 GB/d√≠a de eventos de app m√≥vil
- Dimensiones: usuarios (5 GB), productos (2 GB), campa√±as (200 MB)
- Job actual: Spark en EMR, tarda 45 minutos
- Equipo: 3 data engineers con experiencia en Pandas/Python, sin experiencia en Spark
- Presupuesto: quieren reducir el costo de la infraestructura

Dise√±ar la migraci√≥n a Polars:
1. ¬øEs viable? ¬øQu√© parte del pipeline puede migrar y cu√°l no?
2. ¬øQu√© m√°quina recomiendas? (RAM, cores)
3. ¬øPolars eager o lazy? ¬øModo streaming?
4. ¬øC√≥mo manejas las dimensiones (joins frecuentes)?
5. Estimaci√≥n del nuevo tiempo de ejecuci√≥n y costo

---

### Ejercicio 6.7.5 ‚Äî El pipeline h√≠brido: Polars + Spark

**Tipo: Implementar**

Algunos pipelines se benefician de usar ambas herramientas:

```python
import polars as pl
from pyspark.sql import SparkSession, functions as F

# Escenario: pipeline con tres etapas
# Etapa 1: preprocesar dimensiones peque√±as (Polars, r√°pido)
# Etapa 2: join distribuido con datos grandes (Spark)
# Etapa 3: calcular m√©tricas finales sobre resultado peque√±o (Polars)

spark = SparkSession.builder \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .getOrCreate()

# Etapa 1: Polars preprocesa las dimensiones
df_clientes_polars = (pl.scan_parquet("clientes.parquet")
    .filter(pl.col("activo") == True)
    .with_columns([
        pl.col("nombre").str.strip_chars().str.to_lowercase(),
        pl.when(pl.col("gasto_historico") > 10000)
          .then(pl.lit("premium"))
          .otherwise(pl.lit("standard"))
          .alias("segmento"),
    ])
    .collect()
)

# Convertir a Spark para el join distribuido:
df_clientes_spark = spark.createDataFrame(df_clientes_polars.to_pandas())
df_clientes_spark.cache().count()

# Etapa 2: Spark hace el join distribuido
resultado_spark = (spark.read.parquet("s3://eventos-3tb/")
    .join(df_clientes_spark, on="cliente_id")
    .groupBy("segmento", "mes")
    .agg(F.sum("monto").alias("revenue"))
)

# Etapa 3: Polars procesa el resultado peque√±o
resultado_polars = pl.from_pandas(resultado_spark.toPandas())
reporte_final = (resultado_polars
    .with_columns(
        (pl.col("revenue") / pl.col("revenue").sum()).alias("share")
    )
    .sort("revenue", descending=True)
)
```

**Restricciones:**
1. Implementar el pipeline h√≠brido completo
2. Medir el tiempo de cada etapa
3. ¬øHay una forma m√°s eficiente de transferir los resultados de Spark
   a Polars que via Pandas?
4. ¬øCu√°ndo este patr√≥n h√≠brido es mejor que "solo Spark" o "solo Polars"?

---

## Resumen del cap√≠tulo

**Las cinco razones por las que Polars es r√°pido (y cu√°ndo no lo son):**

```
1. Rust sin GIL ‚Üí paralelismo real en todos los cores
   L√≠mite: el paralelismo est√° limitado por la m√°quina, no el cluster

2. Apache Arrow en memoria ‚Üí SIMD, cache-friendly, zero-copy con el ecosistema
   L√≠mite: datos m√°s grandes que la RAM necesitan el modo streaming

3. Lazy evaluation ‚Üí predicate pushdown, projection pushdown, eliminar duplicados
   L√≠mite: hay que usar scan_* (no read_*) para activar la lazy API

4. Expresiones nativas ‚Üí sin bucles Python, sin GIL por fila
   L√≠mite: l√≥gica compleja que no cabe en expresiones requiere map_elements (lento)

5. Sin overhead de JVM y cluster ‚Üí arranque inmediato, sin serializaci√≥n entre nodos
   L√≠mite: exactamente ese mismo overhead es lo que permite escalar a m√∫ltiples m√°quinas
```

**El criterio de decisi√≥n en una l√≠nea:**

> Si los datos caben en una m√°quina bien equipada y el job no necesita
> tolerancia a fallos ni multi-tenancy, Polars es casi siempre m√°s r√°pido y m√°s simple que Spark.
> Si los datos no caben o el cluster ya existe, Spark es la respuesta.

**Lo que conecta este cap√≠tulo con el Cap.07 (DataFusion):**

> Polars usa Arrow como formato de memoria y est√° escrito en Rust.
> DataFusion tambi√©n usa Arrow y tambi√©n est√° escrito en Rust.
> Pero DataFusion es un motor de queries SQL, no un DataFrame API.
> El Cap.07 explora cu√°ndo la interfaz SQL y la naturaleza embebible de DataFusion
> son ventajas sobre el API de DataFrame de Polars.
